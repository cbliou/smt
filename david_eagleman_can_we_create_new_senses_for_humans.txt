Talk	en	zh-tw
david_eagleman_can_we_create_new_senses_for_humans	"We are built out of very small stuff, and we are embedded in a very large cosmos, and the fact is that we are not very good at understanding reality at either of those scales, and that's because our brains haven't evolved to understand  the world at that scale. Instead, we're trapped on this very thin slice of perception right in the middle. But it gets strange, because even at  that slice of reality that we call home, we're not seeing most of the action that's going on. So take the colors of our world. This is light waves, electromagnetic radiation that bounces off objects and it hits specialized receptors in the back of our eyes. But we're not seeing all the waves out there. In fact, what we see is less than a 10 trillionth of what's out there. So you have radio waves and microwaves and X-rays and gamma rays passing through your body right now and you're completely unaware of it, because you don't come with the proper biological receptors for picking it up. There are thousands of cell phone conversations passing through you right now, and you're utterly blind to it. Now, it's not that these things are inherently unseeable. Snakes include some infrared in their reality, and honeybees include ultraviolet in their view of the world, and of course we build machines in the dashboards of our cars to pick up on signals in the radio frequency range, and we built machines in hospitals to pick up on the X-ray range. But you can't sense any of those by yourself, at least not yet, because you don't come equipped with the proper sensors. Now, what this means is that our experience of reality is constrained by our biology, and that goes against the common sense notion that our eyes and our ears and our fingertips are just picking up the objective reality that's out there. Instead, our brains are sampling just a little bit of the world. Now, across the animal kingdom, different animals pick up on different parts of reality. So in the blind and deaf world of the tick, the important signals are temperature and butyric acid; in the world of the black ghost knifefish, its sensory world is lavishly colored by electrical fields; and for the echolocating bat, its reality is constructed out of air compression waves. That's the slice of their ecosystem that they can pick up on, and we have a word for this in science. It's called the umwelt, which is the German word for the surrounding world. Now, presumably, every animal assumes that its umwelt is the entire objective reality out there, because why would you ever stop to imagine that there's something beyond what we can sense. Instead, what we all do is we accept reality as it's presented to us. Let's do a consciousness-raiser on this. Imagine that you are a bloodhound dog. Your whole world is about smelling. You've got a long snout that has 200 million scent receptors in it, and you have wet nostrils that attract and trap scent molecules, and your nostrils even have slits so you can take big nosefuls of air. Everything is about smell for you. So one day, you stop in your tracks with a revelation. You look at your human owner and you think, ""What is it like to have the pitiful, impoverished nose of a human? (Laughter) What is it like when you take a feeble little noseful of air? How can you not know that there's a cat 100 yards away, or that your neighbor was on this very spot six hours ago?"" (Laughter) So because we're humans, we've never experienced that world of smell, so we don't miss it, because we are firmly settled into our umwelt. But the question is, do we have to be stuck there? So as a neuroscientist, I'm interested in the way that technology might expand our umwelt, and how that's going to change the experience of being human. So we already know that we can marry our technology to our biology, because there are hundreds of thousands of people walking around with artificial hearing and artificial vision. So the way this works is, you take a microphone and you digitize the signal, and you put an electrode strip directly into the inner ear. Or, with the retinal implant, you take a camera and you digitize the signal, and then you plug an electrode grid directly into the optic nerve. And as recently as 15 years ago, there were a lot of scientists who thought these technologies wouldn't work. Why? It's because these technologies speak the language of Silicon Valley, and it's not exactly the same dialect as our natural biological sense organs. But the fact is that it works; the brain figures out how to use the signals just fine. Now, how do we understand that? Well, here's the big secret: Your brain is not hearing or seeing any of this. Your brain is locked in a vault of silence and darkness inside your skull. All it ever sees are electrochemical signals that come in along different data cables, and this is all it has to work with, and nothing more. Now, amazingly, the brain is really good at taking in these signals and extracting patterns and assigning meaning, so that it takes this inner cosmos and puts together a story of this, your subjective world. But here's the key point: Your brain doesn't know, and it doesn't care, where it gets the data from. Whatever information comes in, it just figures out what to do with it. And this is a very efficient kind of machine. It's essentially a general purpose computing device, and it just takes in everything and figures out what it's going to do with it, and that, I think, frees up Mother Nature to tinker around with different sorts of input channels. So I call this the P.H.  model of evolution, and I don't want to get too technical here, but P.H. stands for Potato Head, and I use this name to emphasize that all these sensors that we know and love, like our eyes and our ears and our fingertips, these are merely peripheral plug-and-play devices: You stick them in, and you're good to go. The brain figures out what to do with the data that comes in. And when you look across the animal kingdom, you find lots of peripheral devices. So snakes have heat pits with which to detect infrared, and the ghost knifefish has electroreceptors, and the star-nosed mole has this appendage with 22 fingers on it with which it feels around and constructs a 3D model of the world, and many birds have magnetite so they can orient to the magnetic field of the planet. So what this means is that nature doesn't have to continually redesign the brain. Instead, with the principles of brain operation established, all nature has to worry about is designing new peripherals. Okay. So what this means is this: The lesson that surfaces is that there's nothing really special or fundamental about the biology that we come to the table with. It's just what we have inherited from a complex road of evolution. But it's not what we have to stick with, and our best proof of principle of this comes from what's called sensory substitution. And that refers to feeding information into the brain via unusual sensory channels, and the brain just figures out what to do with it. Now, that might sound speculative, but the first paper demonstrating this was published in the journal Nature in 1969. So a scientist named Paul Bach-y-Rita put blind people in a modified dental chair, and he set up a video feed, and he put something in front of the camera, and then you would feel that poked into your back with a grid of solenoids. So if you wiggle a coffee cup in front of the camera, you're feeling that in your back, and amazingly, blind people got pretty good at being able to determine what was in front of the camera just by feeling it in the small of their back. Now, there have been many modern incarnations of this. The sonic glasses take a video feed right in front of you and turn that into a sonic landscape, so as things move around, and get closer and farther, it sounds like ""Bzz, bzz, bzz."" It sounds like a cacophony, but after several weeks, blind people start getting pretty good at understanding what's in front of them just based on what they're hearing. And it doesn't have to be through the ears: this system uses an electrotactile grid on the forehead, so whatever's in front of the video feed, you're feeling it on your forehead. Why the forehead? Because you're not using it for much else. The most modern incarnation is called the brainport, and this is a little electrogrid that sits on your tongue, and the video feed gets turned into these little electrotactile signals, and blind people get so good at using this that they can throw a ball into a basket, or they can navigate complex obstacle courses. They can come to see through their tongue. Now, that sounds completely insane, right? But remember, all vision ever is is electrochemical signals coursing around in your brain. Your brain doesn't know where the signals come from. It just figures out what to do with them. So my interest in my lab is sensory substitution for the deaf, and this is a project I've undertaken with a graduate student in my lab, Scott Novich, who is spearheading this for his thesis. And here is what we wanted to do: we wanted to make it so that sound from the world gets converted in some way so that a deaf person can understand what is being said. And we wanted to do this, given the power and ubiquity of portable computing, we wanted to make sure that this would run on cell phones and tablets, and also we wanted to make this a wearable, something that you could wear under your clothing. So here's the concept. So as I'm speaking, my sound is getting captured by the tablet, and then it's getting mapped onto a vest that's covered in vibratory motors, just like the motors in your cell phone. So as I'm speaking, the sound is getting translated to a pattern of vibration on the vest. Now, this is not just conceptual: this tablet is transmitting Bluetooth, and I'm wearing the vest right now. So as I'm speaking — (Applause) — the sound is getting translated into dynamic patterns of vibration. I'm feeling the sonic world around me. So, we've been testing this with deaf people now, and it turns out that after just a little bit of time, people can start feeling, they can start understanding the language of the vest. So this is Jonathan. He's 37 years old. He has a master's degree. He was born profoundly deaf, which means that there's a part of his umwelt that's unavailable to him. So we had Jonathan train with the vest for four days, two hours a day, and here he is on the fifth day. Scott Novich: You. David Eagleman: So Scott says a word, Jonathan feels it on the vest, and he writes it on the board. SN: Where. Where. DE: Jonathan is able to translate this complicated pattern of vibrations into an understanding of what's being said. SN: Touch. Touch. DE: Now, he's not doing this — (Applause) — Jonathan is not doing this consciously, because the patterns are too complicated, but his brain is starting to unlock the pattern that allows it to figure out what the data mean, and our expectation is that, after wearing this for about three months, he will have a direct perceptual experience of hearing in the same way that when a blind person passes a finger over braille, the meaning comes directly off the page without any conscious intervention at all. Now, this technology has the potential to be a game-changer, because the only other solution for deafness is a cochlear implant, and that requires an invasive surgery. And this can be built for 40 times cheaper than a cochlear implant, which opens up this technology globally, even for the poorest countries. Now, we've been very encouraged by our results with sensory substitution, but what we've been thinking a lot about is sensory addition. How could we use a technology like this to add a completely new kind of sense, to expand the human umvelt? For example, could we feed real-time data from the Internet directly into somebody's brain, and can they develop a direct perceptual experience? So here's an experiment we're doing in the lab. A subject is feeling a real-time streaming feed from the Net of data for five seconds. Then, two buttons appear, and he has to make a choice. He doesn't know what's going on. He makes a choice, and he gets feedback after one second. Now, here's the thing: The subject has no idea what all the patterns mean, but we're seeing if he gets better at figuring out which button to press. He doesn't know that what we're feeding is real-time data from the stock market, and he's making buy and sell decisions. (Laughter) And the feedback is telling him whether he did the right thing or not. And what we're seeing is, can we expand the human umvelt so that he comes to have, after several weeks, a direct perceptual experience of the economic movements of the planet. So we'll report on that later to see how well this goes. (Laughter) Here's another thing we're doing: During the talks this morning, we've been automatically scraping Twitter for the TED2015 hashtag, and we've been doing an automated sentiment analysis, which means, are people using positive words or negative words or neutral? And while this has been going on, I have been feeling this, and so I am plugged in to the aggregate emotion of thousands of people in real time, and that's a new kind of human experience, because now I can know how everyone's doing and how much you're loving this. (Laughter) (Applause) It's a bigger experience than a human can normally have. We're also expanding the umvelt of pilots. So in this case, the vest is streaming nine different measures from this quadcopter, so pitch and yaw and roll and orientation and heading, and that improves this pilot's ability to fly it. It's essentially like he's extending his skin up there, far away. And that's just the beginning. What we're envisioning is taking a modern cockpit full of gauges and instead of trying to read the whole thing, you feel it. We live in a world of information now, and there is a difference between accessing big data and experiencing it. So I think there's really no end to the possibilities on the horizon for human expansion. Just imagine an astronaut being able to feel the overall health of the International Space Station, or, for that matter, having you feel the invisible states of your own health, like your blood sugar and the state of your microbiome, or having 360-degree vision or seeing in infrared or ultraviolet. So the key is this: As we move into the future, we're going to increasingly be able to choose our own peripheral devices. We no longer have to wait for Mother Nature's sensory gifts on her timescales, but instead, like any good parent, she's given us the tools that we need to go out and define our own trajectory. So the question now is, how do you want to go out and experience your universe? Thank you. (Applause) Chris Anderson: Can you feel it? DE: Yeah. Actually, this was the first time I felt applause on the vest. It's nice. It's like a massage. (Laughter) CA: Twitter's going crazy. Twitter's going mad. So that stock market experiment. This could be the first experiment that secures its funding forevermore, right, if successful? DE: Well, that's right, I wouldn't have to write to NIH anymore. CA: Well look, just to be skeptical for a minute, I mean, this is amazing, but isn't most of the evidence so far that sensory substitution works, not necessarily  that sensory addition works? I mean, isn't it possible that the blind person can see through their tongue because the visual cortex is still there, ready to process, and that that is needed as part of it? DE: That's a great question. We actually have no idea what the theoretical limits are of what kind of data the brain can take in. The general story, though, is that it's extraordinarily flexible. So when a person goes blind, what we used to call their visual cortex gets taken over by other things, by touch, by hearing, by vocabulary. So what that tells us is that the cortex is kind of a one-trick pony. It just runs certain kinds of computations on things. And when we look around at things like braille, for example, people are getting information through bumps on their fingers. So I don't think we have any reason to think there's a theoretical limit that we know the edge of. CA: If this checks out, you're going to be deluged. There are so many possible applications for this. Are you ready for this? What are you most excited about, the direction it might go? DE: I mean, I think there's a lot of applications here. In terms of beyond sensory substitution, the things I started mentioning about astronauts on the space station, they spend a lot of their time monitoring things, and they could instead just get what's going on, because what this is really good for is multidimensional data. The key is this: Our visual systems are good at detecting blobs and edges, but they're really bad at what our world has become, which is screens with lots and lots of data. We have to crawl that with our attentional systems. So this is a way of just feeling the state of something, just like the way you know the state of your body as you're standing around. So I think heavy machinery, safety, feeling the state of a factory, of your equipment, that's one place it'll go right away. CA: David Eagleman, that was one mind-blowing talk. Thank you very much. DE: Thank you, Chris. (Applause)"	我們由極小的物質所構成，我們生活在一個巨大的宇宙中，然後實際情況是，我們並不是真正了解這個世界無論是大型或微型的世界，這是因為我們的大腦還沒有發達到理解這個世界有多大。 相反的，我們被局限在這個狹小的感知範圍中。但是奇怪的是，即使像我們撥電話回家這樣的事，這其中大部分的動作我們都沒有看清是怎麼一回事。看看我們所處世界的色彩。它是由物體反射的光波，電磁波。射線直接擊中位於我們眼球後部的一個專門接收部位。但是我們看不到所有的波。事實上，我們看到的不到 10 兆分之一。所以，你們現在正被無數的無線電波、微波、X 光、伽馬射線穿過身體卻渾然不知。因為人類沒有適當的生物感覺器官來感知這些波。此時，數以萬計的通話訊號正從你身邊通過，你完全看不見這些波。 這並不是因為這些波本身不可見。蛇可以看到某些紅外線，蜜蜂可以看到紫外線，我們可以在汽車儀表板內安裝裝置，來接收無線電波的信號，醫院裡使用相關設備來接收 X 光。我們無法以五官去感受這些波，至少在目前階段無法做到，因為你的身體沒有適當的裝備來接收它們。 這意味著我們對於真實世界的感知力受限於我們本身的生物構造，這有違常理，若我們的眼睛、耳朵、指尖只能感知到我們周圍客觀存在的資訊。相反地，我們的大腦只在這其中揀取了很小一部分。 在整個動物王國，不同的動物會感知不一樣的實況。在蝨子黑暗無聲的世界裡，溫度和丁酸是重要信號；在黑魔鬼刀魚的世界裡，電磁場 構建了其色彩斑斕的感官世界；而對於靠迴聲定位的蝙蝠，空氣壓縮波 構建了它們的世界。這就是牠們生態系統構造特點讓他們獲取片斷的信息，在科學領域有一個詞叫做「客觀世界」(umwelt），這是一個德語詞彙，意指周圍的環境。或許，每一種動物都假設牠的「周圍環境」就是牠的整個客觀存在的世界，因為動物一般都不會無端端地想在我們感知範圍外，還有其他的東西存在。相反的，我們只接受實際環境呈現給我們的訊息。 讓我們提高這方面的意識。想像你是一隻警犬。你的整個世界是聞氣味。你有一個長長的鼻腔，裡面密布 2 億個嗅覺接收器。你濕潤的鼻子用以接收和捕捉氣味，你的鼻孔甚至有縫隙讓你可以大口吸氣。與你而言，一切東西都是氣味。某一天，你停下腳步，突然想到你望著你的主人，在想：人類擁有一個可憐又沒特點的鼻子那會是個什麼情況啊？（笑聲）如果你微弱地嗅著空氣，會是怎麼樣？你怎能不知道100 碼以外有隻貓，或 6 小時前你的鄰居在這呆過？（笑聲） 因為我們是人，從來沒有感知過這樣的嗅覺世界，所以我們也不覺得遺憾，因為我們堅守地生活在自己的環境。但問題是，我們是否必須困在此環境中？身為一個神經學家，我對運用某種科技很感興趣，可以用它來拓展我們的環境，以及如何改變人類的感知和經歷。 我們都知道生物學和科技可以結合在一起，因為已經有成千上萬的人透過人工耳人工視覺在生活。方法是，拿著一個麥克風，把信號數位化，將一個電極帶植入內耳。在視網膜植入一個攝像頭把信號數位化，然後插入一個電極柵直接傳導進入視神經。就在 15 年前，很多科學家認為這些技術不可行。為什麼呢？因為這些高科技只是矽谷的方法，它們和生物學的各個器官不完全相同。但事實上這是可行的；只要大腦知道怎麼運用這些信號就可以了。 但是我們怎麼讀懂它們的語言呢？這裡有個秘密：大腦本身其實根本就看不見也聽不懂任何東西。大腦如同被你的腦殼封閉在一個無光無聲的世界。大腦所看到的是電化學訊號這些訊號來自不同的資料傳輸源。這就是大腦要處理的東西，別無其它。令人驚訝的是，大腦真的很善於獲取這些訊號，萃取其模式，以及它們所代表的含意。將內心世界的元素組合在一起，構建了你的主觀世界。 但是這個關鍵在於：你的大腦並不知道，也不在乎這些訊號從何而來。資訊進來了，大腦就做對應的處理，就如同一台高效運轉的機器。這本質上就是一台一般用途為目標的計算設備，接收每一個訊息，然後分析做出相對應的回應，然後，釋放出自然的天性修補來自不同的輸入通道。 我將之命名為「PH 演化模式」，我不想太著重於技術層面，PH 代表「馬鈴薯頭」(Potato Head)，我採用這個名字是為了強調，所有的感知受體像是我們所知所愛的眼睛、耳朵、手指，這些僅僅是周邊隨插即用的設備：插上它們，即可使用。大腦會根據進來的資訊決定下一步做什麼。當你綜覽動物的世界，你會看到許多這樣的周邊設備。蛇有感受熱能的小洞用來探測紅外線魔鬼刀魚有視網膜電圖，北美星鼻鼹鼠擁有 22 個指頭牠感受並建構三度空間立體世界，許多鳥類擁有磁感應所以牠們能夠透過地球的磁場確定方向。這意味著大自然不需持續重新設計大腦。相反地，大腦運作基本原理的建立，大自然只需要設計新的周邊設備。 好。這意味著：它所呈現的結果是關於我們所討論的生物學沒有牽涉到特別的東西或原理。這只是我們從複雜的進化過程繼承來的。但這不是我們要去堅持的，最好的證明原理來自所謂的感官替代。透過獨特的感官通道，提供訊息給大腦，大腦就做相對應的處理。 或許聽起來頗具推測性，但這是第一篇相關論證的文章，發表於 1969 年的自然雜誌。一位叫做保羅·巴赫·瑞塔的科學家把盲人置於改裝過的牙科椅，他裝設一台錄影設備，在攝影機前放某個東西，你可以感覺到那東西從背後插入一個電磁開關電網。所以如果你在攝影機前擺動咖啡杯，你的背部可以感受到它，令人訝異的是，盲人相當擅長於判定攝影機前的東西只透過背部的一小部分就感受它。現在，有許多這種現代化的概念。聲波眼鏡在你眼前錄影再將影像轉變為聲景，就像事物環繞，走近，走遠，聽起來就像「吱吱，吱吱，吱吱。」聽起來刺耳，但幾週之後，盲人就很擅長依據他所聽到的來理解他面前的事物。而且不必透過耳朵：這個系統使用前額上的電網格，不管前面拍攝到什麼影像，你的前額可以感應到。為什麼用前額？因為你很少使用前額做其他事。 最為現代的儀器稱為「腦端口」(brainport)，這是一個小的電網格安裝在舌頭上，將影像轉變成小的電觸覺訊號，盲人很擅長使用這個設備能將球投進籃子，或是可以在複雜的障礙賽跑訓練場行走。他們透過舌頭看見東西。這聽起來相當瘋狂，對吧？但是記住，所有看見的東西是流過大腦的電化學訊號。你的大腦不知道訊號來自何處。大腦只做相對應的處理。 我在實驗室為聾人做感官替代的研究，這是我和一位研究生史考特.諾維奇所做的專案，他的論文在這方面有領先的成果。以下是我們想要做的：我們要讓來自外界的聲音以某種方式進行轉變讓聾人可以聽懂他人說的話。為提供普遍性高的可擕式設備我們想要這麼做，要確定這些功能可以在手機和平板電腦上執行，我們也想把它做成穿戴式電子裝置，一件可以穿在裡面的裝置。就是這個概念。如我所說的，平板電腦接收到我的聲音，然後對映對到有振動馬達的背心，就如你手機的馬達。當我講話時，聲音被轉換為背心上的震動模式這不只一種概念：平板傳送藍牙訊號而且我正穿著這件背心。當我講話時 — （掌聲）—聲音被轉變成震動的動能模式。我感覺到周圍的有聲世界。 我們正和聾人做這樣的測試，結果在很短的時間之後，聾人開始感覺，開始理解背心的語言。 這是強納生，37 歲，擁有碩士學位。他天生全聾，那表示有一部分的環境他無法感受到，所以我們讓強納生穿上背心訓練 4 天，每天 2 小時。這是第 5 天. 史考特·諾維奇：你。 大衛.伊葛門：史考特說一個字，強納生透過背心感受到了，他把它寫在板子上。 史考特：那裡。那裡。 大衛：強納生能夠將複雜的震動模式翻譯成能理解的語言。 史考特：摸。摸。 大衛：他沒做這個 —(掌聲）—強納生不是有意識去做這個，因為模式太過複雜，但他的大腦正在開啟這個模式以理解這些資料的意義，我的期望是，穿上這件背心三個月之後，他能直接感受聽覺的經驗與盲人點字相同的經驗，沒有任何意識干預下，也可以馬上理解符號的意義。這項技術有改變遊戲規則的潛力，因為幫助聾人的另一個唯一的方法是植入人工電子耳，人工電子耳是一種侵入性手術。而且我說的這項技術比人工電子耳便宜 40 倍，這技術打入全球市場，最為貧窮國家的人民也可行。 我們受到感官替代成果的鼓舞，我們一直在思考許多有關感官附加的技術。我們如何使用這樣的技術去增加一種全新的感受，去發展人類的環境？例如，我到們能否從網路獲得即時資料再將資料提供給大腦？他們能夠直接產生感知經驗嗎？ 這是我們實驗室正在做的一個實驗。一個受試者正在感受網路的即時串流資料有 5 秒的時間。然後，出現二個按鈕，他必需做選擇。他不知道發生什麼事。他做了選擇，一秒之後，他得到反饋。是這樣的：受試者不知道所有模式的意義，我們要得知，他是否知道要對按按鈕做較好的選擇。他不知道我們輸入的是股市的即時資料，他做買入或賣出股票的決策。（笑聲）反饋會告訴他是否做對了。我們看到的是，我們能否拓展人類的環境以便他在幾週之後能夠有一個全球經濟活動的感知經驗。我們稍後會向各位報告這個實驗的進展狀況。（笑聲） 以下是我們做的另一個實驗：在今早的演講中，我們一直自動蒐集推特TED2015 的索引標籤。我們也在做自動情感分析，意即，人們正在用正面、負面，還是中性的語詞？當在進行這個實驗時，我已感受到，我已被連接到上千人所聚集的即時情感，那是一種全新的人類經驗，因為現在我知道每個人的情況如何，以及你們是多麼喜歡這演講。（笑聲）（掌聲）這比人類一般正常的體驗還多。 我們也在拓展飛行員的環境。在這個例子中，背心分流出 9 種不同來自直昇機的測量方式，傾斜、偏航、翻滾、定位、前進，提高飛行員的駕駛能力。這很像是把皮膚延伸到很遠的地方。 這只是個開端。我們想像的是一台充滿儀表的現代化駕駛艙不需要去讀取儀表板數據而是直接去感受到它。我們生活在一個資訊化的世界，存取大數據 和 感受數據是不同的。 所以我想在拓展人類的眼界方面有無限的可能性。想想一個太空人能夠感受到整個國際太空站的生命力，或是，你可以感受到你覺察不出的自己的健康狀況，像是你的血糖和微生物狀況或是擁有 360 度視角或能看見紅外線或紫外線。 所以關鍵點在於，當我們踏入未來，我們漸漸地能夠選擇我們的周圍設備。不需要再等待大自然的時間表賦與我們感知的禮物，然而，就像是好父母一樣，她已經給了我們所需的工具走出來定義我們自己的方向。問題是，你想如何走出來感受你的世界？ 謝謝。 （掌聲） 克里斯.安德森：你能感受到嗎？大衛.伊葛門：可以。 事實上，這是我第一次透過背心感受掌聲。很棒，就像在按摩。（笑聲） 克里斯.安德森：推特網友太瘋狂了。那個股票市場的實驗。這個實驗可能是資金募集的永久保證，對嗎，如果能成功的話？ 大衛.伊葛門：對的，我不再向國家衛生研究所寫申請表。 克里斯.安德森：我提出一些質疑，我的意思是，但到目前為止是不是大部分的實驗證明感官代替是可行的，但感官附加卻不一定可行？我的意思是，盲人可以透過舌頭看東西是不是因為視覺皮質層還在那裡準備運作，而那就是其中必要的一部分？ 大衛.伊葛門：好問題。事實上我們也不知道大腦能接收何種資料這個理論的局限是什麼。一般情況，大腦非常靈活當一個人變成盲人之後，用來傳達視覺皮質層訊息的任務被觸覺、聽覺、字彙所取代。這告訴我們皮質層像是一種單一功能的小東西。它只能按照特定的計算方式運作。例如布拉耶點字聾人透過手指取得訊息。我不認為有任何理由在我們所知的條件下存在任何理論上的限制。 克里斯.安德森：如果這理論可行，各類應用將蜂擁而至。可以運用這技術的地方太多了。你準備好了嗎？最讓你興奮的是什麼？有進展的方向嗎？大衛.伊葛門：我想有許多應用除了我在開始時所提到的感知替代關於太空站的太空人，他們花很多時間監測訊息；有了這個技術，他們能夠理解進展，因為這有利於取得多面向的資料。關鍵是：我們的視覺系統擅長偵測塊狀和邊緣，但不擅長觀察世界所呈現的樣子一個呈現非常大量資料的世界。用我們的注意力系統匍匐前進。這是感知事物的一個方式，就像當你站立時你知道你身體的狀態一樣。我認為重型機械，安全性感覺工廠設備的狀態，那即將實現。 克里斯.安德森：大衛.伊葛門，這是一個令人振奮的演講。非常感謝。 大衛.伊葛門：謝謝你，克里斯（掌聲）
