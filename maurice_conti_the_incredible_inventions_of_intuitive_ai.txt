Talk	en	zh-tw
maurice_conti_the_incredible_inventions_of_intuitive_ai	"How many of you are creatives, designers, engineers, entrepreneurs, artists, or maybe you just have a really big imagination? Show of hands? (Cheers) That's most of you. I have some news for us creatives. Over the course of the next 20 years, more will change around the way we do our work than has happened in the last 2,000. In fact, I think we're at the dawn of a new age in human history. Now, there have been four major historical eras defined by the way we work. The Hunter-Gatherer Age lasted several million years. And then the Agricultural Age lasted several thousand years. The Industrial Age lasted a couple of centuries. And now the Information Age has lasted just a few decades. And now today, we're on the cusp of our next great era as a species. Welcome to the Augmented Age. In this new era, your natural human capabilities are going to be augmented by computational systems that help you think, robotic systems that help you make, and a digital nervous system that connects you to the world far beyond your natural senses. Let's start with cognitive augmentation. How many of you are augmented cyborgs? (Laughter) I would actually argue that we're already augmented. Imagine you're at a party, and somebody asks you a question that you don't know the answer to. If you have one of these, in a few seconds, you can know the answer. But this is just a primitive beginning. Even Siri is just a passive tool. In fact, for the last three-and-a-half million years, the tools that we've had have been completely passive. They do exactly what we tell them and nothing more. Our very first tool only cut where we struck it. The chisel only carves where the artist points it. And even our most advanced tools do nothing without our explicit direction. In fact, to date, and this is something that frustrates me, we've always been limited by this need to manually push our wills into our tools — like, manual, literally using our hands, even with computers. But I'm more like Scotty in ""Star Trek."" (Laughter) I want to have a conversation with a computer. I want to say, ""Computer, let's design a car,"" and the computer shows me a car. And I say, ""No, more fast-looking, and less German,"" and bang, the computer shows me an option. (Laughter) That conversation might be a little ways off, probably less than many of us think, but right now, we're working on it. Tools are making this leap from being passive to being generative. Generative design tools use a computer and algorithms to synthesize geometry to come up with new designs all by themselves. All it needs are your goals and your constraints. I'll give you an example. In the case of this aerial drone chassis, all you would need to do is tell it something like, it has four propellers, you want it to be as lightweight as possible, and you need it to be aerodynamically efficient. Then what the computer does is it explores the entire solution space: every single possibility that solves and meets your criteria — millions of them. It takes big computers to do this. But it comes back to us with designs that we, by ourselves, never could've imagined. And the computer's coming up with this stuff all by itself — no one ever drew anything, and it started completely from scratch. And by the way, it's no accident that the drone body looks just like the pelvis of a flying squirrel. (Laughter) It's because the algorithms are designed to work the same way evolution does. What's exciting is we're starting to see this technology out in the real world. We've been working with Airbus for a couple of years on this concept plane for the future. It's a ways out still. But just recently we used a generative-design AI to come up with this. This is a 3D-printed cabin partition that's been designed by a computer. It's stronger than the original yet half the weight, and it will be flying in the Airbus A320 later this year. So computers can now generate; they can come up with their own solutions to our well-defined problems. But they're not intuitive. They still have to start from scratch every single time, and that's because they never learn. Unlike Maggie. (Laughter) Maggie's actually smarter than our most advanced design tools. What do I mean by that? If her owner picks up that leash, Maggie knows with a fair degree of certainty it's time to go for a walk. And how did she learn? Well, every time the owner picked up the leash, they went for a walk. And Maggie did three things: she had to pay attention, she had to remember what happened and she had to retain and create a pattern in her mind. Interestingly, that's exactly what computer scientists have been trying to get AIs to do for the last 60 or so years. Back in 1952, they built this computer that could play Tic-Tac-Toe. Big deal. Then 45 years later, in 1997, Deep Blue beats Kasparov at chess. 2011, Watson beats these two humans at Jeopardy, which is much harder for a computer to play than chess is. In fact, rather than working from predefined recipes, Watson had to use reasoning to overcome his human opponents. And then a couple of weeks ago, DeepMind's AlphaGo beats the world's best human at Go, which is the most difficult game that we have. In fact, in Go, there are more possible moves than there are atoms in the universe. So in order to win, what AlphaGo had to do was develop intuition. And in fact, at some points, AlphaGo's programmers didn't understand why AlphaGo was doing what it was doing. And things are moving really fast. I mean, consider — in the space of a human lifetime, computers have gone from a child's game to what's recognized as the pinnacle of strategic thought. What's basically happening is computers are going from being like Spock to being a lot more like Kirk. (Laughter) Right? From pure logic to intuition. Would you cross this bridge? Most of you are saying, ""Oh, hell no!"" (Laughter) And you arrived at that decision in a split second. You just sort of knew that bridge was unsafe. And that's exactly the kind of intuition that our deep-learning systems are starting to develop right now. Very soon, you'll literally be able to show something you've made, you've designed, to a computer, and it will look at it and say, ""Sorry, homie, that'll never work. You have to try again."" Or you could ask it if people are going to like your next song, or your next flavor of ice cream. Or, much more importantly, you could work with a computer to solve a problem that we've never faced before. For instance, climate change. We're not doing a very good job on our own, we could certainly use all the help we can get. That's what I'm talking about, technology amplifying our cognitive abilities so we can imagine and design things that were simply out of our reach as plain old un-augmented humans. So what about making all of this crazy new stuff that we're going to invent and design? I think the era of human augmentation is as much about the physical world as it is about the virtual, intellectual realm. How will technology augment us? In the physical world, robotic systems. OK, there's certainly a fear that robots are going to take jobs away from humans, and that is true in certain sectors. But I'm much more interested in this idea that humans and robots working together are going to augment each other, and start to inhabit a new space. This is our applied research lab in San Francisco, where one of our areas of focus is advanced robotics, specifically, human-robot collaboration. And this is Bishop, one of our robots. As an experiment, we set it up to help a person working in construction doing repetitive tasks — tasks like cutting out holes for outlets or light switches in drywall. (Laughter) So, Bishop's human partner can tell what to do in plain English and with simple gestures, kind of like talking to a dog, and then Bishop executes on those instructions with perfect precision. We're using the human for what the human is good at: awareness, perception and decision making. And we're using the robot for what it's good at: precision and repetitiveness. Here's another cool project that Bishop worked on. The goal of this project, which we called the HIVE, was to prototype the experience of humans, computers and robots all working together to solve a highly complex design problem. The humans acted as labor. They cruised around the construction site, they manipulated the bamboo — which, by the way, because it's a non-isomorphic material, is super hard for robots to deal with. But then the robots did this fiber winding, which was almost impossible for a human to do. And then we had an AI that was controlling everything. It was telling the humans what to do, telling the robots what to do and keeping track of thousands of individual components. What's interesting is, building this pavilion was simply not possible without human, robot and AI augmenting each other. OK, I'll share one more project. This one's a little bit crazy. We're working with Amsterdam-based artist Joris Laarman and his team at MX3D to generatively design and robotically print the world's first autonomously manufactured bridge. So, Joris and an AI are designing this thing right now, as we speak, in Amsterdam. And when they're done, we're going to hit ""Go,"" and robots will start 3D printing in stainless steel, and then they're going to keep printing, without human intervention, until the bridge is finished. So, as computers are going to augment our ability to imagine and design new stuff, robotic systems are going to help us build and make things that we've never been able to make before. But what about our ability to sense and control these things? What about a nervous system for the things that we make? Our nervous system, the human nervous system, tells us everything that's going on around us. But the nervous system of the things we make is rudimentary at best. For instance, a car doesn't tell the city's public works department that it just hit a pothole at the corner of Broadway and Morrison. A building doesn't tell its designers whether or not the people inside like being there, and the toy manufacturer doesn't know if a toy is actually being played with — how and where and whether or not it's any fun. Look, I'm sure that the designers imagined this lifestyle for Barbie when they designed her. (Laughter) But what if it turns out that Barbie's actually really lonely? (Laughter) If the designers had known what was really happening in the real world with their designs — the road, the building, Barbie — they could've used that knowledge to create an experience that was better for the user. What's missing is a nervous system connecting us to all of the things that we design, make and use. What if all of you had that kind of information flowing to you from the things you create in the real world? With all of the stuff we make, we spend a tremendous amount of money and energy — in fact, last year, about two trillion dollars — convincing people to buy the things we've made. But if you had this connection to the things that you design and create after they're out in the real world, after they've been sold or launched or whatever, we could actually change that, and go from making people want our stuff, to just making stuff that people want in the first place. The good news is, we're working on digital nervous systems that connect us to the things we design. We're working on one project with a couple of guys down in Los Angeles called the Bandito Brothers and their team. And one of the things these guys do is build insane cars that do absolutely insane things. These guys are crazy — (Laughter) in the best way. And what we're doing with them is taking a traditional race-car chassis and giving it a nervous system. So we instrumented it with dozens of sensors, put a world-class driver behind the wheel, took it out to the desert and drove the hell out of it for a week. And the car's nervous system captured everything that was happening to the car. We captured four billion data points; all of the forces that it was subjected to. And then we did something crazy. We took all of that data, and plugged it into a generative-design AI we call ""Dreamcatcher."" So what do get when you give a design tool a nervous system, and you ask it to build you the ultimate car chassis? You get this. This is something that a human could never have designed. Except a human did design this, but it was a human that was augmented by a generative-design AI, a digital nervous system and robots that can actually fabricate something like this. So if this is the future, the Augmented Age, and we're going to be augmented cognitively, physically and perceptually, what will that look like? What is this wonderland going to be like? I think we're going to see a world where we're moving from things that are fabricated to things that are farmed. Where we're moving from things that are constructed to that which is grown. We're going to move from being isolated to being connected. And we'll move away from extraction to embrace aggregation. I also think we'll shift from craving obedience from our things to valuing autonomy. Thanks to our augmented capabilities, our world is going to change dramatically. We're going to have a world with more variety, more connectedness, more dynamism, more complexity, more adaptability and, of course, more beauty. The shape of things to come will be unlike anything we've ever seen before. Why? Because what will be shaping those things is this new partnership between technology, nature and humanity. That, to me, is a future well worth looking forward to. Thank you all so much. (Applause)"	你們有多少人是創意人、設計師、工程師、企業家、藝術家？或者你只是有無遠弗屆的想像力？請舉一下手？（歡呼聲） 現場大部分人都是。我有一些消息要給我們的創意人。接下來的 20 年，很多我們工作的方式，將會遠遠不同於過去的 2000 年。實際上，我認為我們正處在人類歷史新世代的黎明。 人類工作的方式，有四個主要的歷史階段。人類歷經了幾百萬年的狩獵採集時代。然後經歷了幾千年的農業時代。工業時代則延續了幾世紀。而目前的資訊時代才走了幾十年。如今，身為人類的我們，即將邁入下一個偉大的時代。 歡迎來到「擴增時代」。在這個新時代，人類天生的能力將會被強化擴增，電腦計算系統將幫助你思考、機械人系統協助你製造、遠超過你自然感官強度的數位神經系統，能夠讓你與全世界接軌。我們先從「認知擴增」談起。現場有多少人是「強化的半機械人」？ （笑聲） 其實我想說的是，我們都已經被強化、擴增了。想像你正在參加一場派對，有人問了你一個你不知道如何回答的問題。如果你有這個，只要幾秒鐘，你就會得到答案。但這也只是剛開始而已。甚至 Siri 也只是個被動工具。實際上，在過去的 350 萬年，我們所有的工具都是被動的。它們只會照我們的指令去做，僅此而已。我們最早使用的工具，遵循一個口令一個動作的指示。藝術家指哪裡，雕刻刀就雕刻哪裡。即使最先進的工具，如果沒有我們明確的指令也不會工作。說真的，時到今日，有件事仍讓我感覺很挫敗，我們一直以來都被限制在「需要動手將我們的意念傳達給工具」的這種迷思框框中——就是得動手去做，即使有了電腦還是得靠雙手。但我還是比較喜歡當<<星際迷航>>裡的史考迪。 （笑聲） 我也想跟電腦對話。當我說，「電腦，我們來設計一輛車吧！」然後電腦就會顯示一輛車給我看。然後我說：「不，要拉風一點，德國味兒少一點。」接著「蹦」，電腦給了我一個新選擇。 （笑聲） 這樣的對話可能有點不切實際，也許沒有我們認為的那麼不切實際，但現在，我們正在做這件事。這些工具將帶領我們大躍進，從被動轉為衍生。「衍生設計工具 」是利用電腦及演算法，合成出幾何結構，產製出新的設計圖，全部都是它們自己構思出來的。你只需要設定目標及限制條件。 我給各位舉個例子。就拿這個無人機底盤為例，你唯一要做的，就是告訴它你的需求，像是，你要四個螺旋槳的，它越輕越好，空氣動力學表現效率佳的。電腦做的，就是探索所有可能的解決方案：每一個能解決且符合你標準的可能方案——有上百萬個。這需要大型電腦才能做到。但它回饋給我們的設計方案，是我們單憑自己無法想像出來的設計方案。電腦憑藉著自己的能力做出這些東西——我們人類沒有動筆畫任何東西，完全是它自己從頭、從零畫起的。順便一提，這可不是偶然......無人機的機體長的像飛鼠的骨盆， （笑聲） 那是因為演算法的計算模式，是遵循生物演化模式而設計的。 令人興奮的是，我們開始見證這樣的科技在現實世界中實現。我們與空中巴士（歐洲最大飛機製造商）合作開發未來的概念機已經好幾年了，這計畫目前還在進行。但最近，我們用了衍生設計的人工智慧做出了這一個。這是一個 3D 列印的客艙隔間板，由一台電腦所設計。它比原款式還要堅固，但重量只有原本的一半，今年稍晚，它將跟 A320 空中巴士一起飛上天。所以現在電腦會主動生成、衍生了；它們可以對界定明確的問題，給出自己的答案。但它們並不是靠直覺做事。它們還是每次都得從頭開始，因為它們不會學習。不像瑪姬。 （笑聲） 瑪姬其實比我們最先進的設計工具都還要聰明。這是什麼意思？如果狗主人拿起狗鍊，瑪姬就知道有相當的確定性，主人要帶她去散步了。她是怎麼知道的？因為，每當主人拿起狗鍊，他們就會一起去散步。瑪姬會做三件事：她必須專注、必須記得發生過什麼事、必須在腦中記憶並產生一個模式。 有趣的是，這正是電腦科學家過去 60 年來，一直嘗試要讓人工智慧做的事。回想一下 1952 年，科學家建立了這一台電腦，它會玩井字遊戲。真了不起。45 年後，1997 年，深藍擊敗了當時的西洋棋世界冠軍卡司帕洛夫，2011年，華生 (IBM電腦)在<<危機邊緣>>擊敗這兩個人，對電腦來說，這比下棋難多了。事實上，華生並不是從預先定義的題庫中來找答案，它必須使用推理來擊敗它的人類對手。就在幾個禮拜前，DeepMind 的阿爾法圍棋擊敗了世界圍棋冠軍，而圍棋是我們人類最複雜的遊戲。事實上，圍棋走法的可能性超過全宇宙的原子數量。所以為了取得勝利，阿爾法圍棋必須學會使用直覺。實際上，有些下法，阿爾法圍棋的程式人員也不懂為什麼阿爾法圍棋要那樣下。 世界變化真快。我的意思是，想像一下——在人類壽命這麼長的時間裡，電腦已經從小孩子的遊戲發展到策略思考的頂尖水平。電腦基本上的發展，已經從史巴克大副進化到寇克艦長。 （笑聲） 對吧？從純粹的邏輯運算到直覺判斷。你們會跨過這座橋嗎？大部分人應該都說，「喔，打死我也不要！」 （笑聲） 你瞬間就可以做出這個決定。你就是隱約知道那座橋並不安全。這種直覺判斷，就是目前我們深度學習系統正在發展的能力。很快的，各位就可以把你製作、設計出來的東西拿給電腦評判，然後它看完後會說，「抱歉，兄弟，這東西行不通，你再試試別的吧！」或者你可以問它，人們會不會喜歡你的新歌？或者你冰淇淋的新口味？再或者，更重要的，你可以跟電腦一起解決我們從未面臨過的問題。例如，氣候變遷問題。我們自己沒有做得很好的事，我們當然可以利用身邊各種資源來幫忙解決。這就是我接下來要談的，科技強化了我們的認知能力，所以我們可以想像並設計出，當我們還未具有強化擴增能力時所未能創造出來的東西。 那麼，製造這些我們即將發明設計的瘋狂新產品會如何呢？我認為在人類擴增的時代，現實世界及虛擬智慧領域與其皆有不分軒輊的重要相關性。科技將會如何強化我們？在現實世界，就是機械人系統。沒錯，很多人擔心，機械人會搶走人類的工作，在某些領域，確實是如此。但，我對以下的想法比較有興趣，就是，人類與機械人將會一起工作並互相強化，並開創出一種新的共生空間。 這是我們在舊金山的應用研究實驗室，我們專研的領域之一就是高階機械人，特別是人機合作的領域。這是畢夏普，我們其中的一個機器人。在實驗裡，我將它設定為在建築領域中，幫助人類做重複性的工作——比如說，在石牆上打出一個插座孔或電燈開關孔。 （笑聲） 所以，畢夏普的人類夥伴就可以用簡單的英語和手勢告訴它該做什麼，有點像是在跟狗狗說話。然後畢夏普會以完美的準確度執行人類所下達的指令。我們讓人類做人類擅長的事，像是：需要意識力、洞察力、做決策的工作。我們讓機械人做機械人擅長的事，像是：準確度及重複性的工作。 畢夏普還有另一個很酷的專案。這個專案的目標，我們稱它為<<蜂巢>>，主要目標是把人類、電腦、機械人的經驗結合起來，一起工作解決極複雜的設計問題。人類的工作是在建築基地巡邏監工並熟練地操作竹子——順便一提，因為每一根竹子的材料性質都不一樣，所以機械人操作起來非常困難。但機械人做的是彎曲竹子的纖維，這種事人類幾乎做不來。然後我們讓一台人工智慧來控制所有的東西。它會告訴人類要做什麼，告訴機械人要做什麼，並且對成千上萬個部件進行持續的追蹤。有趣的是，要建造出這樣的亭狀建築物，如果沒有人類、機械、人工智慧的互補強化，根本不可能做得出來。 好，我再分享一個專案，這個有點瘋狂。我們與阿姆斯特丹的藝術家尤爾斯‧拉曼和他的 MX3D 團隊，正使用衍生性設計與機械列印的方式，打造世界第一座機械人自造的橋梁。所以，就在我們談話的這一刻，尤爾斯正和人工智慧一起在阿姆斯特丹設計這座橋梁。等他們設計完成後，我們就會按下「啟動」開關，讓機械人開始用不鏽鋼 3D 列印出橋梁，在沒有人類的介入幫忙下，它們會持續地列印直到橋樑完工為止。 所以，電腦將強化我們的想像及設計新事物的能力，機械人系統將協助我們製造我們以前無法製造的東西。但是我們感知和控制這些東西的能力呢？我們製成東西的神經系統又如何呢？ 我們的神經系統，人類的神經系統，可以告訴我們周遭發生的每一件事。但這些東西的神經系統，最多只能算「尚未成熟」。比如說，車輛本身不會主動通告市政府的工部門，說它在經過百老匯和莫里森轉角口時撞到水坑。建築物本身不會告知它的設計師，裡面的居民是否喜歡住在那裏，玩具製造商也不知道他們的玩具現在是跟誰在玩、在哪玩、是不是玩的很開心。我確定設計師在設計芭比時，一定想像過芭比的生活方式。 （笑聲） 但要是芭比變的很孤單怎麼辦？ （笑聲） 如果設計師知道他們設計的東西，在真實世界裡發生了什麼事，像是道路、建築物、芭比——那他們就可以運用所獲得的訊息，為使用者創造出更好的使用體驗。我們欠缺的就是一個可以連結所有我們設計、製造、使用事物的神經系統。如果大家在真實世界，能收到自己創造的東西所回饋的資訊，那會如何呢？所有我們製造的東西，我們花了很多錢跟精力 ──實際上光是去年，大約就有兩兆美金 ──去說服人們購買我們製造的東西。但如果你所設計製造出來的東西能連結傳送給你回饋的訊息，不管是在它們上市以後，或是在賣出或發表以後，我們就可以改變既有的銷售模式，從說服人們來購買我們的產品，轉變成我們第一時間就做出人們真正需要的東西。 好消息是，我們正在研發的這套數位神經系統能連結我們與我們所設計的產品。我們與在洛杉磯的邦帝圖兄弟公司和他們的團隊正合作進行一個專案。這幾個人做的其中一件事就是製造「瘋狂賽車」，他們做的東西真的很瘋狂。這些人真的是瘋了── （笑聲） 不過是用最厲害的方式。我們跟他們一起合作的模式，就是將傳統的賽車底盤安裝神經系統。 所以，我們在底盤安裝了好幾組感應器，然後請一位世界級車手來駕駛，把車送到沙漠連續開它個一禮拜。之後車子的神經系統就能紀錄到車子發生的所有反應。我們抓到了 40 億個資料點；所有底盤所承受的壓力數據。然後我們做了一些瘋狂的事。我們把所有的資料連接到一個叫做「捕夢者」的衍生設計人工智慧上 。所以當你把神經系統安裝到設計工具上，並請它幫你建造一個終極汽車底盤時，你會得到什麼？你會得到這個。這是人類永遠無法設計出的東西。如果真有人這樣設計過，那個人一定也是透過衍生設計的人工智慧強化、數位神經系統的強化、和機械人一起合作，才做得出來的東西。 所以，如果擴增時代就是我們的未來，而我們的認知、體格、知覺都將被強化、擴增，那會是怎樣的世界？那會是個什麼樣的美麗新世界？ 我認為我們即將見證這麼一個世界，一個東西從製造出來的變成「種 」出來的世界。一個東西從建造出來的變成自己「長 」出來的世界。我們將從自我隔離轉變成相互交流。我們也將從奪取者變成相互擁抱的給予者。我也認為，我們將會從冀望產品順從我們的指令，轉變成重視其自主性。 由於我們的擴增強化能力，我們的世界將會有劇烈的變化。我們的世界會變得更多元、更加連通、更有活力、更多複雜的變化、更有適應力、當然也會更美麗。未來世界的雛型是我們前所未見的。為什麼？因為形塑這個世界的將會是科技、自然與人類的新結盟關係。對我而言，那樣的未來是值得我們期待的。 非常感謝各位。 （掌聲）
