{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "#### Charlie Liou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import re, string\n",
    "import os, glob, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of this is adapted from Kevin Knight's [A Statistical MT Tutorial Workbook.](http://www.isi.edu/natural-language/mt/wkbk.rtf)\n",
    "\n",
    "To write a statistical English to Chinese translator, we will consider English sentences $e$ and Chinese sentences $c$. This may seem like solving the opposite problem, but given $c$, we seek the English sentence that will maximize $P( e \\mid  c)$ (that is, the most likely English sentence when given the Chinese sentence.) This reversal will be explained later. More formally, we are trying to find the english sentence $\\vec{e}$ that satisfies\n",
    "\n",
    "$$\\vec{e} = arg\\hspace{0.05cm} max\\hspace{0.05cm}P(e \\mid c)$$\n",
    "\n",
    "How do we approach maximizing the probability $P(e \\mid  c)?$ Given that we know Bayes' Theorem:\n",
    "\n",
    "$$P( e \\mid  c) = \\dfrac{P(e)\\hspace{0.05cm}P(c\\mid e)}{P(c)}$$\n",
    "\n",
    "the above equation becomes\n",
    "\n",
    "$$\\vec{e} = arg\\hspace{0.05cm} max\\hspace{0.05cm}P(e \\mid c) = arg\\hspace{0.05cm} max\\hspace{0.05cm}P(e)\\hspace{0.05cm}P(c \\mid e)$$\n",
    "\n",
    "This is pure Bayesian reasoning; think of the given Chinese sentence $c$ as a crime scene. Knight gives a good analogy for this: $e$ is the person who did the crime, $P(e)$ is the description of the person, and $P(c \\mid e)$ is how they did it. There are possibly many people who fit the description of $P(e)$ but those people may not have the means of committing the crime. Likewise, there are possibly many people who have the means $P(c \\mid e)$ of committing the crime but they don't fit the personality. We're trying to solve for the person who is most likely to commit the crime and has the means to commit it.\n",
    "\n",
    "Now if we think about translating language itself, accurate syntax and translations are necessary. We can't have one without the other. In our last equation, $P(e)$ is equivalent to correct syntax and $P(c\\mid e)$ is equivalent to correct translations. This is why we must maximize the probability $P(e \\mid c)$. Maximizing this probability is equivalent to finding sentences that have correct syntax and accurate translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "To account for syntax, meaning finding values of $P(e)$, we use a *n*-gram model. I will find a trigram using data from  `casia2015_en`, TED talks, NLTK's Brown corpus, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#my personal computer\n",
    "#path = \"C:\\\\Users\\\\chuck189\\\\Desktop\\\\Cal Poly Summer Research 2017\\\\data\"\n",
    "\n",
    "#math lounge computer\n",
    "path = \"/Users/csmuser/Desktop/Cal Poly Summer Research 2017/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for *n*-gram processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for merging a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge_dicts(dict_args):\n",
    "    result = {}\n",
    "    for dictionary in dict_args:\n",
    "        result.update(dictionary)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function prepares any list of English sentences for *n*-gram processing. It cleans up punctuations among other actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_eng(l, conts, nopunct, exclude, e):\n",
    "    '''\n",
    "    Takes the casia2015_en.txt file and removes unnecessary punctuations. Accounts\n",
    "    for contractions. Replaces %'s with <%> and references to money with <$>\n",
    "    '''\n",
    "\n",
    "    money = \"<$>\"\n",
    "    percent = \"<%>\"\n",
    "    start = \"<s>\"\n",
    "    stop = \"</s>\"\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        \n",
    "        #####remove punctuations except - ! . ?\n",
    "        #####accounts for many apostrophe edge cases including possession\n",
    "        \n",
    "        l[i] = l[i].strip()\n",
    "        \n",
    "        if (e[0] in l[i]) or (e[1] in l[i]) or (e[2] in l[i]) or (e[3] in l[i]) or (e[4] in l[i]) \\\n",
    "            or (e[5] in l[i]) or (e[6] in l[i]) or (e[7] in l[i]) or (e[8] in l[i]) or (e[9] in l[i]) \\\n",
    "            or (e[10] in l[i]) or (e[11] in l[i]) or (e[12] in l[i]) or (e[13] in l[i]) or (e[14] in l[i]) \\\n",
    "            or (e[15] in l[i]) or (e[16] in l[i]) or (e[17] in l[i]): \n",
    "            for item in exclude:\n",
    "                if item is \"\\'\":\n",
    "                    indexes = [m.start() for m in re.finditer(\"\\'\", l[i])]\n",
    "                    num = 0\n",
    "                    if len(l[i]) in indexes:\n",
    "                        l[i] = l[i][len(l[i]) - 1]\n",
    "                        indexes.remove(len(l[i]))\n",
    "                    if 0 in indexes:\n",
    "                        l[i] = l[i][1:]\n",
    "                        indexes.remove(0)\n",
    "                        if 1 in indexes:\n",
    "                            l[i] = l[i][1:]\n",
    "                            indexes.remove(1)\n",
    "                    for index in indexes:\n",
    "                        if (e[14] in l[i]) or (e[15] in l[i]):\n",
    "                            continue\n",
    "                        t = l[i][index - 2 - num: index - num]\n",
    "                        if (t != \"he\") and (t != \"He\") and (t != \"We\") and (t != \"we\") and (t != \"It\") and \\\n",
    "                        (t != \"it\") and (t != \"So\") and (t != \"so\") and (t != \"ho\") and \\\n",
    "                        (index - num != 0) and (index - num != len(l[i])):\n",
    "                            l[i] = l[i][:index - num] + l[i][index + 1 - num:]\n",
    "                            num += 1\n",
    "                else:\n",
    "                    l[i] = l[i].replace(item, \"\")\n",
    "        else:\n",
    "            l[i] = \"\".join(ch for ch in l[i] if ch not in exclude)\n",
    "           \n",
    "        #####replace contractions w/ correct form\n",
    "        \n",
    "        j = l[i].strip()\n",
    "        for x in nopunct:\n",
    "            if x in l[i]:\n",
    "                l[i] = l[i].replace(x, conts[nopunct[x]])\n",
    "        \n",
    "        #####deal with - and --\n",
    "        \n",
    "        if \" - \" in l[i]:\n",
    "            indexes = [m.start() for m in re.finditer(\" - \", l[i])]\n",
    "            num = 0\n",
    "            for index in indexes:\n",
    "                l[i] = l[i][:index - num] + l[i][index + 2 - num:]\n",
    "                num += 2\n",
    "                    \n",
    "        elif \"--\" in l[i]:\n",
    "            indexes = [m.start() for m in re.finditer(\"--\", l[i])]\n",
    "            num = 0\n",
    "            for index in indexes:\n",
    "                l[i] = l[i][:index - num] + l[i][index + 2 - num:]\n",
    "                num += 2\n",
    "                \n",
    "        #####replace $__ with <$>, \n",
    "        \n",
    "        if \"$\" in l[i]:\n",
    "            indexes = [m.start() for m in re.finditer(\"\\$\", l[i])]\n",
    "            nums = [m.end() for m in re.finditer(\"\\$[-+]?([0-9]*?\\.?[\\s][0-9]+|[0-9]+)\", l[i])]\n",
    "            if len(indexes) == len(nums):\n",
    "                num = 0\n",
    "                for j in range(len(indexes)):\n",
    "                    l[i] = l[i][:indexes[j] - num] + money + l[i][nums[j] - num:]\n",
    "                    num += nums[j] - indexes[j] - 3\n",
    "            else:\n",
    "                l[i] = \"\" #bad sentence\n",
    "            \n",
    "        #####replace __% with <%>\n",
    "        #####accounts for cases such as \"29. 2%\"\n",
    "        \n",
    "        if \"%\" in l[i]:\n",
    "            indexes = [m.start() for m in re.finditer(\"([0-9]*?\\.[\\s]?[0-9]+|[0-9]+)%\", l[i])]            \n",
    "            percents = [m.end() for m in re.finditer(\"%\", l[i])]\n",
    "            if len(indexes) == len(percents):\n",
    "                num = 0\n",
    "                for j in range(len(indexes)):\n",
    "                    l[i] = l[i][:indexes[j] - num] + percent + l[i][percents[j] - num:]\n",
    "                    num += percents[j] - indexes[j] - 3\n",
    "            else:\n",
    "                l[i] = \"\"\n",
    "\n",
    "                \n",
    "        l[i] = l[i].replace(\". .\", \"\").replace(\"..\", \"\").replace(\"  \", \" \")\n",
    "        l[i] = start + \" \" + start + \" \" + l[i].strip() + \" \" + stop + \" \" + stop\n",
    " \n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function splits english sentences by sentence stops (!, ?, .) while accounting for titles such as Dr., Mr., Mrs. This is necessary for `casia2015_en.txt` because many sentences are often comprised of two or more sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eng_sentence_split(l):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    start = \"<s>\"\n",
    "    stop = \"</s>\"\n",
    "    titles = \"(?<![A-z])(?<![A-Z][a-z])(?<![A-Z][a-z][a-z])\\.\"\n",
    "    k = []\n",
    "    \n",
    "    if type(l) is list:\n",
    "    \n",
    "        for i in range(len(l)):\n",
    "        \n",
    "            temp = [x.strip() for x in re.split(titles, l[i]) if x.strip() != \"\"] \n",
    "\n",
    "            if len(temp) == 1: #just 1 sentence, no .\n",
    "            \n",
    "                ex = [x.strip() for x in re.split(\"!|\\?\", temp[0]) if x.strip() != \"\"]\n",
    "            \n",
    "                if len(ex) == 1: #just 1 sentence, no . ? !\n",
    "                    j = ex[0].strip()\n",
    "                    k.append(j)\n",
    "                    #print(k[len(k)-1])\n",
    "                    #print()\n",
    "                    #print(len(k), \"one sent no . ? !\")\n",
    "                else:\n",
    "                    for sent in ex:\n",
    "                        j = sent.strip()\n",
    "                        k.append(sent)\n",
    "                        #print(k[len(k)-1])\n",
    "                        #print()\n",
    "                        #print(len(k), \">= two sent no . yes ? !\")\n",
    "            else:\n",
    "                for sent in temp: #>= 2 sentences, no .\n",
    "                \n",
    "                    ex = [x.strip() for x in re.split(\"!|\\?\", sent) if x.strip() != \"\"]\n",
    "                \n",
    "                    if len(ex) == 1:\n",
    "                        j = ex[0].strip()\n",
    "                        k.append(j)\n",
    "                        #print(k[len(k)-1])\n",
    "                        #print()\n",
    "                        #print(len(k), \"two sent yes . no ? !\")\n",
    "                    else:\n",
    "                        for sent in ex:\n",
    "                            j = sent.strip()\n",
    "                            k.append(j)\n",
    "                            #print(k[len(k)-1])\n",
    "                            #print()\n",
    "                            #print(len(k), \"two sent yes . yes ? !\")\n",
    "    elif type(l) is str:\n",
    "        \n",
    "        temp = [x.strip() for x in re.split(titles, l) if x.strip() != \"\"] \n",
    "\n",
    "        if len(temp) == 1: #just 1 sentence, no .\n",
    "            \n",
    "            ex = [x.strip() for x in re.split(\"!|\\?\", temp[0]) if x.strip() != \"\"]\n",
    "            \n",
    "            if len(ex) == 1: #just 1 sentence, no . ? !\n",
    "                j = ex[0].strip()\n",
    "                k.append(j)\n",
    "                #print(k[len(k)-1])\n",
    "                #print()\n",
    "                #print(len(k), \"one sent no . ? !\")\n",
    "            else:\n",
    "                for sent in ex:\n",
    "                    j = sent.strip()\n",
    "                    k.append(sent)\n",
    "                    #print(k[len(k)-1])\n",
    "                    #print()\n",
    "                    #print(len(k), \">= two sent no . yes ? !\")\n",
    "        else:\n",
    "            for sent in temp: #>= 2 sentences, no .\n",
    "                \n",
    "                ex = [x.strip() for x in re.split(\"!|\\?\", sent) if x.strip() != \"\"]\n",
    "                \n",
    "                if len(ex) == 1:\n",
    "                    j = ex[0].strip()\n",
    "                    k.append(j)\n",
    "                    #print(k[len(k)-1])\n",
    "                    #print()\n",
    "                    #print(len(k), \"two sent yes . no ? !\")\n",
    "                else:\n",
    "                    for sent in ex:\n",
    "                        j = sent.strip()\n",
    "                        k.append(sent)\n",
    "                        #print(k[len(k)-1])\n",
    "                        #print()\n",
    "                        #print(len(k), \"two sent yes . yes ? !\")        \n",
    "    \n",
    "    return k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates *n*-grams when n = 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram(list_, temp = {}, freqs = {}):\n",
    "    '''\n",
    "    Takes a list of strings (which are sentences) and a dictionary with phrase frequencies\n",
    "    Returns a trigram and updated phrase frequencies.\n",
    "    '''\n",
    "    \n",
    "    for sent in list_:\n",
    "        \n",
    "        sent = sent.split()\n",
    "        \n",
    "        for i in range(0, len(sent) - 2):\n",
    "            ot = sent[i] + \" \" + sent[i + 1]\n",
    "            tt = sent[i + 1] + \" \" + sent[i + 2]\n",
    "            if ot in temp:\n",
    "                if tt in temp[ot]:\n",
    "                    temp[ot][tt] += 1\n",
    "                else:\n",
    "                    temp[ot].update({tt : 1})\n",
    "            else:\n",
    "                temp.update({ot : {tt : 1}})\n",
    "    \n",
    "    for i in list(temp):\n",
    "        num = 0\n",
    "        for j in list(temp[i]):\n",
    "            num += temp[i][j]\n",
    "            if temp[i][j] in freqs:\n",
    "                freqs[i] += num\n",
    "            else:\n",
    "                freqs[i] = num\n",
    "\n",
    "    \n",
    "    return temp, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables for *n*-gram processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block pulls contractions from two different sites and a list of punctuations to remove (except sentence stops, -, $)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pulling contractions from two different sites\n",
    "c = requests.get(\"http://www.softschools.com/language_arts/grammar/contractions/contractions_list/\").content\n",
    "soup = BeautifulSoup(c, \"lxml\")\n",
    "cont = [x for x in str(soup.find_all(\"span\", class_ = \"myFont14\")[0])\n",
    "                .replace(\"<br/>\", \" \").split(\" \") if \"\\'\" in x]\n",
    "\n",
    "c1 = requests.get(\"http://grammar.wikia.com/wiki/List_of_contractions\").content\n",
    "soup1 = BeautifulSoup(c1, \"lxml\")\n",
    "cont1 = [str(x).replace(\"</td>\", \"\").replace(\"\\n\", \"\").replace(\"<td>\", \"\") \n",
    "         for x in soup1.find_all(\"td\") if \"\\'\" in str(x)]\n",
    "\n",
    "e = [\"He'll \", \" he'll \", \"We'd \", \" we'd \", \"She'll \", \" she'll \", \"We're \", \n",
    "        \" we're \", \"It's \", \" it's \", \"So's \", \" so's \", \"Who're \", \" who're \", \"'s\", \"s'\", \" we'll \", \"We'll \"]\n",
    "conts = list(chain.from_iterable([[x.ljust(len(x) + 1).capitalize(), x.center(len(x) + 2)]\n",
    "                                  for x in list(set(cont).union(cont1))]))\n",
    "[conts.append(x) for x in [\"That'll\", \" that'll \"]]\n",
    "for i in e:\n",
    "    if i in conts:\n",
    "        conts.remove(i)\n",
    "\n",
    "nopunct = merge_dicts(list(chain.from_iterable([[{conts[x].replace(\"\\'\", \"\") : x, conts[x].replace(\"\\'\", \" \") : x}] \n",
    "                                        for x in range(len(conts))])))\n",
    "\n",
    "#punctuations to remove\n",
    "sub = {\".\": \"\", \"!\": \"\", \"?\": \"\", \"-\": \"\", \"$\": \"\", \"%\": \"\"}\n",
    "rep = dict((re.escape(k), v) for k, v in sub.items())\n",
    "pattern = re.compile(\"|\".join(rep.keys()))\n",
    "exclude = set(pattern.sub(lambda m: rep[re.escape(m.group(0))], string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASIA 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first clean the `CASIA 2015` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir(path + \"/casia2015\")\n",
    "files = glob.glob(\"*.txt\")\n",
    "\n",
    "c2015entemp = process_eng(open(files[1], \"r\", encoding = \"utf-8\").read().split(\"\\n\"), conts, nopunct, exclude, e)\n",
    "c2015en = eng_sentence_split(c2015entemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*n*-gram for `casia2015_en`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2015engram, freqs = trigram(c2015en, {}, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c2015eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'It's funny\": 1}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2015engram[\"<s> 'It's\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTLK Brown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*n*-gram for NLTK Brown corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_NLTK_Brown(conts, nopunct, exclude, e):\n",
    "    from nltk.corpus import brown\n",
    "    temp = \" \".join([x for x in brown.words(categories = brown.categories())])\n",
    "    listform = eng_sentence_split(temp)\n",
    "    return process_eng(listform, conts, nopunct, exclude, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "browntemp = process_NLTK_Brown(conts, nopunct, exclude, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "browngram, freqs = trigram(browntemp, c2015engram, freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "c2015cntemp = open(files[0], \"r\", encoding = \"utf-8\").read().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表演的明星是X女孩团队——由一对具有天才技艺的艳舞女孩们组成，其中有些人受过专业的训练。 0\n",
      "表演的压轴戏是闹剧版《天鹅湖》，男女小人们身着粉红色的芭蕾舞裙扮演小天鹅。 1\n",
      "表演和后期制作之间的屏障被清除了，这对演员来说一样大有裨益。 2\n",
      "（表演或背诵时）通过暗示下面忘记或记地不准的东西来帮助某人。 3\n",
      "表演基本上很精彩--我只对她的技巧稍有意见。 4\n",
      "表演结束后，我们看到一对对车灯沿主路一路排回镇上，然后散开来各回各家。 5\n",
      "表演结束后，移走了背景墙，随后全体演员即兴邀请观众上台齐跳并排舞。 6\n",
      "表演结束后用宣纸轻铺水面，可将水面上的画进行拓印保存。 7\n",
      "表演结束后，众人期待已久的园游会终于正式开锣，美味可口的素食佳肴让大家一饱口福。 8\n",
      "表演节目丰富精采，交换礼物的欢乐时刻一到，则形成另一波高潮。 9\n",
      "表演仅仅是造就一个近乎觉察不出来的「直线的地质的移位」。 10\n",
      "表演开始十五分钟后，一帮足球运动员开始集体向着舞台上的女演员发出嘘声。 11\n",
      "表演开始时，舞台上会有一张床，一面镜子，一张椅子以及一位穿着内衣的美女。在你觉察到前，一位性感的女孩会突然变成三位。 12\n",
      "表演开始时，艺人坐在地毯上轻击盅子，徐缓起舞； 13\n",
      "表演“猫女”——在一个笼子里穿着带有一根长尾巴和猫耳的豹纹女内衣。 14\n",
      "表演前，她紧张得浑身颤抖不已。 15\n",
      "表演是他们的嫡传技艺，150多年来他们家族一直都是演员。 16\n",
      "表演算得上是一门残忍的职业，你偏离正统美越远，就越艰难。 17\n",
      "表演我软木塞哪一呼吸和我将表演你一瓶醋。 18\n",
      "表演：悉尼歌剧院首席男高音丁毅先生悉尼歌剧院首席女高音 19\n",
      "表演秀以及开园时间有可能因故不经预告而取消或中止，敬请留意。 20\n",
      "表演一个不限分析国内受损的期望往往带来了个人工作的公司。 21\n",
      "“表演”一结束，他马上给那名黑人“歹徒”发工钱，然后两个人还握手拥抱。 22\n",
      "表演艺术家MarniKotak（持玩偶者）在布鲁克林的“显微镜”画廊生下一婴作为她艺术作品的一部分。 23\n",
      "表演艺术来源于生活而高于生活，生活是艺术创作的来源，取之不尽用之不竭的源泉。 24\n",
      "表演在安慰疗法中也很重要。安慰性注射虽然比安慰药丸有效果，但是却没有虚假手术来得见效。 25\n",
      "“表演在本质上给了他们一个正当的理由去学习如何表达自己。”阿伦森说。 26\n",
      "表演者把信封靠近他的脑袋，然后先给出答案，再打开信封，读问题。 27\n",
      "表演者包括歌手马尼洛和富兰克林；歌手们星期五将向观众献上一场彩排。 28\n",
      "表演者会带来他自己的吸引力，他自己的感性，他自己的作者。 29\n",
      "表演者没有大炮火药了，于是Clifford吹气把她发射出去了。 30\n",
      "表演者拿一个小簸箕在上面炒，过一会，爆米花就出来了。 31\n",
      "表演者热情地向中国贵宾赠送斯特森骑士帽。 32\n",
      "表演者在冰上的那种潇洒的风格为她赢得了许多朋友和一块金牌。 33\n",
      "表演者在开幕典礼庆祝活动溅水的首届青年奥运会星期六2010年8月14日在新加坡举行。 34\n",
      "表演者，指演员或者其他表演文学、艺术作品的人。 35\n",
      "表演中用打击乐伴奏，但狮舞的动作不受音乐节奏的限制，音乐只起烘托气氛的作用。 36\n",
      "表一：对人体肠道的通透性，作为替代标记使用极地糖尿execretionGIPET我的时间。 37\n",
      "表已经成为有身份或有品味——抑或是身份或品味低下——的新标志。 38\n",
      "表意的主观性和抽象性导致小说偏向表现“形而上的现实”。 39\n",
      "表英尺组表页脚始终显示在其他所有行和行组，和之前的底部字幕。 40\n",
      "表有8毫米厚的明确钢化玻璃与抛光边缘和沙丘抨击标志的顶端小组。 41\n",
      "表有结构简单、口径大、重量轻、操作方便等优点。 42\n",
      "表彰他在电路，系统和信号处理，尤其是数字滤波方面的贡献。 43\n",
      "表征的结果显示PAS分子筛具有更多的适合蜡分子异构化反应的孔径分布。 44\n",
      "表征的途径，其中一项是作家揭示了这样一个人物身份证的个性。 45\n",
      "表征影像学表现上皮样血管瘤，我们审查了现有文献关于这一主题的。 46\n",
      "表值对象可以是表、视图、函数、同义词或本地表。 47\n",
      "表中的大多数术语在本书最后所附的“名词汇编”中均有说明。 48\n",
      "表中的第一列包含任务，后面两列指明任务是否在其原始平台上进行操作。 49\n",
      "表中光照度数据为上午和下午两次观测值的平均值。 50\n",
      "表中基因的一列相对于表达数据发生了调换，导致两项不相对应。 51\n",
      "表中继续述说着两个甚至更多世纪的全食，然而持续时间逐渐减少。 52\n",
      "表中所列电参数为参考值，实际电参数会因使用条件的不同而变化。 53\n",
      "表中也显示了每个版本的小计以及顾客的总费用。 54\n",
      "裱糊纸板：用多张较薄纸板裱贴而成的纸板。有别于均质纸板。 55\n",
      "憋了几个月，等啊等，等待市场推出一款你钟爱的平板电脑。 56\n",
      "憋死我了！你也不说，我也不说，咱俩总得有一个来挑明吧！算了，我不下地狱谁下地狱。我说了“我爱你”！ 57\n",
      "瘪版是堵住雕镂或蚀刻制成的，制成的版镀铬后会补充硬度，以提矮耐印度。 58\n",
      "瘪印版滚筒不合为有轴版滚筒和无轴版滚筒。 59\n",
      "瘪印机洋牵引辊与初版辊相邻凡是撤有周速同步机构。 60\n",
      "瘪印在制卡和会员卡制作金属油墨时不入格得当。柔印在上光方面也有其独到之处。 61\n",
      "瘪制卡和会员卡制作又不合为雕镂不不凸版与拍照印版两栽。 62\n",
      "“别啊，”他喊道，冷笑着，“我们可以把这脏东西当奴隶卖了呀，正好可以换一碗酒钱呢。” 63\n",
      "别按你自己的想象认为他们需要什么；做些用户研究去确切地发现他们所需。 64\n",
      "别把关心你的人扔在角落，否则会让你更受伤。 65\n",
      "别把火炬受干扰当成那整个城市或国家不可饶恕的罪过。 66\n",
      "别把莓果掉出来，因为正如大家所知，最快装满桶子的第一只熊就可以赢得比赛喔！ 67\n",
      "别把你的车子停在我的房子外边，你想想人家回怎么说！ 68\n",
      "别把数学想象为硬梆梆的、死绞蛮缠的、令人讨厌的、有悖于常识的东西，它只不过是赋予常识以灵性的东西。 69\n",
      "别把所有的积蓄花在整修住所，存下出游的金额。 70\n",
      "别把他算进去。你还不了解他吗。他的决心持续不了三分钟。 71\n",
      "别把它当作不切实际、或是必须经过选民公投通过的事情。 72\n",
      "别把它丢失在你的爆米花里，ICM公司的老板认为，互联网能够降低新的电影拍摄者的进入门槛。 73\n",
      "“别把我的心搅乱了，我亲爱的保罗，”他的妹妹说道，“因为这会毁了我。 74\n",
      "别把我框死在一个岗位上！除了接电话，我还可以做许多其他事情！ 75\n",
      "别把我们两个人算在一块儿，哈丽特。拿我的演奏同她的相比，就如同一盏灯同太阳相比。 76\n",
      "别把我弄糊涂了，力量举需要很多的技术，我刚才说的意思是举重是更热门的竞技运动。 77\n",
      "别把香蕉船算我那份里，我想我还是想点份三克巧克力华夫饼蛋卷冰激凌吧。 78\n",
      "别把雪柜放置在煮食炉和发热源头旁，也避免放在有阳光直接照射的窗户。 79\n",
      "别把一个人想的有多高尚，也别把一个人想的有多龌龊 80\n",
      "别把衣服扔在椅子上，把它们挂起来。 81\n",
      "别把油漆胡乱刷得这么厚，他们要求外表光滑美观。 82\n",
      "别把这头猪说成是受害者，这个可怜的女人才是受害者！那些帮助她的人都做得很好，但是没必要把它拍下了！ 83\n",
      "别把这些都当成是你自己的事情：即使你是那个拿着高薪，深感不安，并且一次又一次地触及他们失望底线的家伙。 84\n",
      "别把自己弄得像个太监一样，连自己的理想都要去遗忘，这样你真的会开心？ 85\n",
      "别霸占着你不用的东西！既然你下午有课，就把网球拍让我用。 86\n",
      "别拌嘴了，女士们。就让我们也偶尔在舒适的床上享受一个平静的夜晚吧。 87\n",
      "别抱怨你假期花钱太多，你倒是玩得痛快。鱼与熊掌不可兼得嘛。 88\n",
      "别被教条捆住了，否则你就只是生活在别人思考的结果里。 89\n",
      "别被名字给弄混了，它出售的是时髦的围裙、手袋、钱包以及非常传统式样的垫桌布。 90\n",
      "别被秋分日昼夜长短相等的概念愚弄了。 91\n",
      "别被他谦虚的态度骗了，其实他是个很骄傲的人。 92\n",
      "别被这个狡猾的忍者所愚弄！这其实是一个优盘。 93\n",
      "别逼着他们告诉你他们的心事。那将惹怒他们，可能另他们感觉更糟。 94\n",
      "别别，没但是。只要你客观点看待这个情况，这份工作就该归我。 95\n",
      "“别，别着急放下灯笼，看来，还有一个要出来！”，医生不由得惊叫起来。 96\n",
      "别不承认你内心最深处的事情，爱她胜过爱你自己，没有她你的生活毫无意义。 97\n",
      "别不假思索的就怪罪于母亲：一个爱哭的、得腹绞痛的宝宝也同样会是由父亲的情绪而影响形成的，荷兰研究人员的报告说。 98\n",
      "别操心化妆了这样可以让你的皮肤透透气--但得确保先抹上滋润霜。 99\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(lol[i], i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
