{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing tho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob, time\n",
    "import pickle, json\n",
    "import math, string, re\n",
    "import numpy as np\n",
    "from nltk import AlignedSent, IBMModel1, IBMModel2, IBMModel3, word_tokenize, Alignment\n",
    "from collections import Counter, defaultdict\n",
    "from heapq import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ibm_align(numsent, iterations, engtemp, chtemp, fmodel, bmodel):\n",
    "    forwards, backwards = [], []\n",
    "    for i in range(numsent):\n",
    "        forwards.append(AlignedSent(word_tokenize(engtemp[i]), word_tokenize(chtemp[i])))\n",
    "        backwards.append(AlignedSent(word_tokenize(chtemp[i]), word_tokenize(engtemp[i])))\n",
    "    ibm2f = fmodel(forwards, iterations)\n",
    "    ibm2b = bmodel(backwards, iterations)\n",
    "    return forwards, backwards, ibm2f, ibm2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one = [\"i like dogs\", \"i like cats\", \"i like to go running\", \"i like to go swimming\", \"i like\", \"i don't like\", \"i like\", \"i like to do things\", \"i like\", \"i see you\"]\n",
    "two = [\"我 喜 欢 狗\", \"我 喜 欢 猫\", \"我 喜 欢 跑 步\", \"我 喜 欢 去 游 泳\", \"我 喜 欢\", \"我 不 喜 欢\", \"我 喜 欢\", \n",
    "       \"我 喜 欢 做 事 情\", \"我 喜 欢\", \"我 看 见 你\"]\n",
    "f, b, left, right = ibm_align(len(one), 5, one, two, IBMModel2, IBMModel2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment matrix\n",
    "\n",
    "## Growing heuristic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_neighbor(alignment, p):\n",
    "    a, b = False, True\n",
    "    c = alignment[:]\n",
    "    c.append(p)\n",
    "    l, r, u, d = (-1 + p[0], p[1]), (1 + p[0], p[1]), (p[0], 1 + p[1]), (p[0], -1 + p[1])\n",
    "    if (l in alignment) or (r in alignment) or (u in alignment) or (d in alignment): a = True\n",
    "    for i in c:\n",
    "        l, r, u, d = (-1 + i[0], i[1]), (1 + i[0], i[1]), (i[0], 1 + i[1]), (i[0], -1 + i[1])\n",
    "        if (l in c) or (r in c):\n",
    "            if (u in c) or (d in c): b = False\n",
    "        elif (u in c) or (d in c):\n",
    "            if (l in c) or (r in c): b = False\n",
    "    return a and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def t(a): return [(x[1], x[0]) for x in list(a.alignment)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grow(e2c, c2e):\n",
    "    \n",
    "    n = [[x, y] for x in range(-1, 2) for y in range(-1, 2) if (x != 0 or y != 0)]\n",
    "    union = list(set(e2c.alignment).union(t(c2e)))\n",
    "    intersection = list(set(e2c.alignment).intersection(t(c2e)))\n",
    "    alignment = intersection[:]\n",
    "    \n",
    "    for i in range(len(e2c.words)):\n",
    "        for j in range(len(e2c.mots)):\n",
    "            if (i, j) in alignment:\n",
    "                for k in n:\n",
    "                    point = (k[0] + i, k[1] + j)\n",
    "                    if ((point[0] not in [x[0] for x in alignment]) and (point[1] not in [x[1] for x in alignment])) \\\n",
    "                    and (point in union):\n",
    "                        alignment.append(point)\n",
    "\n",
    "    for i in range(len(c2e.words)):\n",
    "        for j in range(len(c2e.mots)):\n",
    "            if ((i not in [x[1] for x in alignment]) or (j not in [x[0] for x in alignment])) \\\n",
    "            and ((i,j) in c2e.alignment) and check_neighbor(alignment, (i,j)):\n",
    "                alignment.append((j,i))\n",
    "                \n",
    "    for i in range(len(e2c.words)):\n",
    "        for j in range(len(e2c.mots)):\n",
    "            #print(alignment)\n",
    "            if ((i not in [x[0] for x in alignment]) or (j not in [x[1] for x in alignment])) \\\n",
    "            and ((i,j) in e2c.alignment) and check_neighbor(alignment,(i,j)):\n",
    "                alignment.append((i,j))\n",
    "       \n",
    "    for i in range(len(e2c.words)):\n",
    "        for j in range(len(e2c.mots)):\n",
    "            if (i, j) in alignment:\n",
    "                for k in n:\n",
    "                    point = (k[0] + i, k[1] + j)\n",
    "                    if ((point[0] not in [x[0] for x in alignment]) and (point[1] not in [x[1] for x in alignment])) \\\n",
    "                    and (point in union):\n",
    "                        alignment.append(point)\n",
    "                        \n",
    "    if alignment == []: return union\n",
    "    return alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consistent phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def consistent(points, temp):\n",
    "    c = [k for j in [z[0] for z in temp] for k in j]\n",
    "    d = [k for j in [z[1] for z in temp] for k in j]\n",
    "    for i in c:\n",
    "        if points[0][0] <= i <= points[0][1]: return False\n",
    "    for i in d:\n",
    "        if points[1][0] <= i <= points[1][1]: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Phrasal Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collapse(temp):\n",
    "    temp, e = sorted(temp), []\n",
    "    for x in temp:\n",
    "        if e == []: e.append(([x[0]], [x[1]]))\n",
    "        else:\n",
    "            a, b = x[0], x[1]\n",
    "            if e[len(e) - 1][0][0] == a: e[len(e) - 1][1].append(x[1])# >1 eng\n",
    "            elif e[len(e) - 1][1][0] == b: e[len(e) - 1][0].append(x[0])# >1 c\n",
    "            else: e.append(([x[0]], [x[1]])) #none\n",
    "                \n",
    "    return e\n",
    "\n",
    "assert collapse([(0,0), (1,1), (1,2), (1,3), (2,4)]) == [([0], [0]), ([1], [1, 2, 3]), ([2], [4])]\n",
    "assert collapse([(0,0), (1,1), (2,1), (3,1), (4,2)]) == [([0], [0]), ([1, 2, 3], [1]), ([4], [2])]\n",
    "assert collapse([(0,0), (1,0), (2,0), (3,1), (3,2), (4,3), (6,4), (5,5), (5,6)]) == \\\n",
    "                [([0, 1, 2], [0]), ([3], [1, 2]), ([4], [3]), ([5], [5, 6]), ([6], [4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_phrases(ans, ccounts, temp, sent, beamwidth, length_limit = 5):\n",
    "    '''\n",
    "    temp: {ch: {eng: [phrase width, count, probability ]}}\n",
    "    \n",
    "    '''\n",
    "    if beamwidth > len(temp): beamwidth = len(temp)\n",
    "    elif beamwidth <= 0: return(\"Beam width must be at least 1\")\n",
    "    \n",
    "    for x in range(beamwidth, 0, -1):\n",
    "        for i in range(len(temp) - x + 1):\n",
    "            \n",
    "            pp, y = {}, temp[i:i + x]\n",
    "            c, d = [k for j in [z[0] for z in y] for k in j], [k for j in [z[1] for z in y] for k in j] #x, y values\n",
    "            min_c, max_c, min_d, max_d = min(c), max(c), min(d), max(d)\n",
    "            \n",
    "            if max_c - min_c < length_limit: #first hard limit to reduce phrase extraction times\n",
    "                e = [z for z in temp if z not in y]\n",
    "                #if e == []: #all points included\n",
    "                if consistent([(min_c, max_c), (min_d, max_d)], e):\n",
    "                    \n",
    "                    #word alignment defined phrase\n",
    "                    a = \" \".join(sent.words[min(c):max(c) + 1])\n",
    "                    b = \" \".join(sent.mots[min(d):max(d) + 1])\n",
    "                    if b in ccounts: ccounts[b] += 1\n",
    "                    else: ccounts.update({b : 1})\n",
    "                    '''\n",
    "                    if a in ans:\n",
    "                        if b in ans[a]: ans[a][b][1] += 1\n",
    "                        else: ans[a].update({b : [x, 1, 0]})\n",
    "                    else: ans.update({a : {b : [x, 1, 0]}})\n",
    "                    '''\n",
    "                    if b in ans:\n",
    "                        if a in ans[b]:\n",
    "                            ans[b][a][1] += 1\n",
    "                        else: ans[b].update({a : [x, 1, 0]})\n",
    "                    else: ans.update({b : {a : [x, 1, 0]}})\n",
    "                        \n",
    "                    #unaligned expansive phrase extraction IMPLEMENTATION\n",
    "                    #grow maximal consistency then iterate over all possible phrases\n",
    "                    #order: up, down, left, right\n",
    "                    \n",
    "                    if e == []: break\n",
    "                    \n",
    "                    #points not in window of y\n",
    "                    f, g = [k for j in [z[0] for z in e] for k in j], [k for j in [z[1] for z in e] for k in j]\n",
    "                    min_f, max_f, min_g, max_g = min(f), max(f), min(g), max(g)\n",
    "\n",
    "                    #edge case first block\n",
    "                    if i == 0:\n",
    "                        \n",
    "                        up = True if min_c > 0 else False\n",
    "                        down = True if max_c < min_f - 1 else False\n",
    "                        left = True if min_d > 0 else False\n",
    "                        right = True if max_d < min_g - 1 else False\n",
    "                            \n",
    "                        if up:\n",
    "                            for j in range(max(max_c - length_limit + 1, 0), min_c):\n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[j:max_c + 1])})\n",
    "                            \n",
    "                        if down:\n",
    "                            for j in range(max_c + 1, min(min_c + length_limit - 1, min_f)): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:j + 1])})\n",
    "                            \n",
    "                        if left:\n",
    "                            for j in range(max(max_d - length_limit + 1, 0), min_d): \n",
    "                                pp.update({\" \".join(sent.mots[j:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                            \n",
    "                        if right:\n",
    "                            for j in range(max_d + 1, min(min_d + length_limit - 1, min_g)): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:j + 1]) :\n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                        \n",
    "                    #edge case final block        \n",
    "                    elif i == len(temp) - 1:\n",
    "                        \n",
    "                        up = True if min_c > max_f - 1 else False\n",
    "                        down = True if max_c < len(sent.words) - 1 else False\n",
    "                        left = True if min_d > max_g - 1 else False\n",
    "                        right = True if max_d < len(sent.mots) - 1 else False\n",
    "                        \n",
    "                        if up:\n",
    "                            for j in range(max(max_c - length_limit + 1, max_f), min_c):\n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[j:max_c + 1])})                                \n",
    "                            \n",
    "                        if down:\n",
    "                            for j in range(max_c + 1, min(min_c + length_limit - 1, len(sent.words))): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:j + 1])})\n",
    "                            \n",
    "                        if left:\n",
    "                            for j in range(max(max_d - length_limit + 1, max_g), min_d): \n",
    "                                pp.update({\" \".join(sent.mots[j:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                            \n",
    "                        if right:\n",
    "                            for j in range(max_d + 1, min(min_d + length_limit - 1, len(sent.mots))): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:j + 1]) :\n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                                \n",
    "                    else:\n",
    "                        \n",
    "                        u_max, d_min, l_max, r_min = max(temp[i - 1][0]), min(temp[i + 1][0]), \\\n",
    "                                                     max(temp[i - 1][1]), min(temp[i + 1][1])\n",
    "                        \n",
    "                        up = True if min_c > u_max - 1 else False\n",
    "                        down = True if max_c < d_min - 1 else False\n",
    "                        left = True if min_d > l_max - 1 else False\n",
    "                        right = True if max_d < r_min - 1 else False\n",
    "                        \n",
    "                        if up:\n",
    "                            for j in range(max(max_c - length_limit + 1, u_max), min_c):\n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) :\n",
    "                                           \" \".join(sent.words[j:max_c + 1])})\n",
    "                                #print(\"up\")\n",
    "                        \n",
    "                        if down:\n",
    "                            for j in range(max_c + 1, min(min_c + length_limit - 1, d_min)): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:j + 1])})\n",
    "                                #print(\"down\")\n",
    "                            \n",
    "                        if left:\n",
    "                            for j in range(max(max_d - length_limit + 1, l_max), min_d): \n",
    "                                pp.update({\" \".join(sent.mots[j:max_d + 1]) : \n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                                #print(\"left\")\n",
    "                            \n",
    "                        if right:\n",
    "                            for j in range(max_d + 1, min(min_d + length_limit - 1, r_min)): \n",
    "                                pp.update({\" \".join(sent.mots[min_d:j + 1]) :\n",
    "                                           \" \".join(sent.words[min_c:max_c + 1])})\n",
    "                                #print(\"right\")\n",
    "                            \n",
    "                    \n",
    "                    #print(pp)\n",
    "                    \n",
    "                    for j in pp:\n",
    "                        a, b = j, pp[j]\n",
    "                        if a in ccounts: ccounts[a] += 1\n",
    "                        else: ccounts.update({a : 1})\n",
    "                        if a in ans:\n",
    "                            if b in ans[a]: \n",
    "                                ans[a][b][1] += 1\n",
    "                            else: ans[a].update({b : [x, 1, 0]})\n",
    "                        else: ans.update({a : {b : [x, 1, 0]}})\n",
    "                        \n",
    "    return ans, ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4]\n",
      "[2, 4]\n",
      "[([0], [0]), ([1], [1]), ([6], [6])]\n",
      "[0, 1, 6]\n",
      "[0, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "temp = [([0], [0]), ([1], [1]), ([2], [2]), ([4], [4]), ([6], [6])]\n",
    "y = [([2], [2]), ([4], [4])]\n",
    "cc, dc = [k for j in [z[0] for z in y] for k in j], [k for j in [z[1] for z in y] for k in j]\n",
    "ec = [z for z in temp if z not in y]\n",
    "fc, gc = [k for j in [z[0] for z in ec] for k in j], [k for j in [z[1] for z in ec] for k in j]\n",
    "print(\"\\n\".join([str(x) for x in [cc,dc,ec,fc,gc]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note to self: `collapse` is sorted on the x values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([0], [0]), ([1], [1]), ([3], [3, 4])]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collapse(grow(t1, t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up\n",
      "up\n",
      "left\n",
      "up\n",
      "up\n",
      "up\n",
      "up\n",
      "up\n",
      "up\n",
      "left\n",
      "up\n",
      "left\n",
      "left\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "up\n",
      "left\n",
      "up\n",
      "up\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "left\n",
      "up\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "left\n",
      "up\n",
      "left\n",
      "right\n",
      "up\n",
      "down\n",
      "left\n",
      "left\n",
      "right\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "up\n",
      "left\n",
      "right\n",
      "right\n",
      "up\n",
      "down\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "up\n",
      "up\n",
      "down\n",
      "down\n",
      "down\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "right\n",
      "up\n",
      "up\n",
      "up\n",
      "up\n",
      "down\n",
      "left\n",
      "left\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "up\n",
      "down\n",
      "up\n",
      "up\n",
      "down\n",
      "right\n",
      "right\n",
      "right\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "up\n",
      "down\n",
      "down\n",
      "left\n",
      "right\n",
      "right\n",
      "right\n",
      "up\n",
      "up\n",
      "up\n",
      "left\n",
      "left\n",
      "left\n",
      "left\n",
      "up\n",
      "right\n",
      "right\n",
      "right\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'.': {'。': [1, 1, 0], '者 。': [1, 1, 0]},\n",
       "  'We': {'我 们': [1, 1, 0]},\n",
       "  'We are': {'在': [1, 1, 0], '我 们 在': [2, 1, 0], '我 们 在 这': [2, 1, 0]},\n",
       "  'We are here': {'在 这 里': [2, 1, 0], '我 们 在 这 里': [3, 1, 0]},\n",
       "  'We are here to': {'我 们 在 这 里': [3, 1, 0]},\n",
       "  'We are here to ignite': {'在 这 里 点 燃': [3, 1, 0],\n",
       "   '我 们 在 这 里 点 燃': [4, 1, 0]},\n",
       "  'a': {'一': [1, 1, 0], '一 个 意': [1, 1, 0]},\n",
       "  'a motion': {'一 个 意 动': [2, 1, 0], '动': [1, 1, 0], '燃 一 个 意 动': [2, 1, 0]},\n",
       "  'and': {'展 的': [1, 1, 0], '的': [1, 1, 0]},\n",
       "  'and development': {'发 展 的': [2, 1, 0], '展': [1, 1, 0], '展 的': [2, 1, 0]},\n",
       "  'and development .': {'展 的 提 升 者 。': [3, 1, 0]},\n",
       "  'and to': {'到': [1, 1, 0]},\n",
       "  'and to carry': {'带 领 那 些 想 要 走 到': [2, 1, 0]},\n",
       "  'are': {'在': [1, 1, 0], '在 这': [1, 1, 0]},\n",
       "  'are here': {'们 在 这 里': [2, 1, 0], '在 这 里': [2, 1, 0], '里': [1, 1, 0]},\n",
       "  'are here to ignite': {'在 这 里 点 燃': [3, 1, 0], '里 点 燃': [2, 1, 0]},\n",
       "  'are here to ignite a': {'在 这 里 点 燃 一': [4, 1, 0], '里 点 燃 一': [3, 1, 0]},\n",
       "  'carry': {'带': [1, 1, 0]},\n",
       "  'carry each': {'带': [1, 1, 0]},\n",
       "  'carry each willing': {'意': [1, 1, 0]},\n",
       "  'development': {'展': [1, 1, 0], '展 的 提 升': [1, 1, 0]},\n",
       "  'development .': {'。': [1, 1, 0]},\n",
       "  'each willing': {'意': [1, 1, 0]},\n",
       "  'heights': {'高': [1, 1, 0], '高 度 定 见': [1, 1, 0]},\n",
       "  'heights of': {'高': [1, 1, 0]},\n",
       "  'heights of mindset': {'高': [1, 1, 0]},\n",
       "  'heights of mindset and': {'的': [1, 1, 0]},\n",
       "  'heights of mindset and development': {'展 的': [2, 1, 0],\n",
       "   '高 度 定 见 及 继 续 发 展 的': [3, 1, 0]},\n",
       "  'here': {'里': [1, 1, 0], '里 点': [1, 1, 0]},\n",
       "  'here to': {'里': [1, 1, 0]},\n",
       "  'here to ignite': {'燃': [1, 1, 0], '这 里 点 燃': [2, 1, 0], '里 点 燃': [2, 1, 0]},\n",
       "  'here to ignite a': {'燃 一': [2, 1, 0],\n",
       "   '这 里 点 燃 一': [3, 1, 0],\n",
       "   '里 点 燃 一': [3, 1, 0]},\n",
       "  'here to ignite a motion': {'燃 一 个 意 动': [3, 1, 0],\n",
       "   '里 点 燃 一 个 意 动': [4, 1, 0]},\n",
       "  'ignite': {'点 燃': [1, 1, 0], '燃': [1, 1, 0]},\n",
       "  'ignite a': {'一': [1, 1, 0], '点 燃 一': [2, 1, 0], '燃 一': [2, 1, 0]},\n",
       "  'ignite a motion': {'一 个 意 动': [2, 1, 0], '燃 一 个 意 动': [3, 1, 0]},\n",
       "  'mindset and': {'的': [1, 1, 0]},\n",
       "  'mindset and development': {'展 的': [2, 1, 0]},\n",
       "  'mindset and development .': {'展 的 提 升 者 。': [3, 1, 0]},\n",
       "  'motion': {'动': [1, 1, 0], '动 ， 这 个': [1, 1, 0]},\n",
       "  'motion that': {'动': [1, 1, 0]},\n",
       "  'motion that restores': {'恢': [1, 1, 0]},\n",
       "  'new': {'个 新': [1, 1, 0], '新': [1, 1, 0]},\n",
       "  'new heights': {'个 新 高': [2, 1, 0], '新 高': [2, 1, 0], '高': [1, 1, 0]},\n",
       "  'of mindset and': {'的': [1, 1, 0]},\n",
       "  'of mindset and development': {'展 的': [2, 1, 0]},\n",
       "  'of mindset and development .': {'展 的 提 升 者 。': [3, 1, 0]},\n",
       "  'of the self within': {'内': [1, 1, 0]},\n",
       "  'restores': {'恢': [1, 1, 0], '恢 复 你': [1, 1, 0]},\n",
       "  'restores the': {'恢': [1, 1, 0]},\n",
       "  'restores the self': {'恢': [1, 1, 0]},\n",
       "  'restores the self of': {'恢': [1, 1, 0]},\n",
       "  'self of the self within': {'内': [1, 1, 0]},\n",
       "  'self within': {'内': [1, 1, 0]},\n",
       "  'self within and to carry': {'内 在 的 真 我 ， 带 领 那 些 想 要 走 到': [3, 1, 0]},\n",
       "  'that restores': {'恢': [1, 1, 0]},\n",
       "  'the self within': {'内': [1, 1, 0]},\n",
       "  'to': {'到': [1, 1, 0], '走 到': [1, 1, 0]},\n",
       "  'to carry': {'带': [1, 1, 0], '带 领 那 些 想 要 走 到': [2, 1, 0]},\n",
       "  'to ignite': {'燃': [1, 1, 0]},\n",
       "  'to ignite a': {'燃 一': [2, 1, 0]},\n",
       "  'to ignite a motion': {'燃 一 个 意 动': [3, 1, 0]},\n",
       "  'to new': {'新': [1, 1, 0]},\n",
       "  'to new heights': {'新 高': [2, 1, 0]},\n",
       "  'willing': {'意': [1, 1, 0], '意 动 将 恢': [1, 1, 0]},\n",
       "  'willing to': {'意': [1, 1, 0]},\n",
       "  'willing to new': {'新': [1, 1, 0]},\n",
       "  'willing to new heights': {'新 高': [2, 1, 0]},\n",
       "  'within': {'内': [1, 1, 0], '内 在 的 真': [1, 1, 0]},\n",
       "  'within and': {'内': [1, 1, 0]},\n",
       "  'within and to': {'到': [1, 1, 0]},\n",
       "  'within and to carry': {'内 在 的 真 我 ， 带 领 那 些 想 要 走 到': [3, 1, 0],\n",
       "   '带 领 那 些 想 要 走 到': [2, 1, 0]}},\n",
       " {'.': 2,\n",
       "  'We': 1,\n",
       "  'We are': 3,\n",
       "  'We are here': 2,\n",
       "  'We are here to': 1,\n",
       "  'We are here to ignite': 2,\n",
       "  'a': 2,\n",
       "  'a motion': 3,\n",
       "  'and': 2,\n",
       "  'and development': 3,\n",
       "  'and development .': 1,\n",
       "  'and to': 1,\n",
       "  'and to carry': 1,\n",
       "  'are': 2,\n",
       "  'are here': 3,\n",
       "  'are here to ignite': 2,\n",
       "  'are here to ignite a': 2,\n",
       "  'carry': 1,\n",
       "  'carry each': 1,\n",
       "  'carry each willing': 1,\n",
       "  'development': 2,\n",
       "  'development .': 1,\n",
       "  'each willing': 1,\n",
       "  'heights': 2,\n",
       "  'heights of': 1,\n",
       "  'heights of mindset': 1,\n",
       "  'heights of mindset and': 1,\n",
       "  'heights of mindset and development': 2,\n",
       "  'here': 2,\n",
       "  'here to': 1,\n",
       "  'here to ignite': 3,\n",
       "  'here to ignite a': 3,\n",
       "  'here to ignite a motion': 2,\n",
       "  'ignite': 2,\n",
       "  'ignite a': 3,\n",
       "  'ignite a motion': 2,\n",
       "  'mindset and': 1,\n",
       "  'mindset and development': 1,\n",
       "  'mindset and development .': 1,\n",
       "  'motion': 2,\n",
       "  'motion that': 1,\n",
       "  'motion that restores': 1,\n",
       "  'new': 2,\n",
       "  'new heights': 3,\n",
       "  'of mindset and': 1,\n",
       "  'of mindset and development': 1,\n",
       "  'of mindset and development .': 1,\n",
       "  'of the self within': 1,\n",
       "  'restores': 2,\n",
       "  'restores the': 1,\n",
       "  'restores the self': 1,\n",
       "  'restores the self of': 1,\n",
       "  'self of the self within': 1,\n",
       "  'self within': 1,\n",
       "  'self within and to carry': 1,\n",
       "  'that restores': 1,\n",
       "  'the self within': 1,\n",
       "  'to': 2,\n",
       "  'to carry': 2,\n",
       "  'to ignite': 1,\n",
       "  'to ignite a': 1,\n",
       "  'to ignite a motion': 1,\n",
       "  'to new': 1,\n",
       "  'to new heights': 1,\n",
       "  'willing': 2,\n",
       "  'willing to': 1,\n",
       "  'willing to new': 1,\n",
       "  'willing to new heights': 1,\n",
       "  'within': 2,\n",
       "  'within and': 1,\n",
       "  'within and to': 1,\n",
       "  'within and to carry': 2})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_phrases({}, {}, collapse(f[0].alignment), f[0], beamwidth = 5, length_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'I': {'我': [1, 1, 0]},\n",
       "  'I like': {'我 喜': [2, 1, 0], '我 喜 欢': [2, 1, 0]},\n",
       "  'I like to': {'我 喜': [2, 1, 0]},\n",
       "  'do': {'做 事': [1, 1, 0], '欢 做 事': [1, 1, 0]},\n",
       "  'do work': {'做 事': [1, 1, 0]},\n",
       "  'like': {'喜': [1, 1, 0]},\n",
       "  'like to do': {'做 事': [1, 1, 0], '喜 欢 做 事': [2, 1, 0]},\n",
       "  'to do': {'做 事': [1, 1, 0]}},\n",
       " {'I': 1,\n",
       "  'I like': 2,\n",
       "  'I like to': 1,\n",
       "  'do': 2,\n",
       "  'do work': 1,\n",
       "  'like': 1,\n",
       "  'like to do': 2,\n",
       "  'to do': 1})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = AlignedSent('I like to do work'.split(), [\"我\",\"喜\",\"欢\",\"做\",\"事\"], Alignment([(0,0), (1,1), (3,3), (3,4)]))\n",
    "t2 = AlignedSent([\"我\",\"喜\",\"欢\",\"做\",\"事\"], 'I like to do work'.split(), Alignment([(1,1), (3,3), (4,3)]))\n",
    "extract_phrases({}, {}, collapse(grow(t1, t2)), t1, beamwidth = 2, length_limit = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to do 我 喜\n",
      "I like to do 喜\n",
      "like to do 我 喜\n",
      "like to do 喜\n",
      "to do 我 喜\n",
      "to do 喜\n",
      "do 我 喜\n",
      "do 喜\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'I': {'我': [1, 1, 0]},\n",
       "  'I like to do': {'喜': [2, 1, 0], '我 喜': [2, 1, 0]},\n",
       "  'do': {'做 事': [1, 1, 0], '喜': [2, 1, 0], '我 喜': [2, 1, 0]},\n",
       "  'like': {'喜': [1, 1, 0]},\n",
       "  'like to do': {'喜': [2, 1, 0], '我 喜': [2, 1, 0]},\n",
       "  'to do': {'喜': [2, 1, 0], '我 喜': [2, 1, 0]},\n",
       "  'work': {'欢': [1, 1, 0]}},\n",
       " {'I': 1,\n",
       "  'I like to do': 2,\n",
       "  'do': 3,\n",
       "  'like': 1,\n",
       "  'like to do': 2,\n",
       "  'to do': 2,\n",
       "  'work': 1})"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = AlignedSent('I like to do work'.split(), [\"我\",\"喜\",\"欢\",\"做\",\"事\"], Alignment([(0,0), (1,1), (4,2), (3,3), (3,4)]))\n",
    "t2 = AlignedSent([\"我\",\"喜\",\"欢\",\"做\",\"事\"], 'I like to do work'.split(), Alignment([(1,1), (3,3), (4,3)]))\n",
    "extract_phrases({}, {}, collapse(grow(t1, t2)), t1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3] [3, 4] [([2], [3]), ([3], [4])] consistent\n",
      "[0, 3] [0, 4] [([0], [0]), ([3], [4])] consistent\n",
      "[([1], [1, 2]), ([2], [3])] [1, 2] [1, 2, 3] 1 [([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4])] 1 1 1 2\n",
      "[0, 1] [0, 1, 2] [([0], [0]), ([1], [1, 2])] consistent\n",
      "[([2], [3]), ([3], [4])] [2, 3] [3, 4] 2 [([0], [0]), ([1], [1, 2]), ([2], [3]), ([3], [4])] 2 3 2 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'I': {'我': [1, 1, 0]},\n",
       "  'I like': {'我 喜 欢': [2, 1, 0]},\n",
       "  'doing': {'做': [1, 1, 0]},\n",
       "  'doing work': {'做 事': [2, 1, 0]},\n",
       "  'like': {'喜 欢': [1, 1, 0]},\n",
       "  'like doing': {'喜 欢 做': [2, 1, 0]},\n",
       "  'work': {'事': [1, 1, 0]}},\n",
       " {'I': 1,\n",
       "  'I like': 1,\n",
       "  'doing': 1,\n",
       "  'doing work': 1,\n",
       "  'like': 1,\n",
       "  'like doing': 1,\n",
       "  'work': 1})"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = AlignedSent('I like doing work'.split(), [\"我\",\"喜\",\"欢\",\"做\",\"事\"], Alignment([(0,0), (1,1), (1,2), (3,2)]))\n",
    "t2 = AlignedSent([\"我\",\"喜\",\"欢\",\"做\",\"事\"], 'I like doing work'.split(), Alignment([(2,1), (3,2), (4,3)]))\n",
    "extract_phrases({}, {}, collapse(grow(t1, t2)), t1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 2), (3, 3), (4, 4)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = AlignedSent('I like to do work'.split(), [\"我\",\"喜\",\"欢\",\"做\",\"事\"], Alignment([(0,0), (1,1), (2,2), (3,2)]))\n",
    "t2 = AlignedSent([\"我\",\"喜\",\"欢\",\"做\",\"事\"], 'I like to do work'.split(), Alignment([(2,3), (3,3), (4,4)]))\n",
    "grow(t1, t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def phrasal_lexicon(e, c, sents, beamwidth = 2, score = \"counts\"):\n",
    "    \n",
    "    p, ccounts = {}, {}\n",
    "    \n",
    "    print(\"Stage I: Word alignment\")\n",
    "    t1 = time.time()\n",
    "    f, b, left, right = ibm_align(sents, 5, e[:sents], c[:sents], IBMModel2, IBMModel2)\n",
    "    t2 = time.time()\n",
    "    print(\"Stage I completed; took {} seconds\".format(t2-t1))\n",
    "    print()\n",
    "    \n",
    "    print(\"Stage II: Phrase extraction\")\n",
    "    t1 = time.time()\n",
    "    \n",
    "    for i in range(sents): \n",
    "        f[i].alignment = Alignment(grow(f[i], b[i])) #heuristic Alignment tables\n",
    "        temp = collapse(f[i].alignment)\n",
    "        p, ccounts = extract_phrases(p, ccounts, temp, f[i], beamwidth)\n",
    "    t2 = time.time()    \n",
    "    print(\"Stage II completed; took {} seconds\".format(t2-t1))\n",
    "    print()\n",
    "        \n",
    "    print(\"Stage III: Scoring + sorting phrases by score\")\n",
    "    t1 = time.time()\n",
    "    \n",
    "    if score == \"counts\":\n",
    "        for i in p:\n",
    "            for j in p[i]:\n",
    "                p[i][j][2] = math.log(p[i][j][1] / ccounts[i])\n",
    "            p[i] = sorted(p[i].items(), key = lambda x: x[1][2], reverse = True)\n",
    "                \n",
    "    t2 = time.time()\n",
    "    print(\"Stage III completed; took {} seconds\".format(t2-t1))\n",
    "    print()\n",
    "    \n",
    "    return f, b, left, right, p, ccounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sents = 300000\n",
    "\n",
    "#my personal computer\n",
    "#path = \"C:/Users/chuck189/Desktop/Cal Poly Summer Research 2017/data/casia2015/casia_clean\"\n",
    "\n",
    "#Desktop computer\n",
    "path = \"C:/Users/Sam/Desktop/Cal Poly Summer Research 2017/data/casia2015/casia_clean\"\n",
    "\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "engtemp = open(\"casia2015_en_pclean.txt\", \"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "ngrameng = open(\"ngram_train.txt\", \"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "chtemp = open(\"casia2015_ch_pclean.txt\", \"r\", encoding = \"utf-8\").read().split(\"\\n\")\n",
    "parallel = {engtemp[i]:chtemp[i] for i in range(len(engtemp))}\n",
    "train_e = np.random.choice(engtemp[:], replace = False, size = sents)\n",
    "train_c = [parallel[x] for x in train_e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stage I: Word alignment\n",
      "Stage I completed; took 17543.79779934883 seconds\n",
      "\n",
      "Stage II: Phrase extraction\n",
      "Stage II completed; took 2209.639599084854 seconds\n",
      "\n",
      "Stage III: Scoring + sorting phrases by score\n",
      "Stage III completed; took 110.11019110679626 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f, b, left, right, phrase_table, chinese_counts = phrasal_lexicon(train_e, train_c, sents, beamwidth = 10, score = \"counts\")\n",
    "\n",
    "os.system(\"mkdir data_structures\")\n",
    "ltt = left.translation_table\n",
    "lat = left.alignment_table\n",
    "rtt = right.translation_table\n",
    "rat = right.alignment_table\n",
    "    \n",
    "with open(path + \"/data_structures/e2c_ttable.json\", \"w\") as file: json.dump(ltt, file)\n",
    "with open(path + \"/data_structures/e2c_atable.json\", \"w\") as file: json.dump(lat, file)  \n",
    "with open(path + \"/data_structures/c2e_ttable.json\", \"w\") as file: json.dump(rtt, file)   \n",
    "with open(path + \"/data_structures/c2e_atable.json\", \"w\") as file: json.dump(rat, file)\n",
    "with open(path + \"/data_structures/phrase_table.json\", \"w\") as file: json.dump(phrase_table, file)   \n",
    "with open(path + \"/data_structures/chinese_counts.json\", \"w\") as file: json.dump(chinese_counts, file)   \n",
    "with open(path + \"/data_structures/e2c_alignment.p\", \"wb\") as file: pickle.dump(f, file)   \n",
    "with open(path + \"/data_structures/c2e_alignment.p\", \"wb\") as file: pickle.dump(b, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune_phrase_table(phrase_table, limit = 20):\n",
    "    '''\n",
    "    For each Chinese word and phrase it keeps the top \"number\" of entries\n",
    "    '''\n",
    "    for x in phrase_table:\n",
    "        if len(phrase_table[x]) > limit: phrase_table[x] = phrase_table[x][:limit]  \n",
    "    return phrase_table\n",
    "\n",
    "phrase_table = prune_phrase_table(phrase_table, 10)\n",
    "with open(path + \"/data_structures/pruned_phrase_table.json\", \"w\") as file: json.dump(phrase_table, file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\arg \\max_{y \\in \\mathcal{Y}(x)} f(y)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$f(y) = h(e(y)) \\hspace{0.1cm}+ \\hspace{0.1cm}\\sum^{L}_{k = 1}g(p_k) \\hspace{0.1cm}+ \\hspace{0.1cm}\\sum^{L-1}_{k=1}\\eta \\hspace{0.08cm}\\times\\hspace{0.08cm} {\\big|}\\hspace{0.05cm}t(p_k) + 1 - s(p_{k+1}) \\hspace{0.05cm}{\\big|}$$\n",
    "\n",
    "States are $(\\alpha, e_1, e_2, b, r)$ where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModifiedKneserNey:\n",
    "    \n",
    "    def __init__(self, sents = [], type_ = \"train\"):\n",
    "        if type_ == \"train\":\n",
    "            self.sents = sents\n",
    "            self.trigram = defaultdict(dict) #need this have O(1) access to N1+(w,w,.), dereference + collapse later\n",
    "            self.tnum = defaultdict(int) #(the, dog, ran): 12\n",
    "            self.tdenom = defaultdict(int) #(the, dog, .) : 123 \n",
    "            self.tdis = [0, 0, 0, 0] #discount values for trigram\n",
    "            self.bnum = defaultdict(int) #(dog, ran): 13\n",
    "            self.bigamma = defaultdict(float) #for objects like tdenom (the, dog, .)\n",
    "            self.avg_bigamma = 0\n",
    "            self.dot_w = defaultdict(set) #(., ran) unique trailing words, bigram numerator\n",
    "            self.bdis = [0, 0, 0, 0]\n",
    "            self.w_dot = defaultdict(set) #(dog, .) unique next words, bigram normalizer\n",
    "            self.num_unique_bigram = 0 #release unique_bigram_holder memory when done, bigram denom\n",
    "            self.unique_bigram_holder = set() #this is the denominator of bigram\n",
    "            self.num_uni = 0\n",
    "            self.udis = [0, 0, 0, 0]\n",
    "            self.unum = defaultdict(int)\n",
    "            #looks like a bunch of stuff but in order to predict trigrams that don't appear we need\n",
    "            #all of this to calculate the probability at runtime.\n",
    "        \n",
    "            self.traverse()\n",
    "            self.get_discount()\n",
    "            self.gamma()\n",
    "        \n",
    "            self.sents = \"\"\n",
    "        else: self.read_data_structures()\n",
    "        \n",
    "    def get_discount(self):\n",
    "        try:\n",
    "            #discount values for trigram\n",
    "            counts = Counter(self.tnum.values())\n",
    "            y = counts[1] / (counts[1] + 2 * counts[2])\n",
    "            d_1 = 1 - 2 * y * counts[2] / counts[1]\n",
    "            d_2 = 2 - 3 * y * counts[3] / counts[2]\n",
    "            d_3 = 3 - 4 * y * counts[4] / counts[3]\n",
    "            self.tdis = [0, d_1, d_2, d_3]\n",
    "            \n",
    "            #discount values for bigram\n",
    "            bcounts = Counter(self.bnum.values())\n",
    "            by = bcounts[1] / (bcounts[1] + 2 * bcounts[2])\n",
    "            bd_1 = 1 - 2 * by * bcounts[2] / bcounts[1]\n",
    "            bd_2 = 2 - 3 * by * bcounts[3] / bcounts[2]\n",
    "            bd_3 = 3 - 4 * by * bcounts[4] / bcounts[3]\n",
    "            self.bdis = [0, bd_1, bd_2, bd_3]\n",
    "\n",
    "            #discount values for unigram\n",
    "            ucounts = Counter(self.unum.values())\n",
    "            uy = ucounts[1] / (ucounts[1] + 2 * ucounts[2])\n",
    "            ud_1 = 1 - 2 * uy * ucounts[2] / ucounts[1]\n",
    "            ud_2 = 2 - 3 * uy * ucounts[3] / ucounts[2]\n",
    "            ud_3 = 3 - 4 * uy * ucounts[4] / ucounts[3]\n",
    "            self.udis = [0, ud_1, ud_2, ud_3]\n",
    "            \n",
    "        except ZeroDivisionError:\n",
    "            raise Exception(\"Invalid *d* values; unable to predict. Dataset too small.\")\n",
    "        \n",
    "        \n",
    "    def traverse(self):\n",
    "        \n",
    "        for i in self.sents:\n",
    "            sent = word_tokenize(i)\n",
    "            for j in range(len(sent) - 2):\n",
    "                \n",
    "                tri, bi, two = \" \".join(sent[j:j+3]), \" \".join(sent[j:j+2]), \" \".join(sent[j+1:j+3])\n",
    "                \n",
    "                if bi in self.trigram:\n",
    "                    if two in self.trigram[bi]: self.trigram[bi][two] += 1\n",
    "                    else: self.trigram[bi].update({two : 1})\n",
    "                else: self.trigram.update({bi : {two : 1}})\n",
    "                \n",
    "                self.tnum[tri] += 1 #trigram counts, used to find discount values\n",
    "                self.tdenom[bi] += 1 #trigram denom\n",
    "                self.bnum[bi] += 1 #NOT bigram numerator; used to find discount values.\n",
    "                self.unum[sent[j]] += 1 #unigram counts\n",
    "                self.dot_w[sent[j+2]].add(sent[j+1]) #THIS IS bigram numerator\n",
    "                self.unique_bigram_holder.add(bi) #bigram denominator\n",
    "                self.w_dot[sent[j+1]].add(sent[j+2]) #bigram normalizer\n",
    "            \n",
    "            #for loop missed these\n",
    "            bi = \" \".join(sent[len(sent)-2:len(sent)])\n",
    "            self.tdenom[bi] += 1\n",
    "            self.bnum[bi] += 1\n",
    "            self.unum[len(sent) - 2] += 1\n",
    "            self.unum[len(sent) - 1] += 1\n",
    "            self.unique_bigram_holder.add(bi)\n",
    "            self.dot_w[sent[len(sent)-1]].add(sent[len(sent)-2])\n",
    "            self.w_dot[sent[len(sent)-2]].add(sent[len(sent)-1])\n",
    "        \n",
    "        for x in self.dot_w: self.dot_w[x] = len(self.dot_w[x])\n",
    "        for x in self.w_dot: self.w_dot[x] = len(self.w_dot[x])\n",
    "            \n",
    "        self.num_uni = sum(self.unum.values())\n",
    "        \n",
    "        self.num_unique_bigram = len(self.unique_bigram_holder)\n",
    "        self.unique_bigram_holder = set()\n",
    "    \n",
    "    def gamma(self):\n",
    "        '''\n",
    "        collapse trigram s.t. it is N1, N2, N3+\n",
    "        '''\n",
    "    \n",
    "        #uval = sum(unigram.values())\n",
    "        \n",
    "        nt = defaultdict(list)\n",
    "        \n",
    "        for i in self.trigram:\n",
    "\n",
    "            one = sum(1 for x in self.trigram[i] if self.trigram[i][x] == 1)\n",
    "            two = sum(1 for x in self.trigram[i] if self.trigram[i][x] == 2)\n",
    "            three = len(self.trigram[i]) - one - two\n",
    "            nt[i] = [one, two, three]\n",
    "            \n",
    "            self.bigamma[i] = (self.tdis[1] * one + self.tdis[2] * two + self.tdis[3] * three) / self.tdenom[i]\n",
    "        \n",
    "        \n",
    "        self.avg_bigamma = sum(self.bigamma.values()) / len(self.bigamma)\n",
    "        self.trigram = \"\"\n",
    "        \n",
    "    def write_data_structures(self):\n",
    "        with open(\"MKN_tnum.json\", \"w\") as f: json.dump(self.tnum, f)\n",
    "        with open(\"MKN_tdenom.json\", \"w\") as f: json.dump(self.tdenom, f)\n",
    "        with open(\"MKN_bnum.json\", \"w\") as f: json.dump(self.bnum, f)\n",
    "        with open(\"MKN_bigamma.json\", \"w\") as f: json.dump(self.bigamma, f)\n",
    "        with open(\"MKN_dot_w.json\", \"w\") as f: json.dump(self.dot_w, f)\n",
    "        with open(\"MKN_w_dot.json\", \"w\") as f: json.dump(self.w_dot, f)\n",
    "        with open(\"MKN_unum.json\", \"w\") as f: json.dump(self.unum, f)\n",
    "        with open(\"MKN_tdis.json\", \"w\") as f: json.dump(self.tdis, f)\n",
    "        with open(\"MKN_bdis.json\", \"w\") as f: json.dump(self.bdis, f)\n",
    "        with open(\"MKN_udis.json\", \"w\") as f: json.dump(self.udis, f)\n",
    "        with open(\"MKN_numbers.txt\", \"w\") as f: \n",
    "            f.write(str(self.avg_bigamma) + \"\\n\")\n",
    "            f.write(str(self.num_unique_bigram) + \"\\n\")\n",
    "            f.write(str(self.num_uni))\n",
    "        f.close()\n",
    "            \n",
    "    def read_data_structures(self):\n",
    "        self.tnum = defaultdict(int,json.load(open(\"MKN_tnum.json\", \"r\")))\n",
    "        self.tdenom = defaultdict(int,json.load(open(\"MKN_tdenom.json\", \"r\")))\n",
    "        self.bnum = defaultdict(int,json.load(open(\"MKN_bnum.json\", \"r\")))\n",
    "        self.bigamma = defaultdict(float,json.load(open(\"MKN_bigamma.json\", \"r\")))\n",
    "        self.dot_w = defaultdict(set,json.load(open(\"MKN_dot_w.json\", \"r\")))\n",
    "        self.w_dot = defaultdict(set,json.load(open(\"MKN_w_dot.json\", \"r\")))\n",
    "        self.unum = defaultdict(int,json.load(open(\"MKN_unum.json\", \"r\")))\n",
    "        self.tdis = json.load(open(\"MKN_tdis.json\", \"r\"))\n",
    "        self.bdis = json.load(open(\"MKN_bdis.json\", \"r\"))\n",
    "        self.udis = json.load(open(\"MKN_udis.json\", \"r\"))\n",
    "        nums = open(\"MKN_numbers.txt\", \"r\").read().split(\"\\n\")\n",
    "        self.avg_bigamma = float(nums[0])\n",
    "        self.num_unique_bigram = int(nums[1])\n",
    "        self.num_uni = int(nums[2])\n",
    "        \n",
    "    def score_phrase(self, l):\n",
    "        '''\n",
    "        l = string of 3 words.\n",
    "        '''\n",
    "        score, ls = 0, word_tokenize(l)\n",
    "        if len(ls) < 3: raise Exception(\"You must enter a phrase that will be tokenized into > 3 tokens.\")\n",
    "        left, l = ls[0] + \" \" + ls[1], \" \".join(ls)\n",
    "        \n",
    "        if self.tnum[\" \".join(ls)] == 0: #trigram does not exist\n",
    "            \n",
    "            if self.dot_w[ls[2]] == set(): #the bigram doesn't exist as well\n",
    "                if ls[2] not in self.unum: score += (1 / len(self.unum))\n",
    "                else:   \n",
    "                    score += 1e-3 * (self.unum[ls[2]] - self.udis[(3 if self.unum[ls[2]] > 3 else self.unum[ls[2]])] +\\\n",
    "                    (1 / len(self.unum)))\n",
    "            else:\n",
    "                d2 = self.bdis[(3 if self.dot_w[ls[2]] > 3 else self.dot_w[ls[2]])]\n",
    "                if self.bigamma[left] == 0: #this is a very bad case.\n",
    "                    if self.w_dot[ls[1]] == set():\n",
    "                        score += self.avg_bigamma * 0.01 *\\\n",
    "                        ((self.unum[ls[2]] - self.udis[(3 if self.unum[ls[2]] > 3 else self.unum[ls[2]])]) /\\\n",
    "                        self.num_uni + (1 / len(self.unum)))\n",
    "                    else:\n",
    "                        score += self.avg_bigamma * \\\n",
    "                        ((self.dot_w[ls[2]] - d2) / self.num_unique_bigram + (d2 / self.num_unique_bigram) *\\\n",
    "                        self.w_dot[ls[1]] * ((self.unum[ls[2]] - self.udis[(3 if self.unum[ls[2]] > 3 else self.unum[ls[2]])]) /\\\n",
    "                        self.num_uni + (1 / len(self.unum))))\n",
    "                else:\n",
    "                    score += self.bigamma[left] * \\\n",
    "                    ((self.dot_w[ls[2]] - d2) / self.num_unique_bigram + (d2 / self.num_unique_bigram) * self.w_dot[ls[1]] *\\\n",
    "                    ((self.unum[ls[2]] - self.udis[(3 if self.unum[ls[2]] > 3 else self.unum[ls[2]])]) / self.num_uni +\\\n",
    "                     (1 / len(self.unum))))\n",
    "        else: #trigram does exist\n",
    "            score += (self.tnum[l] - self.tdis[(3 if self.tnum[l] > 3 else self.tnum[l])]) / self.tdenom[left]\n",
    "            #if self.dot_w[ls[2]] == set(): #the bigram doesn't exist, should not happen because trigram exists.\n",
    "                #print(\"wtf is going on man\")\n",
    "            #else:\n",
    "            d2 = self.bdis[(3 if self.dot_w[ls[2]] > 3 else self.dot_w[ls[2]])]\n",
    "            score += self.bigamma[left] * \\\n",
    "            ((self.dot_w[ls[2]] - d2) / self.num_unique_bigram + (d2 / self.num_unique_bigram) * self.w_dot[ls[1]] *\\\n",
    "            ((self.unum[ls[2]] - self.udis[(3 if self.unum[ls[2]] > 3 else self.unum[ls[2]])]) / self.num_uni +\\\n",
    "            (1 / len(self.unum))))\n",
    "            \n",
    "        return math.log(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trigram(list_, temp = {}, freqs = {}):\n",
    "    '''\n",
    "    Takes a list of strings (which are sentences) and a dictionary with phrase frequencies\n",
    "    Returns a trigram and updated phrase frequencies.\n",
    "    '''\n",
    "    \n",
    "    for sent in list_:\n",
    "        \n",
    "        sent = word_tokenize(sent)\n",
    "        \n",
    "        for i in range(0, len(sent) - 2):\n",
    "            ot = sent[i] + \" \" + sent[i + 1]\n",
    "            tt = sent[i + 1] + \" \" + sent[i + 2]\n",
    "            if ot in temp:\n",
    "                if tt in temp[ot]:\n",
    "                    temp[ot][tt] += 1\n",
    "                else:\n",
    "                    temp[ot].update({tt : 1})\n",
    "            else:\n",
    "                temp.update({ot : {tt : 1}})\n",
    "    \n",
    "    for i in list(temp):\n",
    "        num = 0\n",
    "        for j in list(temp[i]):\n",
    "            num += temp[i][j]\n",
    "            if temp[i][j] in freqs:\n",
    "                freqs[i] += num\n",
    "            else:\n",
    "                freqs[i] = num\n",
    "\n",
    "    \n",
    "    return temp, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_chinese_regex():\n",
    "    '''\n",
    "    Returns a regex of various punctuations that matches 2+ occurences of said punctuation. The purpose is to take multiple\n",
    "    occurences of the punctuation and change it to one occurence e.g. ！！！ -> ！\n",
    "    Note that periods are not included (ellipses are 3 periods)\n",
    "    '''\n",
    "    sub = {\"!\":\"\\!\", \"?\":\"\\?\", \"(\":\"\\(\", \")\":\"\\)\", \"[\":\"\\[\", \"]\":\"\\]\", \n",
    "       \"*\":\"\\*\", \"$\":\"\\$\", \"^\":\"\\^\", \"{\":\"\\{\", \"}\":\"\\}\", \"+\":\"\\+\"}\n",
    "    rep = dict((re.escape(k), v) for k, v in sub.items())\n",
    "    pattern = re.compile(\"|\".join(rep.keys()))\n",
    "    PUNCT = \"|\".join(\"，！？；：（）［］【】。「」﹁﹂『』、‧《》〈〉﹏…—～“”\" + string.punctuation\n",
    "                     .replace(\".\", \"\").replace(\"\\\\\", \"\").replace(\"|\", \"\\|\")).replace(\"||\", \"|\") + \"|’|\\\\\\\\\"\n",
    "    return (\"\".join([x + \"{2,}\" if (x != \"|\" and x != \"\\\\\") else x for x in \n",
    "                    pattern.sub(lambda m: rep[re.escape(m.group())], PUNCT)]) + \"{2,}\").replace(\"\\\\|\", \"\\\\|{2,}\")\n",
    "\n",
    "for i in \"！@#%…&*（）—+|}{“：？》《。，/；’【】、\":\n",
    "    assert len(list(re.finditer(get_chinese_regex(), i * 2))) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_chinese(c, PUNCT):\n",
    "    \n",
    "    if \"...\" in c:\n",
    "        a = list(re.finditer(\"\\.{3,}\", c))\n",
    "        if a != []: \n",
    "            for x in a: c = c.replace(x.group(0), \"…\")\n",
    "    \n",
    "    if \" . . .\" in c:\n",
    "        a = list(re.finditer(\"(\\s\\.){3,}\", c))\n",
    "        if a != []: \n",
    "            for x in a: c = c.replace(x.group(0), \"…\")\n",
    "    \n",
    "    m = list(re.finditer(PUNCT, c))\n",
    "    if m != []:\n",
    "        for i in m: c = c.replace(i.group(), i.group()[0])\n",
    "    \n",
    "    final, temp, a = [], \"\", False\n",
    "    \n",
    "    for i in c:\n",
    "        x = ord(i)\n",
    "        if (97 <= x <= 122) or (65 <= x <= 90) or (48 <= x <= 57) or (x == ord(\".\")) or (x == ord(\"-\")):\n",
    "            temp += i\n",
    "            a = True\n",
    "        elif not ((97 <= x <= 122) or (65 <= x <= 90) or (48 <= x <= 57) or (x == ord(\".\")) or (x == ord(\"-\"))) and (a): \n",
    "            #end of # or [A-z], we now check if the token lacks whitespace\n",
    "            final.append(temp)\n",
    "            final.append(i)\n",
    "            temp, a = \"\", False\n",
    "        else: final.append(i)\n",
    "\n",
    "        if a and (c[len(c) - 1] == i): final.append(temp)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def possible_phrases(s, state, phrase_table, language_model, model, distortion_limit, d_penalty, word_penalty):\n",
    "    '''\n",
    "    Takes in a target sentence and state and returns all states that each have possible phrases appended to the state.\n",
    "    The sentence is a tokenized foreign sentence with whitespace delimiters between each word/character.\n",
    "    Rules:\n",
    "        - Each word can only be translated once (any returned phrase cannot overlap with b)\n",
    "        - Cannot violate distortion limit\n",
    "    '''\n",
    "    \n",
    "    possible = []\n",
    "    \n",
    "    for i in range(len(s) - 1, -1, -1):\n",
    "        for j in range(i):\n",
    "            x = re.sub(\"\\s+\", \" \", \" \".join(s[j:i+1]))\n",
    "            c = [1 if j <= k <= i else 0 for k in range(len(s))]\n",
    "            if (no_overlap(state[3], c)) and (math.fabs(state[4] + 1 - j) <= distortion_limit):\n",
    "                if x in phrase_table:\n",
    "                    for y in phrase_table[x]:\n",
    "                        phrase = state[1].strip() + \" \" + state[2].strip() + \" \" + y[0].strip()\n",
    "                        #print(phrase)\n",
    "                        t, l_score = phrase.split(), language_model_score(phrase, language_model, model)\n",
    "                        if len(t) == 3: w1, w2 = state[2], y[0]\n",
    "                        else: w1, w2 = t[len(t) - 2], t[len(t) - 1]\n",
    "                        b = [state[3][i] + c[i] for i in range(len(state[3]))]\n",
    "                        if word_penalty:\n",
    "                            alpha = state[0] + y[1][2] + l_score + d_penalty * math.fabs(state[4] + 1 - j) \\\n",
    "                                    + 2 ** len(y[0].split())\n",
    "                        else:\n",
    "                            alpha = state[0] + y[1][2] + l_score + d_penalty * math.fabs(state[4] + 1 - j)\n",
    "                        possible.append((alpha, w1, w2, b, i, state[5] + \" \" + y[0]))\n",
    "                       \n",
    "    return possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model_score(sent, l_model, model = \"trigram\"):\n",
    "    '''\n",
    "    Recieves a string\n",
    "    '''\n",
    "    score, sent = 0, word_tokenize(sent)\n",
    "    \n",
    "    if model == \"trigram\":\n",
    "    \n",
    "        for i in range(len(sent) - 2):\n",
    "            j, inner = sent[i] + \" \" + sent[i + 1], False\n",
    "            if j in l_model:\n",
    "                k = sent[i + 1] + \" \" + sent[i + 2]\n",
    "                inner = True\n",
    "                score += l_model[j][k] if k in l_model[j] else -100\n",
    "            if not inner: score += -100\n",
    "    \n",
    "    elif model == \"Kneser-Ney\":\n",
    "        \n",
    "        for i in range(len(sent) - 2):\n",
    "            score += l_model.score_phrase(\" \".join(sent[i:i+3]))\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def no_overlap(a, b):\n",
    "    for i in range(len(a)):\n",
    "        if (a[i] + b[i]) > 1: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eq(a, b):\n",
    "    for i in range(len(a) - 1):\n",
    "        if a[i] != b[i]: return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Hypothesis(object):\n",
    "    \n",
    "    def __init__(self, state, pointer):\n",
    "        self.state = state\n",
    "        self.pointer = pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(s, phrase_table, l_model, l_model_type, d_limit = 3, d_penalty = -1, n_best = 1, max_stack_size = 50, beamwidth = 100, word_penalty = True):\n",
    "    '''\n",
    "    Sentence: String of Chinese characters\n",
    "    Returns the n most likely translations given a \n",
    "    '''\n",
    "    \n",
    "    s = tokenize_chinese(s, get_chinese_regex())\n",
    "    \n",
    "    if len(s) == 1:\n",
    "        try:\n",
    "            print(phrase_table[s[0]][0][0], phrase_table[s[0]][0][1][2])\n",
    "            return\n",
    "        except KeyError:\n",
    "            print(\"Word did not appear in Chinese training data\")\n",
    "            return\n",
    "    hstack = [[(0, \"\\*\", \"\\*\", [0 for x in s], 0, \"\")] if x == 0 else [] for x in range(len(s))]\n",
    "    first = \" \".join(s)\n",
    "    \n",
    "    for i in range(len(s) - 1):\n",
    "        print(\"{}% done\".format(i / len(s) * 100))\n",
    "        if hstack[i] == []: continue\n",
    "        d = max(hstack[i], key = lambda x: x[0])[0] - beamwidth\n",
    "        for j in range(len(hstack[i])):\n",
    "            t = possible_phrases(s, hstack[i][j], phrase_table, l_model, l_model_type, d_limit, d_penalty, word_penalty)\n",
    "            for k in t: heappush(hstack[sum(k[3]) - 1], k)\n",
    "        if (i != len(s) - 2) and (hstack[i + 1] != []):\n",
    "            if max_stack_size != \"unlimited\":\n",
    "                while len(hstack[i + 1]) > max_stack_size: heappop(hstack[i + 1])\n",
    "            while (hstack[i + 1] != []) and (hstack[i + 1][0][0] < d) : heappop(hstack[i + 1])\n",
    "            #print(nlargest(1, hstack[i]), nsmallest(1, hstack[i]))\n",
    "                \n",
    "    for i in nlargest(n_best, hstack[len(s) - 1]): print(i[5], round(i[0],5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Sam/Desktop/Cal Poly Summer Research 2017/data/casia2015/casia_clean/data_structures\"\n",
    "os.chdir(path)\n",
    "\n",
    "phrase_table = json.load(open(path + \"/pruned_phrase_table.json\", \"r\"))\n",
    "l_model = ModifiedKneserNey([], type_ = \"read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sents = len(ngrameng)\n",
    "\n",
    "#real smoothed trigram\n",
    "#l_model = ModifiedKneserNey(ngrameng[:sents], \"train\")\n",
    "#l_model.write_data_structures()\n",
    "\n",
    "\n",
    "#mine\n",
    "#mylm, ubg = modified_kneser_ney_trigram(ngrameng[:sents], d = False, normalize = False, log_prob = True)\n",
    "\n",
    "#regular trigram\n",
    "gram, freqs = trigram(ngrameng[:], {}, {})\n",
    "for i in gram:\n",
    "    for j in gram[i]: gram[i][j] = math.log(gram[i][j] / freqs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "12.5% done\n",
      "25.0% done\n",
      "37.5% done\n",
      "50.0% done\n",
      "62.5% done\n",
      "75.0% done\n",
      " California management work with Every state has a legislature University 33.82778\n",
      " management work with California Every state has a legislature University 33.52778\n",
      " California management work with Every state has a legislature the University 33.46921\n",
      " California Every state has a legislature University management work with 33.42778\n",
      " California Every state has a legislature management work with University 33.42778\n",
      " management work with Every state has a legislature California University 33.32778\n",
      " Every state has a legislature California management work with University 33.32778\n",
      " California management work with Every state has a legislature University of 33.30491\n",
      " California management work with Every state has a legislature college students 33.2308\n",
      " California University Every state has a legislature management work with 33.22778\n",
      " Every state has a legislature University management work with California 33.22778\n",
      " management work with University Every state has a legislature California 33.22778\n",
      " California management work with Every state has a legislature college 33.17671\n",
      " management work with California Every state has a legislature the University 33.16921\n",
      " University Every state has a legislature California management work with 33.12778\n",
      " Every state has a legislature management work with California University 33.12778\n",
      " California Every state has a legislature the University management work with 33.06921\n",
      " California Every state has a legislature management work with the University 33.06921\n",
      " University Every state has a legislature management work with California 33.02778\n",
      " University management work with Every state has a legislature California 33.02778\n",
      " management work with California Every state has a legislature University of 33.00491\n",
      " management work with Every state has a legislature California the University 32.96921\n",
      " Every state has a legislature California management work with the University 32.96921\n",
      " California management work with Every state has a legislature university 32.93555\n",
      " management work with California Every state has a legislature college students 32.9308\n",
      " Every state has a legislature California University management work with 32.92778\n",
      " California Every state has a legislature University of management work with 32.90491\n",
      " California Every state has a legislature management work with University of 32.90491\n",
      " management work with California Every state has a legislature college 32.87671\n",
      " California the University Every state has a legislature management work with 32.86921\n",
      " Every state has a legislature the University management work with California 32.86921\n",
      " management work with the University Every state has a legislature California 32.86921\n",
      " California Every state has a legislature college students management work with 32.8308\n",
      " California Every state has a legislature management work with college students 32.8308\n",
      " management work with Every state has a legislature California University of 32.80491\n",
      " Every state has a legislature California management work with University of 32.80491\n",
      " California Every state has a legislature college management work with 32.77671\n",
      " California Every state has a legislature management work with college 32.77671\n",
      " the University Every state has a legislature California management work with 32.76921\n",
      " Every state has a legislature management work with California the University 32.76921\n",
      " management work with Every state has a legislature California college students 32.7308\n",
      " Every state has a legislature California management work with college students 32.7308\n",
      " Every state has a legislature University of management work with California 32.70491\n",
      " management work with University of Every state has a legislature California 32.70491\n",
      " management work with Every state has a legislature California college 32.67671\n",
      " Every state has a legislature California management work with college 32.67671\n",
      " the University Every state has a legislature management work with California 32.66921\n",
      " the University management work with Every state has a legislature California 32.66921\n",
      " management work with California Every state has a legislature university 32.63555\n",
      " Every state has a legislature college students management work with California 32.6308\n",
      " management work with college students Every state has a legislature California 32.6308\n",
      " University of Every state has a legislature California management work with 32.60491\n",
      " Every state has a legislature management work with California University of 32.60491\n",
      " the California management work with Every state has a legislature University 32.59566\n",
      " Every state has a legislature college management work with California 32.57671\n",
      " management work with college Every state has a legislature California 32.57671\n",
      " Every state has a legislature California the University management work with 32.56921\n",
      " California Every state has a legislature university management work with 32.53555\n",
      " California Every state has a legislature management work with university 32.53555\n",
      " college students Every state has a legislature California management work with 32.5308\n",
      " Every state has a legislature management work with California college students 32.5308\n",
      " University of Every state has a legislature management work with California 32.50491\n",
      " University of management work with Every state has a legislature California 32.50491\n",
      " college Every state has a legislature California management work with 32.47671\n",
      " Every state has a legislature management work with California college 32.47671\n",
      " management work with Every state has a legislature California university 32.43555\n",
      " Every state has a legislature California management work with university 32.43555\n",
      " college students Every state has a legislature management work with California 32.4308\n",
      " college students management work with Every state has a legislature California 32.4308\n",
      " Every state has a legislature California University of management work with 32.40491\n",
      " college Every state has a legislature management work with California 32.37671\n",
      " college management work with Every state has a legislature California 32.37671\n",
      " Every state has a legislature university management work with California 32.33555\n",
      " management work with university Every state has a legislature California 32.33555\n",
      " Every state has a legislature California college students management work with 32.3308\n",
      " management work with the California Every state has a legislature University 32.29566\n",
      " Every state has a legislature California college management work with 32.27671\n",
      " California management work with Every state has a legislature universities 32.24937\n",
      " the California management work with Every state has a legislature the University 32.23709\n",
      " Every state has a legislature management work with California university 32.23555\n",
      " university Every state has a legislature California management work with 32.23555\n",
      " the California Every state has a legislature University management work with 32.19566\n",
      " the California Every state has a legislature management work with University 32.19566\n",
      " university Every state has a legislature management work with California 32.13555\n",
      " university management work with Every state has a legislature California 32.13555\n",
      " Every state has a legislature the California management work with University 32.09566\n",
      " management work with Every state has a legislature the California University 32.09566\n",
      " California management work with Every state has a legislature students 32.08358\n",
      " the California management work with Every state has a legislature University of 32.07279\n",
      " Every state has a legislature California university management work with 32.03555\n",
      " the California management work with Every state has a legislature college students 31.99868\n",
      " management work with University Every state has a legislature the California 31.99566\n",
      " Every state has a legislature University management work with the California 31.99566\n",
      " management work with California Every state has a legislature universities 31.94937\n",
      " the California management work with Every state has a legislature college 31.94459\n",
      " management work with the California Every state has a legislature the University 31.93709\n",
      " Every state has a legislature management work with the California University 31.89566\n",
      " University Every state has a legislature the California management work with 31.89566\n",
      " California Every state has a legislature universities management work with 31.84937\n",
      " California Every state has a legislature management work with universities 31.84937\n"
     ]
    }
   ],
   "source": [
    "decode(\"加州理工州立大学\", \n",
    "       phrase_table,\n",
    "       l_model,\n",
    "       l_model_type = \"Kneser-Ne\",\n",
    "       d_limit = 7,\n",
    "       d_penalty = -0.05,\n",
    "       n_best = 100,\n",
    "       beamwidth = 20,\n",
    "       max_stack_size = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0% done\n",
      "12.5% done\n",
      "25.0% done\n",
      "37.5% done\n",
      "50.0% done\n",
      "62.5% done\n",
      "75.0% done\n",
      " I love the summer studying -27.04098\n",
      " `` I love the summer studying -27.0811\n",
      " Study on the summer , I love -28.85899\n",
      " Study on the summer . I love -30.56285\n",
      " Study on the summer and I love -31.0787\n",
      " Study on the summer I -31.28003\n",
      " and I love the summer studying -31.392\n",
      " Study on the season , I love -32.1323\n",
      " Study on the summer , I love the -32.19723\n",
      " Study on the season . I love -32.54254\n",
      " Study on the hot summer days . I love -32.70928\n",
      " Study on the hot summer days , I love -32.92159\n",
      " Study on the summer months , I love -33.02673\n",
      " Study on the days I -33.30778\n",
      " Study on the season I -33.42991\n",
      " Study on the `` I love the summer -33.46767\n",
      " the summer , I love studying -33.52906\n",
      " Study on the season `` I love -33.71245\n",
      " Study on the hot summer days and I love -33.77165\n",
      " Study on the summer months . I love -33.78047\n"
     ]
    }
   ],
   "source": [
    "decode(\"我爱夏季统计研究\", \n",
    "       phrase_table, \n",
    "       gram,\n",
    "       l_model_type = \"trigram\",\n",
    "       d_limit = 7, \n",
    "       d_penalty = -0.15, \n",
    "       n_best = 20, \n",
    "       beamwidth = 50, \n",
    "       max_stack_size = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
