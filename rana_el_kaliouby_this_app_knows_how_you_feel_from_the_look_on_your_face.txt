Talk	en	zh-tw
rana_el_kaliouby_this_app_knows_how_you_feel_from_the_look_on_your_face	"Our emotions influence every aspect of our lives, from our health and how we learn, to how we do business and make decisions, big ones and small. Our emotions also influence how we connect with one another. We've evolved to live in a world like this, but instead, we're living more and more of our lives like this — this is the text message from my daughter last night — in a world that's devoid of emotion. So I'm on a mission to change that. I want to bring emotions back into our digital experiences. I started on this path 15 years ago. I was a computer scientist in Egypt, and I had just gotten accepted to a Ph.D. program at Cambridge University. So I did something quite unusual for a young newlywed Muslim Egyptian wife: With the support of my husband, who had to stay in Egypt, I packed my bags and I moved to England. At Cambridge, thousands of miles away from home, I realized I was spending more hours with my laptop than I did with any other human. Yet despite this intimacy, my laptop had absolutely no idea how I was feeling. It had no idea if I was happy, having a bad day, or stressed, confused, and so that got frustrating. Even worse, as I communicated online with my family back home, I felt that all my emotions disappeared in cyberspace. I was homesick, I was lonely, and on some days I was actually crying, but all I had to communicate these emotions was this. (Laughter) Today's technology has lots of I.Q., but no E.Q.; lots of cognitive intelligence, but no emotional intelligence. So that got me thinking, what if our technology could sense our emotions? What if our devices could sense how we felt and reacted accordingly, just the way an emotionally intelligent friend would? Those questions led me and my team to create technologies that can read and respond to our emotions, and our starting point was the human face. So our human face happens to be one of the most powerful channels that we all use to communicate social and emotional states, everything from enjoyment, surprise, empathy and curiosity. In emotion science, we call each facial muscle movement an action unit. So for example, action unit 12, it's not a Hollywood blockbuster, it is actually a lip corner pull, which is the main component of a smile. Try it everybody. Let's get some smiles going on. Another example is action unit 4. It's the brow furrow. It's when you draw your eyebrows together and you create all these textures and wrinkles. We don't like them, but it's a strong indicator of a negative emotion. So we have about 45 of these action units, and they combine to express hundreds of emotions. Teaching a computer to read these facial emotions is hard, because these action units, they can be fast, they're subtle, and they combine in many different ways. So take, for example, the smile and the smirk. They look somewhat similar, but they mean very different things. (Laughter) So the smile is positive, a smirk is often negative. Sometimes a smirk can make you become famous. But seriously, it's important for a computer to be able to tell the difference between the two expressions. So how do we do that? We give our algorithms tens of thousands of examples of people we know to be smiling, from different ethnicities, ages, genders, and we do the same for smirks. And then, using deep learning, the algorithm looks for all these textures and wrinkles and shape changes on our face, and basically learns that all smiles have common characteristics, all smirks have subtly different characteristics. And the next time it sees a new face, it essentially learns that this face has the same characteristics of a smile, and it says, ""Aha, I recognize this. This is a smile expression."" So the best way to demonstrate how this technology works is to try a live demo, so I need a volunteer, preferably somebody with a face. (Laughter) Cloe's going to be our volunteer today. So over the past five years, we've moved from being a research project at MIT to a company, where my team has worked really hard to make this technology work, as we like to say, in the wild. And we've also shrunk it so that the core emotion engine works on any mobile device with a camera, like this iPad. So let's give this a try. As you can see, the algorithm has essentially found Cloe's face, so it's this white bounding box, and it's tracking the main feature points on her face, so her eyebrows, her eyes, her mouth and her nose. The question is, can it recognize her expression? So we're going to test the machine. So first of all, give me your poker face. Yep, awesome. (Laughter) And then as she smiles, this is a genuine smile, it's great. So you can see the green bar go up as she smiles. Now that was a big smile. Can you try a subtle smile to see if the computer can recognize? It does recognize subtle smiles as well. We've worked really hard to make that happen. And then eyebrow raised, indicator of surprise. Brow furrow, which is an indicator of confusion. Frown. Yes, perfect. So these are all the different action units. There's many more of them. This is just a slimmed-down demo. But we call each reading an emotion data point, and then they can fire together to portray different emotions. So on the right side of the demo — look like you're happy. So that's joy. Joy fires up. And then give me a disgust face. Try to remember what it was like when Zayn left One Direction. (Laughter) Yeah, wrinkle your nose. Awesome. And the valence is actually quite negative, so you must have been a big fan. So valence is how positive or negative an experience is, and engagement is how expressive she is as well. So imagine if Cloe had access to this real-time emotion stream, and she could share it with anybody she wanted to. Thank you. (Applause) So, so far, we have amassed 12 billion of these emotion data points. It's the largest emotion database in the world. We've collected it from 2.9 million face videos, people who have agreed to share their emotions with us, and from 75 countries around the world. It's growing every day. It blows my mind away that we can now quantify something as personal as our emotions, and we can do it at this scale. So what have we learned to date? Gender. Our data confirms something that you might suspect. Women are more expressive than men. Not only do they smile more, their smiles last longer, and we can now really quantify what it is that men and women respond to differently. Let's do culture: So in the United States, women are 40 percent more expressive than men, but curiously, we don't see any difference in the U.K. between men and women. (Laughter) Age: People who are 50 years and older are 25 percent more emotive than younger people. Women in their 20s smile a lot more than men the same age, perhaps a necessity for dating. But perhaps what surprised us the most about this data is that we happen to be expressive all the time, even when we are sitting in front of our devices alone, and it's not just when we're watching cat videos on Facebook. We are expressive when we're emailing, texting, shopping online, or even doing our taxes. Where is this data used today? In understanding how we engage with media, so understanding virality and voting behavior; and also empowering or emotion-enabling technology, and I want to share some examples that are especially close to my heart. Emotion-enabled wearable glasses can help individuals who are visually impaired read the faces of others, and it can help individuals on the autism spectrum interpret emotion, something that they really struggle with. In education, imagine if your learning apps sense that you're confused and slow down, or that you're bored, so it's sped up, just like a great teacher would in a classroom. What if your wristwatch tracked your mood, or your car sensed that you're tired, or perhaps your fridge knows that you're stressed, so it auto-locks to prevent you from binge eating. (Laughter) I would like that, yeah. What if, when I was in Cambridge, I had access to my real-time emotion stream, and I could share that with my family back home in a very natural way, just like I would've if we were all in the same room together? I think five years down the line, all our devices are going to have an emotion chip, and we won't remember what it was like when we couldn't just frown at our device and our device would say, ""Hmm, you didn't like that, did you?"" Our biggest challenge is that there are so many applications of this technology, my team and I realize that we can't build them all ourselves, so we've made this technology available so that other developers can get building and get creative. We recognize that there are potential risks and potential for abuse, but personally, having spent many years doing this, I believe that the benefits to humanity from having emotionally intelligent technology far outweigh the potential for misuse. And I invite you all to be part of the conversation. The more people who know about this technology, the more we can all have a voice in how it's being used. So as more and more of our lives become digital, we are fighting a losing battle trying to curb our usage of devices in order to reclaim our emotions. So what I'm trying to do instead is to bring emotions into our technology and make our technologies more responsive. So I want those devices that have separated us to bring us back together. And by humanizing technology, we have this golden opportunity to reimagine how we connect with machines, and therefore, how we, as human beings, connect with one another. Thank you. (Applause)"	我們的情緒會影響日常生活的各個層面，從我們的健康到如何學習、如何做事、做決定，無論事情大小都受此影響。我們的情緒也會影響我們如何與他人交流。我們已經進化到生活在一個像這樣的世界，然而我們的生活卻愈來愈像這樣──這是我女兒昨晚傳來的簡訊──一個缺乏情感的世界。所以我帶著使命要改變這種狀況。我想將情感重新注入數位體驗中。 我在 15 年前走上這條路。當時我在埃及是電腦科學家，而且我才拿到劍橋大學博士班的入學許可。所以我做了一件對身為年輕新婚的埃及回教婦女來說相當不尋常的事：在我先生的支持下，他留在埃及，我整理行囊搬到英格蘭。在劍橋，離家千里遠的地方，我發現我與筆電相處的時間，遠超過與人交流的時間。儘管與筆電相處如此親密，它卻完全不了解我的感受，它不知道我是否開心，今天順不順，是否緊張或困惑，所以那令我沮喪。更糟的是，在我上線與遠方的家人聯絡時，我覺得我的情感在這虛擬空間裡消失無蹤。我好想家，我好孤單，有些日子我真的哭了，但我所能傳達的只有這個。（笑聲）今天的科技有很多智商，卻沒有情緒智商；有很多認知智商，卻沒有情緒智商。所以這讓我思考，如果我們的科技可以感受我們的情緒會怎樣？如果我們的電子裝置可以感受我們的感覺並做出相對回應，就像一位高情商的朋友一樣，會是怎樣？這些問題讓我及我的團隊創造出可以讀懂情緒並做出回應的科技，我們的起始點是人的臉。 人類的臉恰好就是有力的管道，能用來傳遞社交及情緒狀態，從愉快、驚訝，到同情、好奇都可以。情緒科學中，我們稱每一種顏面肌肉運動為一個動作單位。舉例來說，動作單位 12，這可不是好萊塢的動作巨片，這其實是拉嘴角，這是微笑的主要部分。大家都試一下吧！讓會場有點笑容。另一個例子是動作單位 4。這是蹙額。就是你把眉頭皺在一起所產生的紋理和皺紋。我們都不喜歡皺紋，但那是負面情緒的重要指標。我們有約 45 種動作單位，排列組合後可以表現出數百種情緒。 要教電腦讀懂這些顏面表情很難，因為這些動作單位很快、很細微，而且還有各種不同的組合法。所以再舉個例子，微笑和假笑。兩者看起來有點像，但是意義大不相同。（笑聲）微笑是正面的，假笑往往是負面的。有時候一個假笑可以讓你成名。但是說真的，要讓電腦能夠辨認出這兩種表情的不同很重要。 所以我們怎麼做呢？我們給我們的演算法成千上萬筆我們知道在微笑的例子，各式人種、年齡、性別都有，假笑也如法泡製。然後，機器用深度學習法，讓演算法找出臉上所有的紋理、皺紋，及臉型的改變，基本上學得所有的微笑都有共同的特點，所有的假笑也有稍稍不同的特點，所以下一次電腦看到新的面孔，它基本上會得知這張臉與微笑有相同的特點，然後它會說，「啊哈！我認得這個，這是微笑的表情。」 要展示怎麼用這項科技的最佳方法，就是來一個現場示範，所以我需要一名志願者，最好是有臉的。（笑聲）我們今天的志願者是克蘿伊。 過去五年，我們從麻省理工的一項研究計畫發展成一家公司，我的團隊很努力讓這項科技能快速傳播，好像我們常說的，（病毒）擴散中。我們也把它縮小，讓核心情緒引擎能用在任何有照相機的行動裝置上，像是這台 iPad。現在來試一下。 正如你們所見，基本上演算法已經找到了克蘿伊的臉，就是這個白色的框框，它正在找她臉上的幾個主要特徵點，像是她的眉毛、眼睛、嘴巴和鼻子。問題是，它能辨識她的表情嗎？我們來考一下機器。首先，來一張撲克臉。對，好極了！（笑聲）然後她微笑的時後，這是真誠的微笑，很棒，你們可以看到她微笑的時候，綠色的信號格增加。那可是個好大的微笑。你可以試一下淺淺的微笑嗎？看看電腦能不能辨識？它的確也能辨識淺淺的微笑。我們真的很努力要做到這一點。然後抬眉毛，表示驚訝。蹙額，表示困惑。皺眉，很好，很完美。這些就是不同的動作單位。還有更多。這只是瘦身版示範。我們稱每一個讀取為一個情緒資料點，然後它們一起發動就能描繪出不同的情緒。右邊的這張示範──表現你很開心。所以那是高興。高興出現了。然後給我一張噁心的臉。試著回想贊恩退出男團一世代的那種感覺。（笑聲）沒錯，皺鼻子。太棒了！效價呈現高負值，所以你一定是大粉絲。效價指的是感受的好壞程度，而投入程度指的是她的表情有多大。想像一下如果克羅伊能使用這套即時情緒串流，而且她還可以跟任何人分享。謝謝妳！（掌聲） 到目前為止我們已經累積了 120 億筆情緒數據點。這是世界上最大的情緒資料庫。我們從 290 萬筆臉孔短片收集資料，由同意與我們分享他們情緒的人提供，來源遍及全球 75 個國家。資料每天都在增加。 這真令我驚異萬分，我們能量化像情緒這麼個人的東西，還能做到這個地步。 所以至今我們學到什麼？性別。我們的數據證實了一些你們大概已經料到的事。女人的表情比男人的更豐富。她們不但更常微笑，微笑的時間還更久，而且我們現在真的能量化造成男女不同反應的東西。來看文化：在美國，女性比男性多 40% 更願意表達情感，但奇怪的是，在英國看不到這樣的差距。（笑聲）再看年齡：50 歲以上的人比年輕人多 25% 更願意表現情感。20 多歲的女性比同年齡的男性更常微笑，大概是因為這是約會必殺技。但是這筆數據最讓我們訝異的，大概是我們隨時都有表情，即使我們獨自坐在裝置前也是如此，而且不只是在我們看臉書上貓短片的的時候。我們在寫信、傳簡訊、網購，甚至在報稅時都表情豐富。 今天這筆數據用在哪裡呢？用在瞭解我們如何與媒體互動，所以能瞭解影片爆紅及投票行為，也用在情緒辨識科技，我想分享幾個讓我特別感動的例子。情緒辨識眼鏡能幫助視障者讀取別人臉上的表情，也能幫助各種程度的自閉症患者解讀情緒，這是他們的最大難題。在教育上，想像一下如果你的學習應用程式感受到你的困惑並放慢速度，或是知道你覺得無聊了所以加快速度，就像一位好老師在課堂上做的一樣。如果你的手錶能追蹤你的心情，或是你的車能感受到你現在很疲倦，或是你的冰箱能知道你現在壓力很大，所以它會自動鎖住，你就不能拿東西來吃。（笑聲）我會喜歡那個，真的。當我在劍橋的時候，如果我能用這套即時情緒串流工具，我就能用非常自然的方法與遠在家鄉的家人分享，就好像我們都在同一間房間一樣，那有多好？ 我想五年後，我們所有的裝置都會有一個情緒晶片，我們就會忘記當年裝置還不會回應我們皺眉的時候說出：「嗯，你不喜歡這個，是吧？」是什麼樣子。我們最大的挑戰是這種科技有許多應用程式，我和我的團隊瞭解我們不可能只靠自己發展全部，所以我們開放這項科技讓其他開發者能繼續開發並激發創意。我們知道會有潛在風險，也可能遭到濫用，但是個人認為，在花了這麼多年做這個之後，我相信這對人類的益處，就是開發情緒智能科技的益處，遠超過誤用的潛在危險。我請大家口耳相傳。愈多人知道這項科技，我們就愈能發聲說明這該如何使用。隨著我們的生活愈來愈數位化，試圖以遏止使用裝置來重拾情緒是一場必敗的仗。與其如此，我寧可把情感帶進科技，讓我們的科技更有回應。所以我想用這些原本使我們疏遠的裝置，讓我們重新結合在一起。藉著把科技人性化，我們擁有這個黃金時機來重新想像我們如何與機器連結，進而想像我們身為人類如何能重新連結彼此。 謝謝。 （掌聲）
