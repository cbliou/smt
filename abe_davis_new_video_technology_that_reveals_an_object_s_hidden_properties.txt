Talk	en	zh-tw
abe_davis_new_video_technology_that_reveals_an_object_s_hidden_properties	"Most of us think of motion as a very visual thing. If I walk across this stage or gesture with my hands while I speak, that motion is something that you can see. But there's a world of important motion that's too subtle for the human eye, and over the past few years, we've started to find that cameras can often see this motion even when humans can't. So let me show you what I mean. On the left here, you see video of a person's wrist, and on the right, you see video of a sleeping infant, but if I didn't tell you that these were videos, you might assume that you were looking at two regular images, because in both cases, these videos appear to be almost completely still. But there's actually a lot of subtle motion going on here, and if you were to touch the wrist on the left, you would feel a pulse, and if you were to hold the infant on the right, you would feel the rise and fall of her chest as she took each breath. And these motions carry a lot of significance, but they're usually too subtle for us to see, so instead, we have to observe them through direct contact, through touch. But a few years ago, my colleagues at MIT developed what they call a motion microscope, which is software that finds these subtle motions in video and amplifies them so that they become large enough for us to see. And so, if we use their software on the left video, it lets us see the pulse in this wrist, and if we were to count that pulse, we could even figure out this person's heart rate. And if we used the same software on the right video, it lets us see each breath that this infant takes, and we can use this as a contact-free way to monitor her breathing. And so this technology is really powerful because it takes these phenomena that we normally have to experience through touch and it lets us capture them visually and non-invasively. So a couple years ago, I started working with the folks that created that software, and we decided to pursue a crazy idea. We thought, it's cool that we can use software to visualize tiny motions like this, and you can almost think of it as a way to extend our sense of touch. But what if we could do the same thing with our ability to hear? What if we could use video to capture the vibrations of sound, which are just another kind of motion, and turn everything that we see into a microphone? Now, this is a bit of a strange idea, so let me try to put it in perspective for you. Traditional microphones work by converting the motion of an internal diaphragm into an electrical signal, and that diaphragm is designed to move readily with sound so that its motion can be recorded and interpreted as audio. But sound causes all objects to vibrate. Those vibrations are just usually too subtle and too fast for us to see. So what if we record them with a high-speed camera and then use software to extract tiny motions from our high-speed video, and analyze those motions to figure out what sounds created them? This would let us turn visible objects into visual microphones from a distance. And so we tried this out, and here's one of our experiments, where we took this potted plant that you see on the right and we filmed it with a high-speed camera while a nearby loudspeaker played this sound. (Music: ""Mary Had a Little Lamb"") And so here's the video that we recorded, and we recorded it at thousands of frames per second, but even if you look very closely, all you'll see are some leaves that are pretty much just sitting there doing nothing, because our sound only moved those leaves by about a micrometer. That's one ten-thousandth of a centimeter, which spans somewhere between a hundredth and a thousandth of a pixel in this image. So you can squint all you want, but motion that small is pretty much perceptually invisible. But it turns out that something can be perceptually invisible and still be numerically significant, because with the right algorithms, we can take this silent, seemingly still video and we can recover this sound. (Music: ""Mary Had a Little Lamb"") (Applause) So how is this possible? How can we get so much information out of so little motion? Well, let's say that those leaves move by just a single micrometer, and let's say that that shifts our image by just a thousandth of a pixel. That may not seem like much, but a single frame of video may have hundreds of thousands of pixels in it, and so if we combine all of the tiny motions that we see from across that entire image, then suddenly a thousandth of a pixel can start to add up to something pretty significant. On a personal note, we were pretty psyched when we figured this out. (Laughter) But even with the right algorithm, we were still missing a pretty important piece of the puzzle. You see, there are a lot of factors that affect when and how well this technique will work. There's the object and how far away it is; there's the camera and the lens that you use; how much light is shining on the object and how loud your sound is. And even with the right algorithm, we had to be very careful with our early experiments, because if we got any of these factors wrong, there was no way to tell what the problem was. We would just get noise back. And so a lot of our early experiments looked like this. And so here I am, and on the bottom left, you can kind of see our high-speed camera, which is pointed at a bag of chips, and the whole thing is lit by these bright lamps. And like I said, we had to be very careful in these early experiments, so this is how it went down. (Video) Abe Davis: Three, two, one, go. Mary had a little lamb! Little lamb! Little lamb! (Laughter) AD: So this experiment looks completely ridiculous. (Laughter) I mean, I'm screaming at a bag of chips — (Laughter) — and we're blasting it with so much light, we literally melted the first bag we tried this on. (Laughter) But ridiculous as this experiment looks, it was actually really important, because we were able to recover this sound. (Audio) Mary had a little lamb! Little lamb! Little lamb! (Applause) AD: And this was really significant, because it was the first time we recovered intelligible human speech from silent video of an object. And so it gave us this point of reference, and gradually we could start to modify the experiment, using different objects or moving the object further away, using less light or quieter sounds. And we analyzed all of these experiments until we really understood the limits of our technique, because once we understood those limits, we could figure out how to push them. And that led to experiments like this one, where again, I'm going to speak to a bag of chips, but this time we've moved our camera about 15 feet away, outside, behind a soundproof window, and the whole thing is lit by only natural sunlight. And so here's the video that we captured. And this is what things sounded like from inside, next to the bag of chips. (Audio) Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go. AD: And here's what we were able to recover from our silent video captured outside behind that window. (Audio) Mary had a little lamb whose fleece was white as snow, and everywhere that Mary went, that lamb was sure to go. (Applause) AD: And there are other ways that we can push these limits as well. So here's a quieter experiment where we filmed some earphones plugged into a laptop computer, and in this case, our goal was to recover the music that was playing on that laptop from just silent video of these two little plastic earphones, and we were able to do this so well that I could even Shazam our results. (Laughter) (Music: ""Under Pressure"" by Queen) (Applause) And we can also push things by changing the hardware that we use. Because the experiments I've shown you so far were done with a camera, a high-speed camera, that can record video about a 100 times faster than most cell phones, but we've also found a way to use this technique with more regular cameras, and we do that by taking advantage of what's called a rolling shutter. You see, most cameras record images one row at a time, and so if an object moves during the recording of a single image, there's a slight time delay between each row, and this causes slight artifacts that get coded into each frame of a video. And so what we found is that by analyzing these artifacts, we can actually recover sound using a modified version of our algorithm. So here's an experiment we did where we filmed a bag of candy while a nearby loudspeaker played the same ""Mary Had a Little Lamb"" music from before, but this time, we used just a regular store-bought camera, and so in a second, I'll play for you the sound that we recovered, and it's going to sound distorted this time, but listen and see if you can still recognize the music. (Audio: ""Mary Had a Little Lamb"") And so, again, that sounds distorted, but what's really amazing here is that we were able to do this with something that you could literally run out and pick up at a Best Buy. So at this point, a lot of people see this work, and they immediately think about surveillance. And to be fair, it's not hard to imagine how you might use this technology to spy on someone. But keep in mind that there's already a lot of very mature technology out there for surveillance. In fact, people have been using lasers to eavesdrop on objects from a distance for decades. But what's really new here, what's really different, is that now we have a way to picture the vibrations of an object, which gives us a new lens through which to look at the world, and we can use that lens to learn not just about forces like sound that cause an object to vibrate, but also about the object itself. And so I want to take a step back and think about how that might change the ways that we use video, because we usually use video to look at things, and I've just shown you how we can use it to listen to things. But there's another important way that we learn about the world: that's by interacting with it. We push and pull and poke and prod things. We shake things and see what happens. And that's something that video still won't let us do, at least not traditionally. So I want to show you some new work, and this is based on an idea I had just a few months ago, so this is actually the first time I've shown it to a public audience. And the basic idea is that we're going to use the vibrations in a video to capture objects in a way that will let us interact with them and see how they react to us. So here's an object, and in this case, it's a wire figure in the shape of a human, and we're going to film that object with just a regular camera. So there's nothing special about this camera. In fact, I've actually done this with my cell phone before. But we do want to see the object vibrate, so to make that happen, we're just going to bang a little bit on the surface where it's resting while we record this video. So that's it: just five seconds of regular video, while we bang on this surface, and we're going to use the vibrations in that video to learn about the structural and material properties of our object, and we're going to use that information to create something new and interactive. And so here's what we've created. And it looks like a regular image, but this isn't an image, and it's not a video, because now I can take my mouse and I can start interacting with the object. And so what you see here is a simulation of how this object would respond to new forces that we've never seen before, and we created it from just five seconds of regular video. (Applause) And so this is a really powerful way to look at the world, because it lets us predict how objects will respond to new situations, and you could imagine, for instance, looking at an old bridge and wondering what would happen, how would that bridge hold up if I were to drive my car across it. And that's a question that you probably want to answer before you start driving across that bridge. And of course, there are going to be limitations to this technique, just like there were with the visual microphone, but we found that it works in a lot of situations that you might not expect, especially if you give it longer videos. So for example, here's a video that I captured of a bush outside of my apartment, and I didn't do anything to this bush, but by capturing a minute-long video, a gentle breeze caused enough vibrations that we could learn enough about this bush to create this simulation. (Applause) And so you could imagine giving this to a film director, and letting him control, say, the strength and direction of wind in a shot after it's been recorded. Or, in this case, we pointed our camera at a hanging curtain, and you can't even see any motion in this video, but by recording a two-minute-long video, natural air currents in this room created enough subtle, imperceptible motions and vibrations that we could learn enough to create this simulation. And ironically, we're kind of used to having this kind of interactivity when it comes to virtual objects, when it comes to video games and 3D models, but to be able to capture this information from real objects in the real world using just simple, regular video, is something new that has a lot of potential. So here are the amazing people who worked with me on these projects. (Applause) And what I've shown you today is only the beginning. We've just started to scratch the surface of what you can do with this kind of imaging, because it gives us a new way to capture our surroundings with common, accessible technology. And so looking to the future, it's going to be really exciting to explore what this can tell us about the world. Thank you. (Applause)"	絕大部分的人認為「動作」是非常視覺的。如果我走過這個舞台，或在說話的時候使用手勢，這些動作都是你可以看見的。但有一些細微的重要動作，是人類的眼睛無法看見的。而在過去的幾年中，我們發現鏡頭，能夠看到這些人類所看不見的動作。 所以讓我來帶大家了解一下，在左邊，你可以看到一個人手腕的影片；在右邊，你可以看到一個睡著嬰兒的影片。但是如果我沒有告訴你這是影片的話，你有可能會認為你只是在看兩張非常普通的圖片。因為在這兩個情況之下，這些影片看起來只是完全靜止的。事實上這裏發生著許多細微的動作。如果你觸碰左邊手腕的話，你會感覺到脈搏。如果你抱起這個右邊嬰兒的話，你會感覺到她的胸部上下起伏，當她正在呼吸的時候。這些動作十分重要，但對我們來說太細微以至於我們看不到。所以我們要用直接的觸碰去感知它們。 但是，幾年前，我麻省理工的同事研究出了動作顯微鏡。這軟體可用來尋找影片中的細微動作，然後將它們放大使得我們可以看到。所以說，如果我們在左邊的影片上使用那個軟體，它能讓我們看到腕部的脈搏。而且如果我們數一數脈搏的話，我們甚至可以得出這個人的心率。如果我們在右邊的影片上用同一軟體的話，它可以讓我們看到嬰兒的每一個呼吸。我們可以將這個軟體視為不用接觸就能觀察嬰兒呼吸的探測器。 所以這種科技非常強大，它記錄的現象是原本我們得觸摸才能感受到的現象，而且它可以讓我們可視地、無創地觀察他們。 所以數年以前，我開始和這些開發軟體的人一起工作，然後我們決定去追尋一個瘋狂的主意。我們認為用這個軟體發現細小的動作是很酷炫的，而且可以將它視為我們觸覺的延伸。但是如果我們可以做出擴展我們聽覺的軟體呢？如果我們可以通過影片從而獲得聲音的振動，振動是另外一種動作，然後將我們看到的所有東西轉化為聲音進入麥克風呢？ 這是一個有點奇怪的主意，所以讓我將它變得更加易懂一些。傳統的麥克風通過將內部隔膜的振動轉換為電信號，設計讓隔膜隨著聲音方便移動。它的振動可以被記錄和轉換成聲音。但是聲音可以使任何物體產生振動。那些振動與我們來說太小太快，以至於我們不能看不見。 要是我們用高速錄影機記錄下振動，然後用軟體從高速錄影機的影片中分離出細微的動作，然後分析那些動作並且搞清楚是什麼聲音創造了振動呢？這樣我們可以在一定距離內將可視物體轉換到可視話筒中。然後我們進行了實驗。這裏可以看到我們的實驗。在右邊我們放置了一盆盆栽，然後我們用高速錄影機記錄了下來。同事在邊上用擴音器發出這個聲音。 （音樂：“瑪麗有隻小綿羊”） 然後下面是我們記錄下的聲音。我們每秒鐘記錄下上千次畫面，但是就算你再仔細地看，你只會看到一些好看的樹葉，就只是靜止在那什麼也不做。那是因為我們的聲音只移動了樹葉大約一微米的距離，那個距離大概是萬分之一釐米，在千分之一和百分之一之間，只是這一個圖像的像素點。所以你可以盡量瞇著眼睛看，但是細小的動作是不容易被感知到的。但結果卻是一些不容易被看到的物體，在數字上仍然非常重要。因為當使用了正確的演算法之後，我們可以獲取這段看起來靜止無聲的影片，然後還原出聲音。 （音樂：“瑪麗有隻小綿羊”） （掌聲） 這是為什麼呢？我們是如何在這細小的動作中得到如此多訊息的呢？那麼讓我們假設這些樹葉只是移動了一點點距離，再者樹葉只是移動了千分之一個像素的距離。那看起來並不多，但是一個單一幀率的影片，可能有不計其數的像素。所以如果我們將這些細小的動作從整個畫面中截取出來的話，可以看到一個像素的千分之一可以累計變得十分重要。 就我個人來說，我們研究出來時高興得都要瘋了。（笑聲）但是就算擁有正確的算法，我們仍然會丟失整個拼圖中最重要的部分。你們知道有許多的因素會對這個技術正常工作造成影響。這些因素包括，物體離得有多少遠、拍攝的時候使用的鏡頭、有多少光照在物體上，還有放出的聲音多響。而且就算擁有正確的算法，我們在早期的試驗中必須十分小心。如果說我們弄錯了其中任何一個細節，我們沒有辦法找出問題所在，只會得到一段噪音。所以我們早期的研究是像這樣的。這就是我。在畫面的左下角可以看到我們的高速錄影機，它正對著一包洋芋片，所有的事物被一盞燈所照亮。就像我說的，我們需要格外小心。這就是這個試驗如何進行的。 （影片）亞伯戴維斯：三二一開始。瑪麗有隻小綿羊小綿羊！小綿羊！ （笑聲） 亞伯戴維斯：所以這個實驗看起來十分可笑。（笑聲）我對著一袋洋芋片尖叫。（笑聲）我們在充足的光照下對著它大叫，我們確實將第一個實驗的洋芋融化了。（笑聲）儘管看上去很可笑，這確實是十分重要，因為我們可以復原這個聲音。 （聲音）瑪麗有隻小綿羊，小綿羊！小綿羊！ （掌聲） 這是十分重要的。因為這是第一次我們從一個物體靜止的影像中，復原出了清楚的人聲。所以這給了我們一個參考，並且可以逐漸去修改這個實驗。用不同的物體或者把物體移到更遠的地方，或者使用少量的光和更加輕的聲音。我們分析了實驗，直到我們弄清楚我們技術的侷限性在哪，因為只要我們明白它們的極限，就可以知道如何去推動它們。 我們的實驗就可能變成這一個，我在同一個地方再一次向一包洋芋片說話，但是這一次我們把攝影機往後移動了15英尺，放置在隔音玻璃後面，所有的東西僅僅是被太陽光所照亮。這是我們拍攝到的影片。聽起來這個聲音是從洋芋片內部發出來的。 （聲音）瑪麗有隻小綿羊，牠的毛白得像雪一樣。並且瑪麗走到哪裏，小綿羊就跟到哪裏。 這是我們能夠通過在玻璃外面捕捉的靜止影像中還原出來的。 （聲音）瑪麗有隻小綿羊，牠的毛白得像雪一樣。並且瑪麗走到哪裏，小綿羊就跟到哪裏。 （掌聲） 還有其他辦法去推動這些限制，所以下面是一個更安靜的實驗。我們拍攝了一些插在電腦上的耳機。我們的目標是還原出在手提電腦上所放出的聲音，從這兩個小耳機中的從靜止影片中得到。並且我們可以做得很好，甚至能夠用聽歌識曲軟體鑑別我們的結果。（笑聲） （音樂：皇后樂隊 “壓力之下”） （掌聲） 我們也可以通過改變硬體來推動事物。我給你們看的這些實驗都使用了攝影機，高速攝影機，我們可以比大多數手機快一百倍地記錄影片。但是我們也找到了用普通攝影機使用這一項技術的方法。我們採用普通照相機像百葉窗一樣記錄東西優點來記錄。你們知道，大多數照相機一段時間記錄一排的圖像，所以如果一個物體只在被記錄的圖像中移動，在記錄的每一排之間幾乎沒有延遲。這樣就可以使物體被記錄到影片的每一個部分之中。我們發現透過分析這些東西，實際上只是用了更改過的算法來還原出聲音。下面就是我們所做的實驗。我們拍攝了一袋糖，同時邊上有一個擴音器正在播放著與之前相同的“瑪麗有隻小綿羊”。但是這一次我們僅使用從商店買來的攝影機。馬上我就向你們播放我們還原出的聲音。這一次聲音聽起來有一些扭曲，但是請聽聽看能否分辨出這音樂。 （聲音：“瑪麗有隻小綿羊”） 聲音確實被扭曲了，但是神奇的是，我們能夠做這個事情，運用一些用完以後就可以在 Best Buy 買到的東西。 所以在這時很多人可以看到我們的研究結果，然後他們立刻會想到監視。公平的說，不難想到你們可以用這項技術去監視其他人。但是要記住早就有許多成熟的技術為監視所準備。事實上，人們數十年來使用雷射去竊聽別的事物。但是，這個技術新穎的地方、完全不同的地方，是我們現在有辦法拍攝出物體的振動。讓我們獲得了觀察這個世界的新鏡頭，並且可以使用這鏡頭，不僅僅是為了去瞭解導致物體振動的聲音，還瞭解了物體本身。 所以我想往回退一步去思考這個技術會如何改變我們應用影片的方法。因為我們用影片通常來看一些東西，並且我剛才已經展示如何使用它去聽一些東西。但是另外有一個我們瞭解世界的重要方法，那就是和它互動。我們推、拉、戳、刺一些事物，我們搖動物體來明白發生了什麼事。那是影片無法做到的。至少過去不行。所以我想向你們展示一些新的成品，這源自我幾個月之前的想法，所以這是我第一次公眾展示。而且基本的想法就是我們會用影片之中的振動，來捕捉物體在某種程度上這樣可以使我們與物體互動，並且可以知道它們如何對我們進行反應。 這是一個物體。這是一個用線做成的人。我們要用普通的相機去拍攝它，所以這個相機沒有什麼特別之處。事實上我曾經用我的手機完成過這件事但是我們確實希望這個物體振動。所以為了做到這點，我們在拍攝的時候在它放置的地方敲擊。 這就是全部了：一個僅僅五秒鐘的普通影片，拍攝我們敲擊表面的時候。我們將要用這個影片的震動去瞭解這個物體的結構和組織組成，然後使用這個訊息去創造新穎和互動性的東西。這就是我們所創造的。這看起來像一個普通的圖片，但是這不是圖片也不是影片。因為我可以用我的游標，也可以和我這個物體互動。所以你們看到的是一個我們從來沒看到過的，關於物體如何對新的力量進行反應。我們只是使用了五秒鐘的影片。 （掌聲） 所以這是個十分有力的看世界的方法，讓我們能推測物體是如何在新環境做出反應的。並且可以想像，例如看一個古老的橋樑，並思考開車經過那座橋時它會如何支撐住。那是一個在你開車穿過之前，你會想先知道答案的問題。的確，這項技術還是有侷限性的，就像視覺麥克風也有缺陷一樣，但是我們發現它適用於許多情況，你可能沒有想到的，尤其是拍攝更長影片的時候。 例如，這是我拍的一段影片，是我公寓外的灌木叢。我沒有對這灌木叢做什麼事，但是透過拍攝一段一分鐘的影片，一陣輕風可以產生足夠的振動，我們可以足夠地瞭解這個灌木叢從而創造出這樣的模擬情況。（掌聲）所以你可以想像將這個技術給一個電影導演，讓他來控制影片拍攝完後的風力強度和方向。我們也將相機指向了一個掛著的窗簾，你幾乎看不到影片中有任何動作，但是拍攝兩分鐘的影片後，在這個房間中的天然氣流創造了足夠細微、不可被察覺的動作和振動，這樣我們也可以透過振動製造出模擬。 可笑的是，我們只是在虛擬的物體上，電視遊戲和3D模型中使用這種互動。但是僅僅使用簡單普通的影片去捕捉現實世界中的真實物體，仍然有很大的潛力。 這裏有許多傑出的人與我共同研究這些計劃。（掌聲） 我今天展示給你們看的只是個開始。我們僅僅開始挖出表面的一部分，看看這樣的成像技術能做到什麼事。因為它給了我們一個新的方法透過平常可得到的技術去捕捉周圍的東西。所以展望未來，探索這個技術可以告訴我們關於這個世界會變得格外激動人心。 謝謝。 （掌聲）
