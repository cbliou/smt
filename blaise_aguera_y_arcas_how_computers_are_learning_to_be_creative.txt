Talk	en	zh-tw
blaise_aguera_y_arcas_how_computers_are_learning_to_be_creative	"So, I lead a team at Google that works on machine intelligence; in other words, the engineering discipline of making computers and devices able to do some of the things that brains do. And this makes us interested in real brains and neuroscience as well, and especially interested in the things that our brains do that are still far superior to the performance of computers. Historically, one of those areas has been perception, the process by which things out there in the world — sounds and images — can turn into concepts in the mind. This is essential for our own brains, and it's also pretty useful on a computer. The machine perception algorithms, for example, that our team makes, are what enable your pictures on Google Photos to become searchable, based on what's in them. The flip side of perception is creativity: turning a concept into something out there into the world. So over the past year, our work on machine perception has also unexpectedly connected with the world of machine creativity and machine art. I think Michelangelo had a penetrating insight into to this dual relationship between perception and creativity. This is a famous quote of his: ""Every block of stone has a statue inside of it, and the job of the sculptor is to discover it."" So I think that what Michelangelo was getting at is that we create by perceiving, and that perception itself is an act of imagination and is the stuff of creativity. The organ that does all the thinking and perceiving and imagining, of course, is the brain. And I'd like to begin with a brief bit of history about what we know about brains. Because unlike, say, the heart or the intestines, you really can't say very much about a brain by just looking at it, at least with the naked eye. The early anatomists who looked at brains gave the superficial structures of this thing all kinds of fanciful names, like hippocampus, meaning ""little shrimp."" But of course that sort of thing doesn't tell us very much about what's actually going on inside. The first person who, I think, really developed some kind of insight into what was going on in the brain was the great Spanish neuroanatomist, Santiago Ramón y Cajal, in the 19th century, who used microscopy and special stains that could selectively fill in or render in very high contrast the individual cells in the brain, in order to start to understand their morphologies. And these are the kinds of drawings that he made of neurons in the 19th century. This is from a bird brain. And you see this incredible variety of different sorts of cells, even the cellular theory itself was quite new at this point. And these structures, these cells that have these arborizations, these branches that can go very, very long distances — this was very novel at the time. They're reminiscent, of course, of wires. That might have been obvious to some people in the 19th century; the revolutions of wiring and electricity were just getting underway. But in many ways, these microanatomical drawings of Ramón y Cajal's, like this one, they're still in some ways unsurpassed. We're still more than a century later, trying to finish the job that Ramón y Cajal started. These are raw data from our collaborators at the Max Planck Institute of Neuroscience. And what our collaborators have done is to image little pieces of brain tissue. The entire sample here is about one cubic millimeter in size, and I'm showing you a very, very small piece of it here. That bar on the left is about one micron. The structures you see are mitochondria that are the size of bacteria. And these are consecutive slices through this very, very tiny block of tissue. Just for comparison's sake, the diameter of an average strand of hair is about 100 microns. So we're looking at something much, much smaller than a single strand of hair. And from these kinds of serial electron microscopy slices, one can start to make reconstructions in 3D of neurons that look like these. So these are sort of in the same style as Ramón y Cajal. Only a few neurons lit up, because otherwise we wouldn't be able to see anything here. It would be so crowded, so full of structure, of wiring all connecting one neuron to another. So Ramón y Cajal was a little bit ahead of his time, and progress on understanding the brain proceeded slowly over the next few decades. But we knew that neurons used electricity, and by World War II, our technology was advanced enough to start doing real electrical experiments on live neurons to better understand how they worked. This was the very same time when computers were being invented, very much based on the idea of modeling the brain — of ""intelligent machinery,"" as Alan Turing called it, one of the fathers of computer science. Warren McCulloch and Walter Pitts looked at Ramón y Cajal's drawing of visual cortex, which I'm showing here. This is the cortex that processes imagery that comes from the eye. And for them, this looked like a circuit diagram. So there are a lot of details in McCulloch and Pitts's circuit diagram that are not quite right. But this basic idea that visual cortex works like a series of computational elements that pass information one to the next in a cascade, is essentially correct. Let's talk for a moment about what a model for processing visual information would need to do. The basic task of perception is to take an image like this one and say, ""That's a bird,"" which is a very simple thing for us to do with our brains. But you should all understand that for a computer, this was pretty much impossible just a few years ago. The classical computing paradigm is not one in which this task is easy to do. So what's going on between the pixels, between the image of the bird and the word ""bird,"" is essentially a set of neurons connected to each other in a neural network, as I'm diagramming here. This neural network could be biological, inside our visual cortices, or, nowadays, we start to have the capability to model such neural networks on the computer. And I'll show you what that actually looks like. So the pixels you can think about as a first layer of neurons, and that's, in fact, how it works in the eye — that's the neurons in the retina. And those feed forward into one layer after another layer, after another layer of neurons, all connected by synapses of different weights. The behavior of this network is characterized by the strengths of all of those synapses. Those characterize the computational properties of this network. And at the end of the day, you have a neuron or a small group of neurons that light up, saying, ""bird."" Now I'm going to represent those three things — the input pixels and the synapses in the neural network, and bird, the output — by three variables: x, w and y. There are maybe a million or so x's — a million pixels in that image. There are billions or trillions of w's, which represent the weights of all these synapses in the neural network. And there's a very small number of y's, of outputs that that network has. ""Bird"" is only four letters, right? So let's pretend that this is just a simple formula, x ""x"" w = y. I'm putting the times in scare quotes because what's really going on there, of course, is a very complicated series of mathematical operations. That's one equation. There are three variables. And we all know that if you have one equation, you can solve one variable by knowing the other two things. So the problem of inference, that is, figuring out that the picture of a bird is a bird, is this one: it's where y is the unknown and w and x are known. You know the neural network, you know the pixels. As you can see, that's actually a relatively straightforward problem. You multiply two times three and you're done. I'll show you an artificial neural network that we've built recently, doing exactly that. This is running in real time on a mobile phone, and that's, of course, amazing in its own right, that mobile phones can do so many billions and trillions of operations per second. What you're looking at is a phone looking at one after another picture of a bird, and actually not only saying, ""Yes, it's a bird,"" but identifying the species of bird with a network of this sort. So in that picture, the x and the w are known, and the y is the unknown. I'm glossing over the very difficult part, of course, which is how on earth do we figure out the w, the brain that can do such a thing? How would we ever learn such a model? So this process of learning, of solving for w, if we were doing this with the simple equation in which we think about these as numbers, we know exactly how to do that: 6 = 2 x w, well, we divide by two and we're done. The problem is with this operator. So, division — we've used division because it's the inverse to multiplication, but as I've just said, the multiplication is a bit of a lie here. This is a very, very complicated, very non-linear operation; it has no inverse. So we have to figure out a way to solve the equation without a division operator. And the way to do that is fairly straightforward. You just say, let's play a little algebra trick, and move the six over to the right-hand side of the equation. Now, we're still using multiplication. And that zero — let's think about it as an error. In other words, if we've solved for w the right way, then the error will be zero. And if we haven't gotten it quite right, the error will be greater than zero. So now we can just take guesses to minimize the error, and that's the sort of thing computers are very good at. So you've taken an initial guess: what if w = 0? Well, then the error is 6. What if w = 1? The error is 4. And then the computer can sort of play Marco Polo, and drive down the error close to zero. As it does that, it's getting successive approximations to w. Typically, it never quite gets there, but after about a dozen steps, we're up to w = 2.999, which is close enough. And this is the learning process. So remember that what's been going on here is that we've been taking a lot of known x's and known y's and solving for the w in the middle through an iterative process. It's exactly the same way that we do our own learning. We have many, many images as babies and we get told, ""This is a bird; this is not a bird."" And over time, through iteration, we solve for w, we solve for those neural connections. So now, we've held x and w fixed to solve for y; that's everyday, fast perception. We figure out how we can solve for w, that's learning, which is a lot harder, because we need to do error minimization, using a lot of training examples. And about a year ago, Alex Mordvintsev, on our team, decided to experiment with what happens if we try solving for x, given a known w and a known y. In other words, you know that it's a bird, and you already have your neural network that you've trained on birds, but what is the picture of a bird? It turns out that by using exactly the same error-minimization procedure, one can do that with the network trained to recognize birds, and the result turns out to be ... a picture of birds. So this is a picture of birds generated entirely by a neural network that was trained to recognize birds, just by solving for x rather than solving for y, and doing that iteratively. Here's another fun example. This was a work made by Mike Tyka in our group, which he calls ""Animal Parade."" It reminds me a little bit of William Kentridge's artworks, in which he makes sketches, rubs them out, makes sketches, rubs them out, and creates a movie this way. In this case, what Mike is doing is varying y over the space of different animals, in a network designed to recognize and distinguish different animals from each other. And you get this strange, Escher-like morph from one animal to another. Here he and Alex together have tried reducing the y's to a space of only two dimensions, thereby making a map out of the space of all things recognized by this network. Doing this kind of synthesis or generation of imagery over that entire surface, varying y over the surface, you make a kind of map — a visual map of all the things the network knows how to recognize. The animals are all here; ""armadillo"" is right in that spot. You can do this with other kinds of networks as well. This is a network designed to recognize faces, to distinguish one face from another. And here, we're putting in a y that says, ""me,"" my own face parameters. And when this thing solves for x, it generates this rather crazy, kind of cubist, surreal, psychedelic picture of me from multiple points of view at once. The reason it looks like multiple points of view at once is because that network is designed to get rid of the ambiguity of a face being in one pose or another pose, being looked at with one kind of lighting, another kind of lighting. So when you do this sort of reconstruction, if you don't use some sort of guide image or guide statistics, then you'll get a sort of confusion of different points of view, because it's ambiguous. This is what happens if Alex uses his own face as a guide image during that optimization process to reconstruct my own face. So you can see it's not perfect. There's still quite a lot of work to do on how we optimize that optimization process. But you start to get something more like a coherent face, rendered using my own face as a guide. You don't have to start with a blank canvas or with white noise. When you're solving for x, you can begin with an x, that is itself already some other image. That's what this little demonstration is. This is a network that is designed to categorize all sorts of different objects — man-made structures, animals ... Here we're starting with just a picture of clouds, and as we optimize, basically, this network is figuring out what it sees in the clouds. And the more time you spend looking at this, the more things you also will see in the clouds. You could also use the face network to hallucinate into this, and you get some pretty crazy stuff. (Laughter) Or, Mike has done some other experiments in which he takes that cloud image, hallucinates, zooms, hallucinates, zooms hallucinates, zooms. And in this way, you can get a sort of fugue state of the network, I suppose, or a sort of free association, in which the network is eating its own tail. So every image is now the basis for, ""What do I think I see next? What do I think I see next? What do I think I see next?"" I showed this for the first time in public to a group at a lecture in Seattle called ""Higher Education"" — this was right after marijuana was legalized. (Laughter) So I'd like to finish up quickly by just noting that this technology is not constrained. I've shown you purely visual examples because they're really fun to look at. It's not a purely visual technology. Our artist collaborator, Ross Goodwin, has done experiments involving a camera that takes a picture, and then a computer in his backpack writes a poem using neural networks, based on the contents of the image. And that poetry neural network has been trained on a large corpus of 20th-century poetry. And the poetry is, you know, I think, kind of not bad, actually. (Laughter) In closing, I think that per Michelangelo, I think he was right; perception and creativity are very intimately connected. What we've just seen are neural networks that are entirely trained to discriminate, or to recognize different things in the world, able to be run in reverse, to generate. One of the things that suggests to me is not only that Michelangelo really did see the sculpture in the blocks of stone, but that any creature, any being, any alien that is able to do perceptual acts of that sort is also able to create because it's exactly the same machinery that's used in both cases. Also, I think that perception and creativity are by no means uniquely human. We start to have computer models that can do exactly these sorts of things. And that ought to be unsurprising; the brain is computational. And finally, computing began as an exercise in designing intelligent machinery. It was very much modeled after the idea of how could we make machines intelligent. And we finally are starting to fulfill now some of the promises of those early pioneers, of Turing and von Neumann and McCulloch and Pitts. And I think that computing is not just about accounting or playing Candy Crush or something. From the beginning, we modeled them after our minds. And they give us both the ability to understand our own minds better and to extend them. Thank you very much. (Applause)"	我在 Google 帶領一個團隊做機械智慧；換句話說，就是制定一些訓練方法，讓電腦和裝置能做些大腦做的事。而這也讓我們對真實的大腦以及神經科學產生了興趣，特別是一些我們大腦能做但電腦仍無法呈現出來的事。 長期以來，機械智慧的其中一個領域談的就是機械感知，它是一種轉化的過程——像是把聲音和影像——轉化成心智上的概念。這是我們大腦必備的能力，這個能力對電腦來說也很有用。所謂的機械感知演算法，像是我們團隊做的，能讓你 Google 相簿裡的照片根據照片裡的東西把它們變成可以被搜尋的資料。感知的另一面是創意：把概念轉化成另一種東西。所以過去幾年，我們團隊在機器感知上的努力，已經可以把創意與機器藝術結合在一起。 我覺得米開朗基羅對「感知」與「創意」這兩者之間的關係有一種很透析的看法。他有一句名言：「每一塊石頭裡都藏著一座雕像，等待雕刻家將它雕塑出來。」所以我覺得米開朗基羅當時的體悟是：我們的「創意」來自「感知」，而感知本身就是一個想像行為及創意的來源。 人體中有一個器官能做出思考、感受和想像，當然，那就是我們的大腦。我想先簡單地來談一談我們對大腦認知的歷史。因為大腦不像我們的心臟或腸道，你不能光用看的來瞭解大腦，光靠肉眼根本看不出個所以然來。早期研究大腦的解剖學家，在大腦表皮結構上取了許多稀奇古怪的名字，例如海馬體，意思是「小蝦子」。當然，這樣的命名方式並沒有讓我們對大腦的認識有太多的幫助。 我認為，第一個有真正深入了解大腦如何運作的，是偉大的西班牙神經解剖學家桑地牙哥·拉蒙卡哈，他在十九世紀，就已經開始用顯微鏡和特殊染劑把大腦裡的特定細胞篩選出來染色，或以強烈的對比色來觀察細胞，這樣做，是為了瞭解它們的形態結構。這些是他在十九世紀時畫的神經細胞圖， 這一張是鳥的大腦。但當時已經可以看到各式各樣不同的細胞圖片，即使細胞的原理在當時是個相當新穎的概念。這些結構，這些樹枝狀的細胞結構，可以延伸到相當相當長──在當時來講，這樣的發現算是相當神奇了。當然，它們也會讓人聯想到電線，這對 19 世紀的人來說，這樣的比喻可能比較恰當，因為當時電線和電力的變革正如火如荼的進行。但就很多方面來說，像拉蒙卡哈這樣的顯微鏡解剖圖現在看來還是很厲害。 但我們卻在一個世紀後，才想試著去完成當年拉蒙卡哈的研究。這些原始資料，來自我們馬克斯·普朗克神經科學機構的合作夥伴。而我們的合作夥伴的工作就是把大腦組織切成一小片一小片的圖像。整個樣本的大小大約只有 1 立方毫米，我展示給各位看的只有小小的一片。你可以看到，左邊的長度標誌僅有一微米。各位現在看到的結構是粒線體，大小跟細菌一樣。這些連續切片圖，是由一塊很小的組織中一片片切出來的。舉個例子做比較，一根頭髮的直徑大約有 100 微米。我們在研究的是比一根頭髮還更細更小的東西。 而這一系列的電子顯微鏡切片圖像，可以組成像這樣的神經元 3D 立體成像。這些和拉蒙卡哈當年的研究相去不遠。但只有幾個神經元可以打光，否則我們會看不到東西。因為空間太壅擠、結構太複雜了，神經元蜿蜒地一個接著一個。 所以，拉蒙卡哈在當時也算是走在時代的尖端，但在那之後的幾十年，人類對大腦的認識卻相當緩慢。但我們已經知道神經元是利用電子傳遞訊號，到第二次世界大戰前，我們的科技已經進步到可以在活體神經元上做電子實驗，用來更好地理解它們是如何運作的。這也是電腦被發明出來的時間，當初有一個模擬人腦的基礎想法——是由艾倫·圖靈所提出，他稱之為「智能機械」，他是計算機科學之父之一。 當時沃倫麥卡洛克和華特彼特斯（人工神經科學家）看到的視覺皮質圖，就是上面這張拉蒙卡哈的圖片。這個皮質層是負責把眼睛傳來的訊號轉換成圖像。他們當時發現，它看起來像是一張電路圖。雖然麥卡洛克和彼特斯在電路圖上有很多細節不太正確，但這樣的基礎概念，視覺皮層的工作原理像一系列的計算子在串聯的電路圖上傳遞著資訊，這樣的概念卻是相當正確的。 我們稍微聊一下，產生視覺資訊的模型，需要做哪些事情。覺察力的基本任務就是比如說，看到這一張圖片，就要會判斷出，「這是一隻鳥」，這對我們大腦來說是很簡單的任務。但各位要知道，這對電腦來說在幾年前根本是不可能的事。傳統的計算模式根本不太容易跑出來這樣的任務。 所以，像素、鳥圖與文字之間，一定要有一組彼此連結的神經元在神經網路內相互作用著，就像我這張示意圖。這張神經網路圖就像我們的視覺皮質運作原理。如今，我們已經有能力用電腦來模擬這樣的神經網路。接下來我向各位展示一下，實際的操作大概是怎樣。 圖片的像素你可以把它想像成是第一層的神經元，實際上，就是眼睛裡面像素的呈現方式，像素是透過視網膜上的神經元做傳遞。而這些前饋資訊會一層一層地傳遞到下一層神經元，全部由不同的「突觸權重」所連結。神經網路的行為全都由這些突觸的強度所控制。它們決定了神經網路的計算模式。最後，會有一個或一小群的神經元發出訊號，辨識出該圖片就是，「鳥」。 我現在要來解釋一下這三個元素——輸入的「像素」、神經網路裡的「突觸」、還有「鳥」這個輸出的字元——它們是如何運作的。它們是由三種變數所組成，x、w 和 y。圖片中可能有一百多萬個 x ——100 多萬個像素。而 w 可能有數十億或好幾兆個，它們代表著神經網路中各個突觸的權重。而這個網路能輸出的 y只有少數幾個。「bird」只有四個字母，對吧?我們假設它的原理是一個簡單的公式，x 「乘以」 w = y我把乘法符號用引號標示起來因為它其實是一個非常複雜的數學運算概念。 這個方程式有三個變數，我們都知道，如果你想要解開這個方程式，可以從兩個已知數交叉算出未知的數。所以要推斷出圖片中的影像是一隻鳥，可以用這種方式得知：y 是未知數，而 w 和 x 是已知數。已知神經網路和圖片像素，其實可以很直接的就得到答案，2x3=6，就做完了。我向各位展示一個我們最近做的人工神經網路， 它可以在手機上做及時的操作，當然，手機的運算能力相當驚人，手機每秒可以做出數十億至上兆次的運算。你現在看到的是一隻手機正對著一張張的鳥圖拍照，手機不但可以正確的說出，「是的，這是一隻鳥。」還能透過神經網路分類分辨出這是哪一種鳥。所以，在這些圖片上，x 和 w 是已知，而 y 是未知。我現在來解釋一下這個最困難的 「w」，我們到底是如何算出來的？為什麼大腦可以做出這樣的判斷？我們到底是如何學到這樣的認知模式的？ 這個學習的過程，是一個求解 w 的過程，如果我們要解這個一次方程式，當它們都是數字時，我們都知道如何解 6=2 x w，我們只要把 6 除以 2 就可以得到答案。問題在於這個運算符號，除法這個符號——我們會用除法的方式求解，是因為它跟乘法相反，但就如同我剛剛提到的，乘法在這裡有點像是個幌子。這是非常非常複雜的概念，它們是「非線性運算」的概念；無法直接用除的求解。所以，我們要另外找個方法來解方程式，而不能直接用除的。方法相當簡單，可以說，我們只用了點代數的小技巧，將 6 移動到等號的右邊。如此我們就可以繼續用乘法來運算。而等號左邊的零——我們把它想像成是誤差。換言之，如果要解出 w，誤差就要變成 0。如果我們沒找到答案誤差會永遠大於 0。 所以，我們現在只能用猜的來縮小誤差，而這就是電腦非常擅長的地方。所以，你會從頭開始猜：假設 w=0那誤差會等於6但假如 w=1 呢？誤差等於 4。接下來電腦有點像是在玩馬可波羅探索遊戲，探索到誤差接近零為止。當它一直探索到零，那麼 w 就解出來了。原則上，它會不停探索直到接近零，但大約經過多次步驟後，我們就能得出 w=2.999，相當接近了。這就是電腦學習的過程。 回想一下剛剛發生了什麼事情，我們有很多已知的 x 和 y，透過重複迭代的過程解出了 w。而這就是我們人類學習的過程，我們從小看了很多圖片被告知「這是鳥」，「這不是鳥」；經過了一段時間，不停地重複，我們解出了 w，產生了神經元的連結關係。 所以現在，我們的 x 和 w 是固定數，可以解出 y；這就是我們人類每天經常性的快速直覺判斷。我們搞懂了如何解出 w，而學習本身是一條相當艱辛的路程，因為為了讓誤差最小化，我們必須使用很多的訓練樣本。 約一年前，我們團隊的艾力克斯摩文斯夫決定做個實驗，看看如果我們試著給出了 w 和 y，解出來的 x 會變什麼樣。換句話說，電腦知道它是一隻鳥，電腦有你給它訓練出來辨識鳥圖片的神經網路，但對電腦而言，鳥是怎樣的圖像？原來，使用一模一樣的「誤差最小化」程序以及訓練出來用來辨識鳥的神經網路，你就能辨識出……這是一張鳥圖，所以，這是一張完全由訓練辨認鳥的神經網路自行創造出來的鳥圖，只要透過不斷地重複解出 x，而不是解 y 就可以了。 這裡有另一個有趣的範例。我們團隊裡的另外一位組員麥克泰卡，他稱這些畫為《動物大遊行》。這讓我有點回想起了威廉肯特基的作品，他畫好素描後，擦掉它，然後反覆地畫、反覆地擦透過這樣的方式，創造出了一部影片。在這個展示裡，麥可做的就是把不同動物的 y ，透過設計好的神經網路，彼此辨認並分別出不一樣的動物。如此，你就能得到一張像艾雪一樣的不同動物的變體圖像。 這一張是他和艾力克斯一起完成的，他們試著減少 y 的數量，將這些圖案丟到一個 2D 平面上，透過這個網路的辨識，創造出了這一張有各種動物的地圖。要做出這樣的綜合體，或透過整張圖面產出圖像，你只要在圖面上給出各式各樣的 y ，你就能做出一張地圖來——一張由神經網路辨識出的視覺地圖。所有動物都會在這上面，犰狳就在圖上這個點。 你也可以透過不同的神經網路，做出類似這樣的作品，這一張由辨識臉的神經網路所做出來的作品，這一張是用「我」當作 y ，所做出來的圖畫，用我的臉當參數。當電腦解出 x 後，它就畫出了這一張相當瘋狂、有點像立體派藝術、超現實、迷幻效果的我，同一張圖卻有不同的視角。而會有這種「同一張圖不同視角」的感覺，是因為這個神經網路的設計，可以將不同姿勢臉之間的模糊地帶移除掉，透過觀察不同的光源就可以做到。所以，當你重新製作圖像時，如果你沒有使用指導圖，或特定的統計資料，那你就能得到來自不同角度的混合體圖像，因為它是模糊的。所以如果艾力克斯用他自己的臉當作指導圖在優化過程中重新建造我的臉，就會產生這樣的圖像。各位可以看到，這作品還不是很完美，在圖像優化的過程方面，還有很多工作要做。但如果用我的臉當指導圖，就能漸漸地顯現出比較條理分明的臉。 你不需要從一張空白的畫布或用白雜訊畫起。當你解出 x 後，你就可以從 x 開始畫起，因為它本身就有一些圖像。這個小小的展示說明了它的運作原理。這個網路是設計用來分辨各種不同的物體，像是人造結構、動物……等。這一張畫我們是從雲朵的圖像開始畫起的，當我們把它優化後，基本上，這個神經網路正在搞懂它在雲朵中看見了什麼。當你看得越久，你就能在雲層中看得越多。你也可以運用人臉網路讓它產生幻覺，然後就會跑出相當瘋狂的畫作。 （笑聲） 或者，麥可已經有作出一些其它的實驗，他用那張雲朵的圖像，使電腦產生幻覺、然後放大、產生幻覺、再放大。用這樣的方式，我在想，你就能得到一種像是在神遊狀態的網路，或者像是一種無拘束的聯想，彷彿神經網路正在吃著自己的尾巴。所以每一張圖像基本上像是正在想：「我接下來會看到什麼？接下來會看到什麼？接下來會看到什麼？」 我第一次在一個公眾場合上展示這個影片，是在西雅圖的「高等教育」機構做演說時展示的，當時剛好是大麻剛合法化的時候。 （笑聲） 所以，我快速總結一下，這項技術並不會受到約束。我剛剛展示的是純粹的視覺範例，因為觀察它的變化，真的很好玩。它不單只有視覺科技。我們的藝術合作者，羅斯谷穎已經做了一些實驗，他用相機拍了一張照片，然後他背包裡的電腦會根據圖片上的內容，透過神經網路，創作出一首詩。這個會作詩的神經網路是透過大量 20 世紀的詩集所訓練出來的，而做出來的詩，實際上，我覺得還得不錯。 （笑聲） 整體而言，我在想，米開朗基羅，他是對的；感知和創意的關係是相當緊密的。我們剛剛看的神經網路，它們是被訓練出來分辯或辨認世界上不同的東西，也可以反過來，自行創作出東西來。而我從中所得到的不僅有米開朗基羅的啟發：「看見石頭裡的雕像」，還有任何能做出感知活動的生物、生命、外來物種都能透過這樣的方式被呈現並創造出來，因為這兩者與剛才舉的例子都有著相同的機制。 我也認為，感知及創意不是只有我們人類獨有。我們已經有電腦模式可以做出相當類似的事。所以不需要感到驚訝；因為大腦是會運算的。 最後，我要說的是，設計智能機器已經開始成為電腦界的活動。在如何讓機器更智能的領域方面，已經有很多的模式產生。我們終於開始完成一些早期前輩們像是圖靈、馮諾伊曼、馬庫洛奇和皮斯的期望。而我也認為電腦不是只有拿來計算或玩玩 Candy Crush 而已，回到初衷，我們想要的是讓電腦能仿效人腦。它不僅讓我們更了解了人類的心智，並讓我們獲得延伸發展心智的能力。 非常感謝大家。 （掌聲）
