Talk	en	zh-tw
anders_ynnerman_visualizing_the_medical_data_explosion	"I will start by posing a little bit of a challenge: the challenge of dealing with data, data that we have to deal with in medical situations. It's really a huge challenge for us. And this is our beast of burden — this is a Computer Tomography machine, a CT machine. It's a fantastic device. It uses X-rays, X-ray beams, that are rotating very fast around the human body. It takes about 30 seconds to go through the whole machine and is generating enormous amounts of information that comes out of the machine. So this is a fantastic machine that we can use for improving health care, but as I said, it's also a challenge for us. And the challenge is really found in this picture here. It's the medical data explosion that we're having right now. We're facing this problem. And let me step back in time. Let's go back a few years in time and see what happened back then. These machines that came out — they started coming in the 1970s — they would scan human bodies, and they would generate about 100 images of the human body. And I've taken the liberty, just for clarity, to translate that to data slices. That would correspond to about 50 megabytes of data, which is small when you think about the data we can handle today just on normal mobile devices. If you translate that to phone books, it's about one meter of phone books in the pile. Looking at what we're doing today with these machines that we have, we can, just in a few seconds, get 24,000 images out of a body, and that would correspond to about 20 gigabytes of data, or 800 phone books, and the pile would then be 200 meters of phone books. What's about to happen — and we're seeing this; it's beginning — a technology trend that's happening right now is that we're starting to look at time-resolved situations as well. So we're getting the dynamics out of the body as well. And just assume that we will be collecting data during five seconds, and that would correspond to one terabyte of data — that's 800,000 books and 16 kilometers of phone books. That's one patient, one data set. And this is what we have to deal with. So this is really the enormous challenge that we have. And already today — this is 25,000 images. Imagine the days when we had radiologists doing this. They would put up 25,000 images, they would go like this, ""25,0000, okay, okay. There is the problem."" They can't do that anymore. That's impossible. So we have to do something that's a little bit more intelligent than doing this. So what we do is that we put all these slices together. Imagine that you slice your body in all these directions, and then you try to put the slices back together again into a pile of data, into a block of data. So this is really what we're doing. So this gigabyte or terabyte of data, we're putting it into this block. But of course, the block of data just contains the amount of X-ray that's been absorbed in each point in the human body. So what we need to do is to figure out a way of looking at the things we do want to look at and make things transparent that we don't want to look at. So transforming the data set into something that looks like this. And this is a challenge. This is a huge challenge for us to do that. Using computers, even though they're getting faster and better all the time, it's a challenge to deal with gigabytes of data, terabytes of data and extracting the relevant information. I want to look at the heart. I want to look at the blood vessels. I want to look at the liver. Maybe even find a tumor, in some cases. So this is where this little dear comes into play. This is my daughter. This is as of 9 a.m. this morning. She's playing a computer game. She's only two years old, and she's having a blast. So she's really the driving force behind the development of graphics-processing units. As long as kids are playing computer games, graphics is getting better and better and better. So please go back home, tell your kids to play more games, because that's what I need. So what's inside of this machine is what enables me to do the things that I'm doing with the medical data. So really what I'm doing is using these fantastic little devices. And you know, going back maybe 10 years in time when I got the funding to buy my first graphics computer — it was a huge machine. It was cabinets of processors and storage and everything. I paid about one million dollars for that machine. That machine is, today, about as fast as my iPhone. So every month there are new graphics cards coming out, and here is a few of the latest ones from the vendors — NVIDIA, ATI, Intel is out there as well. And you know, for a few hundred bucks you can get these things and put them into your computer, and you can do fantastic things with these graphics cards. So this is really what's enabling us to deal with the explosion of data in medicine, together with some really nifty work in terms of algorithms — compressing data, extracting the relevant information that people are doing research on. So I'm going to show you a few examples of what we can do. This is a data set that was captured using a CT scanner. You can see that this is a full data [set]. It's a woman. You can see the hair. You can see the individual structures of the woman. You can see that there is [a] scattering of X-rays on the teeth, the metal in the teeth. That's where those artifacts are coming from. But fully interactively on standard graphics cards on a normal computer, I can just put in a clip plane. And of course all the data is inside, so I can start rotating, I can look at it from different angles, and I can see that this woman had a problem. She had a bleeding up in the brain, and that's been fixed with a little stent, a metal clamp that's tightening up the vessel. And just by changing the functions, then I can decide what's going to be transparent and what's going to be visible. I can look at the skull structure, and I can see that, okay, this is where they opened up the skull on this woman, and that's where they went in. So these are fantastic images. They're really high resolution, and they're really showing us what we can do with standard graphics cards today. Now we have really made use of this, and we have tried to squeeze a lot of data into the system. And one of the applications that we've been working on — and this has gotten a little bit of traction worldwide — is the application of virtual autopsies. So again, looking at very, very large data sets, and you saw those full-body scans that we can do. We're just pushing the body through the whole CT scanner, and just in a few seconds we can get a full-body data set. So this is from a virtual autopsy. And you can see how I'm gradually peeling off. First you saw the body bag that the body came in, then I'm peeling off the skin — you can see the muscles — and eventually you can see the bone structure of this woman. Now at this point, I would also like to emphasize that, with the greatest respect for the people that I'm now going to show — I'm going to show you a few cases of virtual autopsies — so it's with great respect for the people that have died under violent circumstances that I'm showing these pictures to you. In the forensic case — and this is something that ... there's been approximately 400 cases so far just in the part of Sweden that I come from that has been undergoing virtual autopsies in the past four years. So this will be the typical workflow situation. The police will decide — in the evening, when there's a case coming in — they will decide, okay, is this a case where we need to do an autopsy? So in the morning, in between six and seven in the morning, the body is then transported inside of the body bag to our center and is being scanned through one of the CT scanners. And then the radiologist, together with the pathologist and sometimes the forensic scientist, looks at the data that's coming out, and they have a joint session. And then they decide what to do in the real physical autopsy after that. Now looking at a few cases, here's one of the first cases that we had. You can really see the details of the data set. It's very high-resolution, and it's our algorithms that allow us to zoom in on all the details. And again, it's fully interactive, so you can rotate and you can look at things in real time on these systems here. Without saying too much about this case, this is a traffic accident, a drunk driver hit a woman. And it's very, very easy to see the damages on the bone structure. And the cause of death is the broken neck. And this women also ended up under the car, so she's quite badly beaten up by this injury. Here's another case, a knifing. And this is also again showing us what we can do. It's very easy to look at metal artifacts that we can show inside of the body. You can also see some of the artifacts from the teeth — that's actually the filling of the teeth — but because I've set the functions to show me metal and make everything else transparent. Here's another violent case. This really didn't kill the person. The person was killed by stabs in the heart, but they just deposited the knife by putting it through one of the eyeballs. Here's another case. It's very interesting for us to be able to look at things like knife stabbings. Here you can see that knife went through the heart. It's very easy to see how air has been leaking from one part to another part, which is difficult to do in a normal, standard, physical autopsy. So it really, really helps the criminal investigation to establish the cause of death, and in some cases also directing the investigation in the right direction to find out who the killer really was. Here's another case that I think is interesting. Here you can see a bullet that has lodged just next to the spine on this person. And what we've done is that we've turned the bullet into a light source, so that bullet is actually shining, and it makes it really easy to find these fragments. During a physical autopsy, if you actually have to dig through the body to find these fragments, that's actually quite hard to do. One of the things that I'm really, really happy to be able to show you here today is our virtual autopsy table. It's a touch device that we have developed based on these algorithms, using standard graphics GPUs. It actually looks like this, just to give you a feeling for what it looks like. It really just works like a huge iPhone. So we've implemented all the gestures you can do on the table, and you can think of it as an enormous touch interface. So if you were thinking of buying an iPad, forget about it. This is what you want instead. Steve, I hope you're listening to this, all right. So it's a very nice little device. So if you have the opportunity, please try it out. It's really a hands-on experience. So it gained some traction, and we're trying to roll this out and trying to use it for educational purposes, but also, perhaps in the future, in a more clinical situation. There's a YouTube video that you can download and look at this, if you want to convey the information to other people about virtual autopsies. Okay, now that we're talking about touch, let me move on to really ""touching"" data. And this is a bit of science fiction now, so we're moving into really the future. This is not really what the medical doctors are using right now, but I hope they will in the future. So what you're seeing on the left is a touch device. It's a little mechanical pen that has very, very fast step motors inside of the pen. And so I can generate a force feedback. So when I virtually touch data, it will generate forces in the pen, so I get a feedback. So in this particular situation, it's a scan of a living person. I have this pen, and I look at the data, and I move the pen towards the head, and all of a sudden I feel resistance. So I can feel the skin. If I push a little bit harder, I'll go through the skin, and I can feel the bone structure inside. If I push even harder, I'll go through the bone structure, especially close to the ear where the bone is very soft. And then I can feel the brain inside, and this will be the slushy like this. So this is really nice. And to take that even further, this is a heart. And this is also due to these fantastic new scanners, that just in 0.3 seconds, I can scan the whole heart, and I can do that with time resolution. So just looking at this heart, I can play back a video here. And this is Karljohan, one of my graduate students who's been working on this project. And he's sitting there in front of the Haptic device, the force feedback system, and he's moving his pen towards the heart, and the heart is now beating in front of him, so he can see how the heart is beating. He's taken the pen, and he's moving it towards the heart, and he's putting it on the heart, and then he feels the heartbeats from the real living patient. Then he can examine how the heart is moving. He can go inside, push inside of the heart, and really feel how the valves are moving. And this, I think, is really the future for heart surgeons. I mean it's probably the wet dream for a heart surgeon to be able to go inside of the patient's heart before you actually do surgery, and do that with high-quality resolution data. So this is really neat. Now we're going even further into science fiction. And we heard a little bit about functional MRI. Now this is really an interesting project. MRI is using magnetic fields and radio frequencies to scan the brain, or any part of the body. So what we're really getting out of this is information of the structure of the brain, but we can also measure the difference in magnetic properties of blood that's oxygenated and blood that's depleted of oxygen. That means that it's possible to map out the activity of the brain. So this is something that we've been working on. And you just saw Motts the research engineer, there, going into the MRI system, and he was wearing goggles. So he could actually see things in the goggles. So I could present things to him while he's in the scanner. And this is a little bit freaky, because what Motts is seeing is actually this. He's seeing his own brain. So Motts is doing something here, and probably he is going like this with his right hand, because the left side is activated on the motor cortex. And then he can see that at the same time. These visualizations are brand new. And this is something that we've been researching for a little while. This is another sequence of Motts' brain. And here we asked Motts to calculate backwards from 100. So he's going ""100, 97, 94."" And then he's going backwards. And you can see how the little math processor is working up here in his brain and is lighting up the whole brain. Well this is fantastic. We can do this in real time. We can investigate things. We can tell him to do things. You can also see that his visual cortex is activated in the back of the head, because that's where he's seeing, he's seeing his own brain. And he's also hearing our instructions when we tell him to do things. The signal is really deep inside of the brain as well, and it's shining through, because all of the data is inside this volume. And in just a second here you will see — okay, here. Motts, now move your left foot. So he's going like this. For 20 seconds he's going like that, and all of a sudden it lights up up here. So we've got motor cortex activation up there. So this is really, really nice, and I think this is a great tool. And connecting also with the previous talk here, this is something that we could use as a tool to really understand how the neurons are working, how the brain is working, and we can do this with very, very high visual quality and very fast resolution. Now we're also having a bit of fun at the center. So this is a CAT scan — Computer Aided Tomography. So this is a lion from the local zoo outside of Norrkoping in Kolmarden, Elsa. So she came to the center, and they sedated her and then put her straight into the scanner. And then, of course, I get the whole data set from the lion. And I can do very nice images like this. I can peel off the layer of the lion. I can look inside of it. And we've been experimenting with this. And I think this is a great application for the future of this technology, because there's very little known about the animal anatomy. What's known out there for veterinarians is kind of basic information. We can scan all sorts of things, all sorts of animals. The only problem is to fit it into the machine. So here's a bear. It was kind of hard to get it in. And the bear is a cuddly, friendly animal. And here it is. Here is the nose of the bear. And you might want to cuddle this one, until you change the functions and look at this. So be aware of the bear. So with that, I'd like to thank all the people who have helped me to generate these images. It's a huge effort that goes into doing this, gathering the data and developing the algorithms, writing all the software. So, some very talented people. My motto is always, I only hire people that are smarter than I am and most of these are smarter than I am. So thank you very much. (Applause)"	首先我先向大家介紹一個亟需解決的難題如何有效處理醫療過程中生成的海量數據.這些數據處理起來十分棘手.這個正是解決此難題的關鍵.一台x光斷層掃描儀即CT機.這台機器非常先進.它使用X線管發出X線束對患者進行掃描,同時這些X線管會圍繞患者高速旋轉.CT機完成一次掃描需要大約30秒同時採集並輸出掃描所生成的大量信息.這台機器真的非常厲害它可以用來提高衛生保健的質量.但正如之前所說,它也為我們帶來了一個問題.從這張圖片大家可以看到這個問題.它正是我們現在正在面臨的醫療數據爆炸.我們正在努力解決這個問題.讓我們先來回顧一下過去. 現在我們回到幾十年前我要說的這些機器於上世紀70年代開始投入使用醫生們用這些機器對患者進行人體掃描.之後會生成大約100張人體影像.恕我冒昧,為了方便理解,我把這圖像轉換成等量的數據切片.這些圖像大約相當於50MB的數據,這個數量很小如果和現在我們每天打交道的信息量相比只與普通的移動設備相當.如果拿電話簿的信息量做比的話,約相當於一米高的電話簿疊加.現在我們來看看今天對這些機器的使用.現在,只需數秒一位患者的人體掃描可以得到24,000張影像.這相當於 20 GB的數據,800本電話簿.壘起來大約有200米.然後呢,會發生甚麼?我們可以看到,它已經開始了——一種新的技術趨勢已經出現我們開始考量時間分辨力.我們也需要得到書面化的診斷結果.現在我們假設我們收集到了掃描5秒鐘所得數據,大約為1 TB.相當於800,000本電話簿,壘起來約16千米.這還只是掃描一個病患所得數據集.這也正是我們需要處理的數據量 所以說這個難題真的十分棘手.現在正是這個問題. 這裡有25,000張影像.想像一下在以前醫生們是這樣研究病理的.放到現在,他們要放25,000張圖像上去,到時候他們就要像這樣去看,”第25,000張,哎,對,問題找到了.”現實不允許他們再這樣去找了.所以我們要想一些更聰明的辦法.我們得把這些切片再整合起來.我們先把自己沿這些各種方向切成數據片,然後把這些切片再重新放回到一起,這樣一堆數據就成了一個數據塊這就是我們要做的.把這許多GB、TB的數據合成一個數據塊當然,這個數據塊只包含了在各個部分被人體吸收了的X線束的數據.接下來我們要做的就是想個辦法只顯示我們想看到那部分的數據,隱去我們不想看到的部份.於是我們要把這個數據集變成這個樣子.這個不容易做到.這個非常不容易做到. 雖然現在電腦運行已越來越快且穩定利用電腦處理上GB 的數據,或者說上TB的數據並提取出所需信息依然並不容易.有時候要看一下心臟,有時候要看一下血管,有時看肝臟,也許有時候要找一下看沒有腫瘤.現在該我的小女兒出現了.這就是我女兒.這大概是今天上午9點.她在玩電腦遊戲.她才只有兩歲,但是玩得很開心.這樣的她正是催動圖像處理器進步的原動力.只要小孩子還在玩電腦遊戲,電腦圖像處理技術就會越來越好.所以請大家回去告誡你們的小孩多玩遊戲吧,我真的很需要這個. 我要說的是,我處理醫療數據要用的東西就包含在這機器裡面.我要用到的就是這些能幹的小設備.多年前大約十年前我得到足夠的資金買了我的第一台繪圖電腦那台電腦體型非常大像塞滿處理器,存儲器等等等等的格子這台機器花了我大約一百萬美金.現在這機器運行速度大概和我的iPhone一樣.每個月都會有不同的新顯示卡面世.這是銷售商們推出的最新的顯示卡——NVIDIA, ATI, 還有Intel.只要花個幾百塊就能買到這些裝到電腦裡面去,然後就可以做很多想做的事情.要解決醫療數據爆炸的問題靠的正是這個.再加上一些其他邏輯運算之類的技術活——比如數據壓縮,以及提取醫生需要研究部分的信息. 接下來我為大家演示一下我們能做到的部分.這是使用CT機掃描時建成的一個數據集.大家可以看到這是一套完整的數據.這是一位女性. 從頭髮可以分辨出來.大家可以看到這位女性身體各處的生理構造.她牙齒上一塊散布的X線束,那是牙齒上的一塊金屬.也就是人造物所在的地方.然後只需在裝有普通顯示卡的普通電腦上進行適當的編程,解析出一個剖面.當然所有的數據都沒在表面,通過旋轉可以從不同的角度進行觀察,我可以看到這位女性有一個問題,她的大腦顱腔有一處出血,醫生用一個支架和一個金屬夾子夾緊血管來控制出血.通過改變功能設置,我可以決定讓哪部分隱去哪部分顯示出來.我可以只看他的頭骨部分,然後可以觀察出,哦,醫生是從這裡打開她的頭蓋骨,然後是從這個地方著手進行手術.這些都是非常有用的圖像.他們能提供非常有用的信息,能告訴我們今天用普通顯示卡我們能做些甚麼. 現在我們確實已經開始利用起這些顯卡,我們希望能利用這些顯卡處理盡量多的數據.我們正在開發的一個應用——這個應用已經開始全球推廣——虛擬屍檢.這一次,從這些信息量巨大的數據集中大家可以再一次看到全身掃描的使用.把屍體完全推進CT掃描儀,只要數秒就可以得到一個全身數據集.這是一次虛擬屍檢的資料.大家可以看到一層一層的解構如何完成.首先是覆著屍體的停屍袋,然後是屍體然後剖開皮膚,出現肌肉最後可以看到這位女性的骨骼結構. 在這裡,我要強調一下,對接下來我要展示的屍檢範例我是懷著極高的敬意.這些都是虛擬屍檢的應用案例我是懷著對死者最高的敬意向大家展示這些暴力死亡的屍檢案例.法醫屍檢裡近四年來單就瑞典我所在的地區來說目前已經有大約400例法醫屍檢採用了虛擬驗屍.虛擬驗屍的流程大概是這樣的.首先,大概在晚上警方到達案發現場根據現場情況決定是否需要虛擬驗屍.之後大概在早上6點到7點,警察們把屍體裝進停屍袋送到我們研究中心使用CT機進行掃描.接著放射性專家以及病理學專家有時候再加上法醫學家共同研究從CT機得到的數據.由他們確定接下來實際屍檢的步驟. 現在我們來看一些真實案例.這是早期案例中的一個.我們可以看到非常詳盡的數據,它們能提供很大的幫助.然後我們利用電腦的邏輯演算可以對所有想看到的細節進一步放大.這一次,同樣非常智能,這個系統中我們可以像實際屍檢一樣根據需要對屍體進行旋轉.對這個案例無需做過多的描述.這是一起交通意外,司機醉酒駕駛,一名女性被撞.大家可以很清楚看到骨架上所受創傷.死因是頸骨骨折.被害者當場死亡.撞擊發生時,死者受到重創. 這裡是另一個案子.一起持刀行凶案.這一次我們來看看可以發現甚麼.屍體體內的金屬人造物部分非常明顯.你還可以看到牙齒裡也有一些人造物,那是補牙的填充物.這裡我設定了只顯示金屬,其他被自動屏蔽.這是另一起暴力案件.這裡並不是致命傷.真正死因是心臟被刺.後來兇手又把刀插進了被害人的眼睛.再來看另外一個案子.能直觀看到諸如東西被刀刺破的樣子是很有意思的一件事情.這裡你可以看到心臟被刀刺穿.可以很清楚看到空氣從一個部位漏往另一個部位.這在常規屍檢中是很難觀察到的.所以說,犯罪研究中,虛擬屍檢可以幫助判斷死者真實死因,以及必要時候幫助建立正確的緝凶方向. 接下來也是一個很有意思的案子.這裡可以看到有一顆子彈.子彈是擦著脊柱飛入的.接著我們把這顆子彈變成一個發光體,子彈變成發光體後要找子彈碎片就容易多了.如果在實際屍檢中,要從屍體中搜尋出這些彈片可謂相當困難. 今天還有一樣我非常想展示給大家的東西,就是我們的虛擬驗屍檯.這其實是一套觸屏設備配置有普通顯示卡,加上電腦邏輯演算開發而得.大家可以看得更清楚一點就是這個樣子.用起來就像一個放大版的iPhone.在模擬驗屍檯上你可以做任何實際驗屍中可能的操作,你可以就把它當作一個大型觸屏.所以如果你有想買個iPad,別管iPad了, 這個才是你想要的.史提夫(蘋果公司現任董事長)，聽到了吧這真的是一個很有意思的玩意有機會你們一定要試一下這個是非親身體驗不能明白的.它已經獲得了一定認可,我們正在準備它的首次亮相,希望能把它應用到相關教學中同時,也希望在將來,能將它應用到臨床醫學中去.如果大家想把虛擬驗屍檯介紹給其他人知道的話,這次演講的影片可以在YouTube下載到. 好了,說到觸得到接下來我們來看一些真正觸得到的數據.這個聽起來還有一點科幻,因為我們現在要先進入未來的景象.現在的醫生並沒有真的在使用這種儀器,但是我希望以後能夠.屏幕左側是一個觸控裝置.一隻觸控筆.筆裡面置有高速步進電動機,能通過力反饋信號模擬出”真實”的觸感.用這支筆觸碰這些虛擬數據,會在筆中生成觸力信號從而得到力反饋效果.這次示範中使用的是一套活人掃描數據.我拿著筆, 掃描數據在我面前.把筆伸向掃瞄出的頭部影像我立刻就能感覺到所遇到的阻礙.我感覺到了皮膚的阻礙.繼續用力, 穿透皮膚就能感覺到裡面的骨骼構架.如果再加點力,就能穿過骨骼,尤其是在耳朵附近軟骨部分做這個實驗的話.穿過骨骼,能感覺到大腦內部存在,到處黏糊糊的. 這玩意真的不錯.接下來進一步我們來看心臟.這又得歸功於那些新一代掃描儀,短短0.3秒就掃描完了整個心臟.時間分辨率極高.大家請先看這個心臟,接下來我為大家放一段視頻.這是卡爾約安,我的一個研究生他也是這個研究項目中的一員.他正坐在這套力反饋系統觸覺設備前面,用觸控筆研究那顆心臟.這心臟就在他眼前勃勃跳動.拿著筆他就能檢查這顆心臟跳動是否正常.現在他正拿著觸控筆,把它移近心臟,然後放在心臟表面,感受來自那位患者的真實心跳.這樣他就可以對患者的心臟機能進行檢查.他還可以把筆伸進心臟裡面切切實實的感受心臟瓣膜是如何一張一翕.我想這個正是心臟外科醫生所需要的.有了這項技術,恐怕這些醫生們作夢也會笑醒.這樣醫生們就能夠在實際外科手術前深入觀察患者心臟,並且有高度精確的數據做保證.非常值得期待. 現在我們來講一點更科幻的東西.大家大概都聽說過功能磁共振成像.這是一個非常有意思的研究項目.磁共振成像的原理是利用磁場和射頻脈衝對大腦或身體其他部位進行掃描.通常我們可以通過磁共振成像得到大腦結構的信息當然利用磁共振也可以測出含氧血和不含氧血的不同磁性.這就意味著我們可以繪製出大腦活躍區域圖.這正是我們現在在研究的東西.這裡大家可以看到我們的研究工程師默特戴著護目鏡進入到磁共振成像設備.他可以從護目鏡上獲得外界的信息.所以他在掃描儀裡時我就從外界向他傳遞信息.這其實有一點詭異,因為默特看到的其實是這個他自己的大腦.圖像顯示出默特並不是安安靜靜躺著的.他大概在用右手做這個動作,因為大腦的左半球運動皮層處於活躍狀態.他自己也能同步看到這些畫面.這些可視化技術還相當新,但我們對其研究已經進行了一段時間. 這是另一次默特大腦的成像.這次成像我們讓默特從100開始倒數.於是他開始倒數 “100, 97, 94”一直數一直數.大家可以看到大腦在進行這個簡單數學演算漸漸的整個大腦都活躍起來.看起來非常有意思,哪天我們自己也可以試試.我們還可以指示默特做特定動作來做一些研究.大家可以看到他大腦後側視覺皮層活躍起來了,因為他自己正在看那裡,看自己的大腦.同時他又在聽從我們的指令進行動作.雖然大腦信號是在大腦深處傳遞,但它可以通過成像凸顯出來.因為所有的數據都集中在活躍區域.接下來大家就會觀察到變化——好,就是這裡.默特,動一下你的左腿.好,就是這裡.默特,動一下你的左腿.持續了大約20秒,於是突然大腦這一部分顏色鮮艷起來.大腦運動皮層活躍了.非常,非常不錯.這真的是一個非常厲害的工具.和前面所作演講結合起來看的話,利用這個工具我們可以直觀地觀察到神經系統是如何工作, 大腦是如何工作,而且這樣的觀察是高度可視化的,同時也具有高速分辨力. 最近我們中心也做了一些很有意思的研究.這是台CAT掃描儀——計算機輔助斷層攝影.這是瑞典諾爾雪平市郊動物園裡的一頭獅子.工作人員把她送到我們中心給她打了鎮靜劑然後把她放平送進掃描儀.接著就得到了這頭獅子的一套完整數據集.我們可以得到像這樣的清晰圖像,也可以把獅子的表皮剖開觀察她的內部結構.我們確實有這樣做過實驗.我想這也是未來對這種技術的某種絕好應用.因為目前我們對動物解剖依然知之甚少.對獸醫來說這些都是亟需掌握的基本信息.基本上所有東西都能拿來掃描,所有動物都可以,只要能塞進掃描儀.於是一頭熊就出現了.把牠塞進掃描儀稍微費了點功夫.這頭熊倒是非常溫順,討人喜歡.掃描結果出來了.這是熊的鼻子.對著這個鼻子也許你還想去摸摸,調整設置顯示成這樣以後大概就不想了.所以對熊還是要小心一點好. 結束前,我要感謝所有幫助我整理這些圖片的人.你們花費了很大精力來完成這些,收集數據,優化算法,編寫所有需要的軟體.你們都極具天賦.我一直堅持:只雇用比我聰明的人,這些人就幾乎人人比我聰明. 謝謝各位. （掌聲）
