Talk	en	zh-tw
ed_ulbrich_shows_how_benjamin_button_got_his_face	"I'm here today representing a team of artists and technologists and filmmakers that worked together on a remarkable film project for the last four years. And along the way they created a breakthrough in computer visualization. So I want to show you a clip of the film now. Hopefully it won't stutter. And if we did our jobs well, you won't know that we were even involved. Voice (Video): I don't know how it's possible ... but you seem to have more hair. Brad Pitt: What if I told you that I wasn't getting older ... but I was getting younger than everybody else? I was born with some form of disease. Voice: What kind of disease? BP: I was born old. Man: I'm sorry. BP: No need to be. There's nothing wrong with old age. Girl: Are you sick? BP: I heard momma and Tizzy whisper, and they said I was gonna die soon. But ... maybe not. Girl: You're different than anybody I've ever met. BB: There were many changes ... some you could see, some you couldn't. Hair started growing in all sorts of places, along with other things. I felt pretty good, considering. Ed Ulbrich: That was a clip from ""The Curious Case of Benjamin Button."" Many of you, maybe you've seen it or you've heard of the story, but what you might not know is that for nearly the first hour of the film, the main character, Benjamin Button, who's played by Brad Pitt, is completely computer-generated from the neck up. Now, there's no use of prosthetic makeup or photography of Brad superimposed over another actor's body. We've created a completely digital human head. So I'd like to start with a little bit of history on the project. This is based on an F. Scott Fitzgerald short story. It's about a man who's born old and lives his life in reverse. Now, this movie has floated around Hollywood for well over half a century, and we first got involved with the project in the early '90s, with Ron Howard as the director. We took a lot of meetings and we seriously considered it. But at the time we had to throw in the towel. It was deemed impossible. It was beyond the technology of the day to depict a man aging backwards. The human form, in particular the human head, has been considered the Holy Grail of our industry. The project came back to us about a decade later, and this time with a director named David Fincher. Now, Fincher is an interesting guy. David is fearless of technology, and he is absolutely tenacious. And David won't take ""no."" And David believed, like we do in the visual effects industry, that anything is possible as long as you have enough time, resources and, of course, money. And so David had an interesting take on the film, and he threw a challenge at us. He wanted the main character of the film to be played from the cradle to the grave by one actor. It happened to be this guy. We went through a process of elimination and a process of discovery with David, and we ruled out, of course, swapping actors. That was one idea: that we would have different actors, and we would hand off from actor to actor. We even ruled out the idea of using makeup. We realized that prosthetic makeup just wouldn't hold up, particularly in close-up. And makeup is an additive process. You have to build the face up. And David wanted to carve deeply into Brad's face to bring the aging to this character. He needed to be a very sympathetic character. So we decided to cast a series of little people that would play the different bodies of Benjamin at the different increments of his life and that we would in fact create a computer-generated version of Brad's head, aged to appear as Benjamin, and attach that to the body of the real actor. Sounded great. Of course, this was the Holy Grail of our industry, and the fact that this guy is a global icon didn't help either, because I'm sure if any of you ever stand in line at the grocery store, you know — we see his face constantly. So there really was no tolerable margin of error. There were two studios involved: Warner Brothers and Paramount. And they both believed this would make an amazing film, of course, but it was a very high-risk proposition. There was lots of money and reputations at stake. But we believed that we had a very solid methodology that might work ... But despite our verbal assurances, they wanted some proof. And so, in 2004, they commissioned us to do a screen test of Benjamin. And we did it in about five weeks. But we used lots of cheats and shortcuts. We basically put something together to get through the meeting. I'll roll that for you now. This was the first test for Benjamin Button. And in here, you can see, that's a computer-generated head — pretty good — attached to the body of an actor. And it worked. And it gave the studio great relief. After many years of starts and stops on this project, and making that tough decision, they finally decided to greenlight the movie. And I can remember, actually, when I got the phone call to congratulate us, to say the movie was a go, I actually threw up. (Laughter) You know, this is some tough stuff. So we started to have early team meetings, and we got everybody together, and it was really more like therapy in the beginning, convincing each other and reassuring each other that we could actually undertake this. We had to hold up an hour of a movie with a character. And it's not a special effects film; it has to be a man. We really felt like we were in a — kind of a 12-step program. And of course, the first step is: admit you've got a problem. (Laughter) So we had a big problem: we didn't know how we were going to do this. But we did know one thing. Being from the visual effects industry, we, with David, believed that we now had enough time, enough resources, and, God, we hoped we had enough money. And we had enough passion to will the processes and technology into existence. So, when you're faced with something like that, of course you've got to break it down. You take the big problem and you break it down into smaller pieces and you start to attack that. So we had three main areas that we had to focus on. We needed to make Brad look a lot older — needed to age him 45 years or so. And we also needed to make sure that we could take Brad's idiosyncrasies, his little tics, the little subtleties that make him who he is and have that translate through our process so that it appears in Benjamin on the screen. And we also needed to create a character that could hold up under, really, all conditions. He needed to be able to walk in broad daylight, at nighttime, under candlelight, he had to hold an extreme close-up, he had to deliver dialogue, he had to be able to run, he had to be able to sweat, he had to be able to take a bath, to cry, he even had to throw up. Not all at the same time — but he had to, you know, do all of those things. And the work had to hold up for almost the first hour of the movie. We did about 325 shots. So we needed a system that would allow Benjamin to do everything a human being can do. And we realized that there was a giant chasm between the state of the art of technology in 2004 and where we needed it to be. So we focused on motion capture. I'm sure many of you have seen motion capture. The state of the art at the time was something called marker-based motion capture. I'll give you an example here. It's basically the idea of, you wear a leotard, and they put some reflective markers on your body, and instead of using cameras, there're infrared sensors around a volume, and those infrared sensors track the three-dimensional position of those markers in real time. And then animators can take the data of the motion of those markers and apply them to a computer-generated character. You can see the computer characters on the right are having the same complex motion as the dancers. But we also looked at numbers of other films at the time that were using facial marker tracking, and that's the idea of putting markers on the human face and doing the same process. And as you can see, it gives you a pretty crappy performance. That's not terribly compelling. And what we realized was that what we needed was the information that was going on between the markers. We needed the subtleties of the skin. We needed to see skin moving over muscle moving over bone. We needed creases and dimples and wrinkles and all of those things. Our first revelation was to completely abort and walk away from the technology of the day, the status quo, the state of the art. So we aborted using motion capture. And we were now well out of our comfort zone, and in uncharted territory. So we were left with this idea that we ended up calling ""technology stew."" We started to look out in other fields. The idea was that we were going to find nuggets or gems of technology that come from other industries like medical imaging, the video game space, and re-appropriate them. And we had to create kind of a sauce. And the sauce was code in software that we'd written to allow these disparate pieces of technology to come together and work as one. Initially, we came across some remarkable research done by a gentleman named Dr. Paul Ekman in the early '70s. He believed that he could, in fact, catalog the human face. And he came up with this idea of Facial Action Coding System, or FACS. He believed that there were 70 basic poses or shapes of the human face, and that those basic poses or shapes of the face can be combined to create infinite possibilities of everything the human face is capable of doing. And of course, these transcend age, race, culture, gender. So this became the foundation of our research as we went forward. And then we came across some remarkable technology called Contour. And here you can see a subject having phosphorus makeup stippled on her face. And now what we're looking at is really creating a surface capture as opposed to a marker capture. The subject stands in front of a computer array of cameras, and those cameras can, frame-by-frame, reconstruct the geometry of exactly what the subject's doing at the moment. So, effectively, you get 3D data in real time of the subject. And if you look in a comparison, on the left, we see what volumetric data gives us and on the right you see what markers give us. So, clearly, we were in a substantially better place for this. But these were the early days of this technology, and it wasn't really proven yet. We measure complexity and fidelity of data in terms of polygonal count. And so, on the left, we were seeing 100,000 polygons. We could go up into the millions of polygons. It seemed to be infinite. This was when we had our ""Aha!"" This was the breakthrough. This is when we're like, ""OK, we're going to be OK, This is actually going to work."" And the ""Aha!"" was, what if we could take Brad Pitt, and we could put Brad in this device, and use this Contour process, and we could stipple on this phosphorescent makeup and put him under the black lights, and we could, in fact, scan him in real time performing Ekman's FACS poses. Right? So, effectively, we ended up with a 3D database of everything Brad Pitt's face is capable of doing. (Laughter) From there, we actually carved up those faces into smaller pieces and components of his face. So we ended up with literally thousands and thousands and thousands of shapes, a complete database of all possibilities that his face is capable of doing. Now, that's great, except we had him at age 44. We need to put another 40 years on him at this point. We brought in Rick Baker, and Rick is one of the great makeup and special effects gurus of our industry. And we also brought in a gentleman named Kazu Tsuji, and Kazu Tsuji is one of the great photorealist sculptors of our time. And we commissioned them to make a maquette, or a bust, of Benjamin. So, in the spirit of ""The Great Unveiling"" — I had to do this — I had to unveil something. So this is Ben 80. We created three of these: there's Ben 80, there's Ben 70, there's Ben 60. And this really became the template for moving forward. Now, this was made from a life cast of Brad. So, in fact, anatomically, it is correct. The eyes, the jaw, the teeth: everything is in perfect alignment with what the real guy has. We have these maquettes scanned into the computer at very high resolution — enormous polygonal count. And so now we had three age increments of Benjamin in the computer. But we needed to get a database of him doing more than that. We went through this process, then, called retargeting. This is Brad doing one of the Ekman FACS poses. And here's the resulting data that comes from that, the model that comes from that. Retargeting is the process of transposing that data onto another model. And because the life cast, or the bust — the maquette — of Benjamin was made from Brad, we could transpose the data of Brad at 44 onto Brad at 87. So now, we had a 3D database of everything Brad Pitt's face can do at age 87, in his 70s and in his 60s. Next we had to go into the shooting process. So while all that's going on, we're down in New Orleans and locations around the world. And we shot our body actors, and we shot them wearing blue hoods. So these are the gentleman who played Benjamin. And the blue hoods helped us with two things: one, we could easily erase their heads; and we also put tracking markers on their heads so we could recreate the camera motion and the lens optics from the set. But now we needed to get Brad's performance to drive our virtual Benjamin. And so we edited the footage that was shot on location with the rest of the cast and the body actors and about six months later we brought Brad onto a sound stage in Los Angeles and he watched on the screen. His job, then, was to become Benjamin. And so we looped the scenes. He watched again and again. We encouraged him to improvise. And he took Benjamin into interesting and unusual places that we didn't think he was going to go. We shot him with four HD cameras so we'd get multiple views of him and then David would choose the take of Brad being Benjamin that he thought best matched the footage with the rest of the cast. From there we went into a process called image analysis. And so here, you can see again, the chosen take. And you are seeing, now, that data being transposed on to Ben 87. And so, what's interesting about this is we used something called image analysis, which is taking timings from different components of Benjamin's face. And so we could choose, say, his left eyebrow. And the software would tell us that, well, in frame 14 the left eyebrow begins to move from here to here, and it concludes moving in frame 32. And so we could choose numbers of positions on the face to pull that data from. And then, the sauce I talked about with our technology stew — that secret sauce was, effectively, software that allowed us to match the performance footage of Brad in live action with our database of aged Benjamin, the FACS shapes that we had. On a frame-by-frame basis, we could actually reconstruct a 3D head that exactly matched the performance of Brad. So this was how the finished shot appeared in the film. And here you can see the body actor. And then this is what we called the ""dead head,"" no reference to Jerry Garcia. And then here's the reconstructed performance now with the timings of the performance. And then, again, the final shot. It was a long process. (Applause) The next section here, I'm going to just blast through this, because we could do a whole TEDTalk on the next several slides. We had to create a lighting system. So really, a big part of our processes was creating a lighting environment for every single location that Benjamin had to appear so that we could put Ben's head into any scene and it would exactly match the lighting that's on the other actors in the real world. We also had to create an eye system. We found the old adage, you know, ""The eyes are the window to the soul,"" absolutely true. So the key here was to keep everybody looking in Ben's eyes. And if you could feel the warmth, and feel the humanity, and feel his intent coming through the eyes, then we would succeed. So we had one person focused on the eye system for almost two full years. We also had to create a mouth system. We worked from dental molds of Brad. We had to age the teeth over time. We also had to create an articulating tongue that allowed him to enunciate his words. There was a whole system written in software to articulate the tongue. We had one person devoted to the tongue for about nine months. He was very popular. Skin displacement: another big deal. The skin had to be absolutely accurate. He's also in an old age home, he's in a nursing home around other old people, so he had to look exactly the same as the others. So, lots of work on skin deformation, you can see in some of these cases it works, in some cases it looks bad. This is a very, very, very early test in our process. So, effectively we created a digital puppet that Brad Pitt could operate with his own face. There were no animators necessary to come in and interpret behavior or enhance his performance. There was something that we encountered, though, that we ended up calling ""the digital Botox effect."" So, as things went through this process, Fincher would always say, ""It sandblasts the edges off of the performance."" And thing our process and the technology couldn't do, is they couldn't understand intent, the intent of the actor. So it sees a smile as a smile. It doesn't recognize an ironic smile, or a happy smile, or a frustrated smile. So it did take humans to kind of push it one way or another. But we ended up calling the entire process and all the technology ""emotion capture,"" as opposed to just motion capture. Take another look. Brad Pitt: Well, I heard momma and Tizzy whisper, and they said I was gonna die soon, but ... maybe not. EU: That's how to create a digital human in 18 minutes. (Applause) A couple of quick factoids; it really took 155 people over two years, and we didn't even talk about 60 hairstyles and an all-digital haircut. But, that is Benjamin. Thank you."	"今天我代表一群藝術家、技術人員、電影製作團隊過去四年間，我們共同完成了一項非凡的電影工程過程中也突破了電影繪圖技術 我先讓大家看個電影片段希望播放時不會卡住如果我們做好工作的話，你們不會看出我們動了什麼手腳 影片: 我不知道這是怎麼回事但你頭髮好像變多了 班傑明巴頓: 如果我告訴你，我沒有越來越老反而是越來越年輕了呢? 我出生就有一種病 什麼樣的病? 我出生就是老人的樣子 我很遺憾 不用遺憾，老又沒有錯 你生病了嗎? 我聽到媽和Tizzy講悄悄話說我活不了多久但... 也許他們說錯了 我沒見過像你這樣的人 我身體開始變化有些看的見，有些則不行體毛開始到處長還有一些其他東西其實這感覺還不賴 Ed Ulbrich: 這是電影""班傑明的奇幻旅程""片段相信很多人已經看過、或聽過這故事但你也許不知道電影前將近一小時由布萊德彼特飾演的主角 班傑明巴頓頸部以上完全是電腦合成的沒有靠任何化妝技術也不是布萊德的臉，蓋過另一個演員的身體我們全以數位方式製造出人頭 我先稍微說一下製作起源電影改編自史考特．費滋傑羅的短篇故事關於一個出生時80歲的男人，越活越年輕的故事這部電影在好萊塢就謠傳了很久超過50年有吧我們第一次接觸這計劃是在1990年初導演是朗霍華我們討論了很久，認真考慮要接到後來真的必須放棄的時候這已經被認為是不可能的任務當時的科技無法刻畫出返老還童的樣子整個人，特別是頭的部份在我們這一行，稱作是聖杯傳奇 約十年後又有人提起這計畫這次的導演是大衛芬奇芬奇真的是怪咖一個一點都不怕技術上困難就是堅持一定要做沒有人能拒絕他大衛、和我們做特效的持同樣看法沒有什麼是不可能的只要有足夠的時間、資源，當然還有資金大衛對這部電影有獨特的想法丟給我們這項考驗他要這部電影的主角，從出生到死亡都要是同一個人來演也就是這個人 我們經過了一連串淘汰、一連串新發現由於大衛堅持，我們排除使用不同演員的方法本來是想利用不同演員分別演出不同階段的人生 我們也排除利用化妝因為用石膏來弄根本做不出來尤其是近拍的時候況且也要先做出一張臉才能化大衛想要在布萊德彼特的臉上刻畫出皺紋來表現出這角色的年老樣貌他必須是個能讓人感同身受的角色所以我們決定選出一批小人物來演出班傑明在不同人生階段的各種身體樣貌然後我們利用電腦合成出布萊德的頭弄出不同年紀的班傑明在接上其他演員的身體聽起來很可行 當然，這可是我們這行的聖杯傳奇加上這傢伙沒有人不認識只要去過雜貨店一定看過他的臉這可不是什麼好處正因如此我們不能出錯過程中由兩個製片公司一起合作: 華納兄弟和派拉蒙當然，他們都深信這會是一部很讚的電影無庸置疑也是很冒險的計劃攸關大把金錢與名聲我們認為我們有很可靠的方法有可能成功的 但我們的口頭保證不夠他們要看到證據所以在2004年，他們要求看到班傑明的試鏡畫面我們做了約五週用了些旁門左道基本上我們東加西減地混過會談現在放給各位看。這是第一個班傑明巴頓在這裡可以看出是電腦合成出來的頭其實不錯，可以接到演員的身體也算成功，製片那邊終於放下大石經過多年走走停停作出一些困難的決定他們終於同意了我還記得，當我接到電話恭喜我們電影終於可以開拍我還吐了(笑聲)這是艱難的工作 我們開始初期的團隊會議集合大家剛開始有點像在諮商治療告訴彼此，我們一定做得到的要能撐過電影一個小時而這不是特效片，必須用真人這真的感覺像是戒酒那種12步計畫當然第一步就是: 承認問題的存在我們問題可大了不曉得到底怎麼開始但我們知道作視覺特效這行我們和大衛相信的，只有足夠時間足夠資源，天啊，希望我們有足夠資金我們也有足夠的熱誠，成功的決心 當你面對這樣的情況當然得先開始分析問題你面前有個大問題，把它拆成小問題個個攻破主要集中在三個點我們要布萊德看起來很老我們要幫他加個45歲左右我們得確定它有布萊德的臉部特徵臉部肌肉的抽動，各種他特有的細微變化經過我們的製作過程讓班傑明能夠呈現於大螢幕 我們也需要創造一個能夠做出每件事的角色他要能走在陽光下、夜晚、燭光下近拍不能露餡要能唸台詞要能跑、能流汗能洗澡、能哭甚至要能嘔吐當然不是同時但他要能做到每件事 而且要能撐住電影前一小時我們拍了325個景我們需要一個系統讓班傑明做出人類能做的所有事我們知道2004年當時最先進的技術和我們所想要的技術仍有很大一段距離 所以我們專注於動態捕捉技術我相信大部分的人都看過動態捕捉當時最先進的技術是利用標記點的動態捕捉給大家看個例子主要就是，穿著緊身衣在身上標出一些反光標記點物體四周放置紅外線感應而不是攝影機這些紅外線感應，會追蹤標記點當時的3D位置接著動畫師把標記點的數據輸入到電腦做出來的角色上可以看到右邊的動畫角色也能做出和舞者一樣的複雜動作 我們也看了一些使用臉部標記點的電影就是把點標記在臉上過程相同可以看到，感覺蠻假的這樣可沒法說服別人我們了解到我們所需要的是點與點之間的數據我們要皮膚的微妙變化要能看到骨頭到肌肉到皮膚的變化需要皺紋、酒窩、細紋等等 我們了解現在需要完全放棄當時最先進的技術我們放棄使用動態捕捉丟下了自己所熟悉的技術到了陌生的領域我們有了一種想法後來我們稱為科技大雜燴我們開始往別的領域找目的是要尋找出其他科技領域像是醫學影像或電玩空間技術之中的稀世珍品拿來重新研究雜燴也需要點醬料就是軟體程式碼讓這些不同的領域的技術能整合在一起 起先，我們找到一項傑出的研究是70年代初期，Paul Ekman博士提出的他相信他有辦法將人臉目錄化他想到使用臉部表情辨識系統(FACS)他相信人的臉型有70種基本表情動作只要把基本動作或臉型結合起來就能創造出人臉做的出的無限表情不論年齡、種族、文化、性別，這70種動作都是一樣的這變成我們往後的基礎點 之後我們發現了項非凡的技術叫做輪廓提取(Contour)現在看到的，是在人臉上輕輕塗上磷接下來是表面捕捉正好與點的捕捉相反接著站在一排電腦攝影機前面攝影機就會一景一景的重製出人當時的動作立刻就能得到3D數據各位可以看看比較左邊是體積數據的結果右邊是標記點的可以清楚看出，我們有很大的進步但這還是初期的結果還沒完全成功但我們依據多邊形數量來計算數據的複雜性和準確性左邊可以看到有十萬的多邊形還可以做到幾十萬個甚至無限多 這就是我們""啊哈""的時候這是個大突破這時我們才告訴自己一切都會沒事""啊哈""就是，我們或許可以請布萊德彼特使用這個裝置再搭配輪廓提取塗上發磷光的顏料用螢光燈照射拍攝Ekman的FACS臉部動作時就能即時掃描出來對吧，所以最後我們有了所有布萊德彼特的臉能做的所有表情(笑聲) 之後，將他的這些臉劃分成小塊小塊的最後有了成千上萬的形狀就是這張臉可以做的所有表情 看來很不錯，但我們只有44歲的他現在我們需要的是40年後的他我們找來了Rick BakerRick是我們這行的化妝、特效大師之一我們也請來一位Kazu Tsuji先生Kazu Tsuji是位很厲害的超現實雕刻家我們請他做班傑明的雕塑模型也可以說半身像本著盛大揭幕的精神我一定要揭露一下這是80歲的班傑明我們製作了三個80歲、70歲、60歲這就是之後我們依照的模板 這是以布萊德的模樣刻的所以當然是正確的樣貌眼睛、下巴、牙齒...每一樣都是真人比例我們把雕塑圖掃瞄到電腦使用超高解析很多的多邊形現在電腦裡有了三種班傑明年老的樣子 但我們需要更多的數據接下來的過程，叫做重定向這是布萊德做的Ekman FACS這是獲得的數據弄出來的樣子重定向就是將這些數據轉移到另一個模型上利用依照布萊德做出的班傑明的半身像我們就能把44歲布萊德的數據轉移到87歲的布萊德所以我們有了布萊德彼特87、70、60歲的所有臉部3D資料 接下來就是進行拍攝了同時間我們去了紐奧良和其他各地我們先拍了身體演員讓他們穿了藍色頭套這位先生飾演班傑明藍色頭套的功用是第一，可以輕易消除他們的頭還有，可以直接標上追蹤點以便讓我們能在片廠重製攝影動作和鏡片光學 現在需要拍布萊德的表演，來結合虛擬的班傑明我們重新編輯了實體拍攝的其他演員片段約半年後布萊德來到了洛杉磯片廠他看了已經拍好的畫面他的工作就是變成班傑明我們重複放了片段他看了又看我們鼓勵他即興唸台詞他也把班傑明演的有趣獨特出乎我們料想我們用四台高清攝影機拍攝為了取得各種角度的畫面然後大衛會選最符合該片段的臉部角度以搭配身體部分 再來的過程是影像分析可以看到，選好的片段資料轉換成班傑明87歲最有趣的就是我們使用畫面分析擷取各時間點班傑明臉部的每個部分所以我們可以隨便選，例如他的左眉軟體就會告訴我們在第14景，左眉從這裡移到這裡然後在第32景停止可以隨便取得任何一個表情的臉部數據 再來就是剛剛提過，科技大雜燴的醬料獨門配方就是，一個軟體能讓我們用布萊德的表演配上電腦裡班傑明年老的FACS數據有了各景的基礎我們可以重建出完全符合布萊德演出的3D頭 這就是電影最終畫面可以看到身體演員這個我們稱呼為死人頭，跟Jerry Garcia無關 這個是重建的演出配上正確的時間點最後，完成的畫面是很冗長的過程(掌聲) 下一段我只能稍微帶過要細說可能需要再做一次TEDTalk 我們必須創造出燈光系統這部分最難的是，創造班傑明每一場景的各種燈光環境到時候把頭接上才不會怪怪的而且才能完美搭配真實演員的身體演出部份 我們也需要創造眼睛系統因為俗話說的對""眼睛是靈魂之窗""真是一點也沒錯關鍵就是，要讓大家能在班傑明眼中感覺的到溫暖，感覺的到人性從眼中感受的到熱誠這樣就是成功了我們有個同仁專門弄眼睛系統快兩年的時間 再來是嘴巴系統我們從布萊德的齒模下手再來弄不同年紀的牙齒 還有要個能夠清晰發音的舌頭有個完整程式負責清晰地發音有位同仁花九個月的時間，致力於舌頭他很受歡迎 皮膚替換: 另一個挑戰皮膚必須完全精確他待過安養院、療養院身邊有其他老人所以他必須看起來跟其他人一樣老化皮膚是項大工程可以看到有時候成功有時候失敗這是我們非常早期的成果蠻成功的做出數位人偶讓布萊德彼特可以自己控制不需要動畫師來製作或加強表演 我們還經歷了一個過程我們稱作是數位肉毒桿菌在過程中芬奇說，這修飾了最後演出製作過程與技術唯一做不到的是無法理解意圖演員的意圖微笑就是微笑它不會分辨是諷刺的笑、或開心的笑或挫折的笑沒辦法理解其中的情緒意思 我們最後稱呼這個過程情緒捕捉技術與動態捕捉相反再看一次 布萊德彼特: 我聽到媽和Tizzy講悄悄話說我活不了多久但是... 也許他們說錯了 這就是18分鐘內解釋完(掌聲) 再補件有趣的事整個過程大概花了155人兩年的時間我們甚至沒聊到60個髮型和數位剪髮但，這就是大家的班傑明，謝謝"
