Talk	en	zh-tw
kwabena_boahen_on_a_computer_that_works_like_the_brain	"I got my first computer when I was a teenager growing up in Accra, and it was a really cool device. You could play games with it. You could program it in BASIC. And I was fascinated. So I went into the library to figure out how did this thing work. I read about how the CPU is constantly shuffling data back and forth between the memory, the RAM and the ALU, the arithmetic and logic unit. And I thought to myself, this CPU really has to work like crazy just to keep all this data moving through the system. But nobody was really worried about this. When computers were first introduced, they were said to be a million times faster than neurons. People were really excited. They thought they would soon outstrip the capacity of the brain. This is a quote, actually, from Alan Turing: ""In 30 years, it will be as easy to ask a computer a question as to ask a person."" This was in 1946. And now, in 2007, it's still not true. And so, the question is, why aren't we really seeing this kind of power in computers that we see in the brain? What people didn't realize, and I'm just beginning to realize right now, is that we pay a huge price for the speed that we claim is a big advantage of these computers. Let's take a look at some numbers. This is Blue Gene, the fastest computer in the world. It's got 120,000 processors; they can basically process 10 quadrillion bits of information per second. That's 10 to the sixteenth. And they consume one and a half megawatts of power. So that would be really great, if you could add that to the production capacity in Tanzania. It would really boost the economy. Just to go back to the States, if you translate the amount of power or electricity this computer uses to the amount of households in the States, you get 1,200 households in the U.S. That's how much power this computer uses. Now, let's compare this with the brain. This is a picture of, actually Rory Sayres' girlfriend's brain. Rory is a graduate student at Stanford. He studies the brain using MRI, and he claims that this is the most beautiful brain that he has ever scanned. (Laughter) So that's true love, right there. Now, how much computation does the brain do? I estimate 10 to the 16 bits per second, which is actually about very similar to what Blue Gene does. So that's the question. The question is, how much — they are doing a similar amount of processing, similar amount of data — the question is how much energy or electricity does the brain use? And it's actually as much as your laptop computer: it's just 10 watts. So what we are doing right now with computers with the energy consumed by 1,200 houses, the brain is doing with the energy consumed by your laptop. So the question is, how is the brain able to achieve this kind of efficiency? And let me just summarize. So the bottom line: the brain processes information using 100,000 times less energy than we do right now with this computer technology that we have. How is the brain able to do this? Let's just take a look about how the brain works, and then I'll compare that with how computers work. So, this clip is from the PBS series, ""The Secret Life of the Brain."" It shows you these cells that process information. They are called neurons. They send little pulses of electricity down their processes to each other, and where they contact each other, those little pulses of electricity can jump from one neuron to the other. That process is called a synapse. You've got this huge network of cells interacting with each other — about 100 million of them, sending about 10 quadrillion of these pulses around every second. And that's basically what's going on in your brain right now as you're watching this. How does that compare with the way computers work? In the computer, you have all the data going through the central processing unit, and any piece of data basically has to go through that bottleneck, whereas in the brain, what you have is these neurons, and the data just really flows through a network of connections among the neurons. There's no bottleneck here. It's really a network in the literal sense of the word. The net is doing the work in the brain. If you just look at these two pictures, these kind of words pop into your mind. This is serial and it's rigid — it's like cars on a freeway, everything has to happen in lockstep — whereas this is parallel and it's fluid. Information processing is very dynamic and adaptive. So I'm not the first to figure this out. This is a quote from Brian Eno: ""the problem with computers is that there is not enough Africa in them."" (Laughter) Brian actually said this in 1995. And nobody was listening then, but now people are beginning to listen because there's a pressing, technological problem that we face. And I'll just take you through that a little bit in the next few slides. This is — it's actually really this remarkable convergence between the devices that we use to compute in computers, and the devices that our brains use to compute. The devices that computers use are what's called a transistor. This electrode here, called the gate, controls the flow of current from the source to the drain — these two electrodes. And that current, electrical current, is carried by electrons, just like in your house and so on. And what you have here is, when you actually turn on the gate, you get an increase in the amount of current, and you get a steady flow of current. And when you turn off the gate, there's no current flowing through the device. Your computer uses this presence of current to represent a one, and the absence of current to represent a zero. Now, what's happening is that as transistors are getting smaller and smaller and smaller, they no longer behave like this. In fact, they are starting to behave like the device that neurons use to compute, which is called an ion channel. And this is a little protein molecule. I mean, neurons have thousands of these. And it sits in the membrane of the cell and it's got a pore in it. And these are individual potassium ions that are flowing through that pore. Now, this pore can open and close. But, when it's open, because these ions have to line up and flow through, one at a time, you get a kind of sporadic, not steady — it's a sporadic flow of current. And even when you close the pore — which neurons can do, they can open and close these pores to generate electrical activity — even when it's closed, because these ions are so small, they can actually sneak through, a few can sneak through at a time. So, what you have is that when the pore is open, you get some current sometimes. These are your ones, but you've got a few zeros thrown in. And when it's closed, you have a zero, but you have a few ones thrown in. Now, this is starting to happen in transistors. And the reason why that's happening is that, right now, in 2007 — the technology that we are using — a transistor is big enough that several electrons can flow through the channel simultaneously, side by side. In fact, there's about 12 electrons can all be flowing this way. And that means that a transistor corresponds to about 12 ion channels in parallel. Now, in a few years time, by 2015, we will shrink transistors so much. This is what Intel does to keep adding more cores onto the chip. Or your memory sticks that you have now can carry one gigabyte of stuff on them — before, it was 256. Transistors are getting smaller to allow this to happen, and technology has really benefitted from that. But what's happening now is that in 2015, the transistor is going to become so small, that it corresponds to only one electron at a time can flow through that channel, and that corresponds to a single ion channel. And you start having the same kind of traffic jams that you have in the ion channel. The current will turn on and off at random, even when it's supposed to be on. And that means your computer is going to get its ones and zeros mixed up, and that's going to crash your machine. So, we are at the stage where we don't really know how to compute with these kinds of devices. And the only kind of thing — the only thing we know right now that can compute with these kinds of devices are the brain. OK, so a computer picks a specific item of data from memory, it sends it into the processor or the ALU, and then it puts the result back into memory. That's the red path that's highlighted. The way brains work, I told you all, you have got all these neurons. And the way they represent information is they break up that data into little pieces that are represented by pulses and different neurons. So you have all these pieces of data distributed throughout the network. And then the way that you process that data to get a result is that you translate this pattern of activity into a new pattern of activity, just by it flowing through the network. So you set up these connections such that the input pattern just flows and generates the output pattern. What you see here is that there's these redundant connections. So if this piece of data or this piece of the data gets clobbered, it doesn't show up over here, these two pieces can activate the missing part with these redundant connections. So even when you go to these crappy devices where sometimes you want a one and you get a zero, and it doesn't show up, there's redundancy in the network that can actually recover the missing information. It makes the brain inherently robust. What you have here is a system where you store data locally. And it's brittle, because each of these steps has to be flawless, otherwise you lose that data, whereas in the brain, you have a system that stores data in a distributed way, and it's robust. What I want to basically talk about is my dream, which is to build a computer that works like the brain. This is something that we've been working on for the last couple of years. And I'm going to show you a system that we designed to model the retina, which is a piece of brain that lines the inside of your eyeball. We didn't do this by actually writing code, like you do in a computer. In fact, the processing that happens in that little piece of brain is very similar to the kind of processing that computers do when they stream video over the Internet. They want to compress the information — they just want to send the changes, what's new in the image, and so on — and that is how your eyeball is able to squeeze all that information down to your optic nerve, to send to the rest of the brain. Instead of doing this in software, or doing those kinds of algorithms, we went and talked to neurobiologists who have actually reverse engineered that piece of brain that's called the retina. And they figured out all the different cells, and they figured out the network, and we just took that network and we used it as the blueprint for the design of a silicon chip. So now the neurons are represented by little nodes or circuits on the chip, and the connections among the neurons are represented, actually modeled by transistors. And these transistors are behaving essentially just like ion channels behave in the brain. It will give you the same kind of robust architecture that I described. Here is actually what our artificial eye looks like. The retina chip that we designed sits behind this lens here. And the chip — I'm going to show you a video that the silicon retina put out of its output when it was looking at Kareem Zaghloul, who's the student who designed this chip. Let me explain what you're going to see, OK, because it's putting out different kinds of information, it's not as straightforward as a camera. The retina chip extracts four different kinds of information. It extracts regions with dark contrast, which will show up on the video as red. And it extracts regions with white or light contrast, which will show up on the video as green. This is Kareem's dark eyes and that's the white background that you see here. And then it also extracts movement. When Kareem moves his head to the right, you will see this blue activity there; it represents regions where the contrast is increasing in the image, that's where it's going from dark to light. And you also see this yellow activity, which represents regions where contrast is decreasing; it's going from light to dark. And these four types of information — your optic nerve has about a million fibers in it, and 900,000 of those fibers send these four types of information. So we are really duplicating the kind of signals that you have on the optic nerve. What you notice here is that these snapshots taken from the output of the retina chip are very sparse, right? It doesn't light up green everywhere in the background, only on the edges, and then in the hair, and so on. And this is the same thing you see when people compress video to send: they want to make it very sparse, because that file is smaller. And this is what the retina is doing, and it's doing it just with the circuitry, and how this network of neurons that are interacting in there, which we've captured on the chip. But the point that I want to make — I'll show you up here. So this image here is going to look like these ones, but here I'll show you that we can reconstruct the image, so, you know, you can almost recognize Kareem in that top part there. And so, here you go. Yes, so that's the idea. When you stand still, you just see the light and dark contrasts. But when it's moving back and forth, the retina picks up these changes. And that's why, you know, when you're sitting here and something happens in your background, you merely move your eyes to it. There are these cells that detect change and you move your attention to it. So those are very important for catching somebody who's trying to sneak up on you. Let me just end by saying that this is what happens when you put Africa in a piano, OK. This is a steel drum here that has been modified, and that's what happens when you put Africa in a piano. And what I would like us to do is put Africa in the computer, and come up with a new kind of computer that will generate thought, imagination, be creative and things like that. Thank you. (Applause) Chris Anderson: Question for you, Kwabena. Do you put together in your mind the work you're doing, the future of Africa, this conference — what connections can we make, if any, between them? Kwabena Boahen: Yes, like I said at the beginning, I got my first computer when I was a teenager, growing up in Accra. And I had this gut reaction that this was the wrong way to do it. It was very brute force; it was very inelegant. I don't think that I would've had that reaction, if I'd grown up reading all this science fiction, hearing about RD2D2, whatever it was called, and just — you know, buying into this hype about computers. I was coming at it from a different perspective, where I was bringing that different perspective to bear on the problem. And I think a lot of people in Africa have this different perspective, and I think that's going to impact technology. And that's going to impact how it's going to evolve. And I think you're going to be able to see, use that infusion, to come up with new things, because you're coming from a different perspective. I think we can contribute. We can dream like everybody else. CA: Thanks Kwabena, that was really interesting. Thank you. (Applause)"	"我在阿克拉(迦納首都)的童年時期曾獲得一台電腦那真的是一台非常酷的機械你可以用來玩遊戲，也可以用BASIC語言來寫程式我從那時開始對它深深地著迷所以我便去圖書館想找出這東西到底是如何運作的我讀了有關CPU(中央處理器)是如何來回傳送資料在RAM(隨機存取記憶體)和ALU(算術邏輯單元)之間也就是負責算術和邏輯運算的單元我就想到，CPU必須拼了命工作才能將所有資料傳送到每個系統中 但根本沒人會去想過這個問題當電腦最初被發明時他們聲稱傳輸速度可以比神經元細胞要快一百萬倍大家對此十分興奮認為很快就可以開發出超過人腦容量的機械這裡有句艾倫‧圖靈所講過的話:""三十年內，問電腦一個問題就會變的跟問人一樣容易。""當年是1946年，但現在已經2007年了，還沒實現問題在於，為什麼我們看不到電腦像人腦一樣的潛在力量 一般人無法理解的，而我才正要開始探索的是我們為了追求運算速度而付出了許多代價而且聲稱那是電腦的一大優勢讓我們來看看一些例子這是""藍色基因""，世界上最快的電腦這裡總共有十二萬顆處理器美秒可以處理千兆以上位元運算的能力也就是十的十六次方。 但它們必須消耗一百五十萬瓦特的電力那是一個極大的的數目，如果你把這個數量加進坦尚尼亞的勞動生產力中那將會大大地增進當地的經濟成長好吧，主題回到美國如果你把那些電腦所使用的電力轉換成每戶美國人家庭用電的量可以供應1200戶的家庭使用這可以顯示出那些電腦需吃掉多少電 現在，讓我們跟大腦做個比較這張影像，其實是羅瑞‧賽爾的女朋友的大腦羅瑞是史丹佛大學的一名研究生他用核磁共振研究大腦並且說這是他看過最漂亮的大腦(笑)多麼動人的真愛啊，就在你眼前。回到正題，人腦到底可以做多少計算?我估計是每秒十的十六次方位元就跟""藍色基因""做的很相像所以問題來了，問題就是，當他們在進行同樣的運算處理、同量的資料到底人腦需要花掉多少的能量或是電力?事實上就跟你的筆記型電腦差不多就只有十瓦特所以我們現在要改進電腦的地方是那些花掉一千兩百戶家庭用電的超級電腦所做的事人腦只需要消耗像筆電一樣的能量就可完成 問題是，人腦到底怎樣才可以達到這樣子的效率?先讓我做個結論，重點是與我們現有的電腦科技相比，人腦可以用十萬分之一的能量去處理同樣的訊息量大腦到底是怎麼做到的?我們先來了解一下大腦的運作方式然後我再跟電腦的運作方式作比較這個影片片斷是來自於PBS系列""大腦的秘密""你可以看到細胞是如何處理資訊他們被稱作神經元在處理訊息時會放出微弱的電流訊號給彼此當互相接觸時電流訊號即可從神經元移動到下個神經元這些被稱為突觸你現在了解這龐大的細胞網路是如何與別人互動大概一億個細胞每秒送出約一千萬億個脈衝這就是你現在看著這支影片時你大腦正在做的事 如果電腦運作的方式跟大腦相比呢?對於電腦，所有資料都必須經過中央處理單元(CPU)而且所有的資料基本上都必須通過那瓶頸但在大腦中，我們擁有的是神經元資訊只需要流過這些神經元連結的網路根本不存在所謂的瓶頸這的的確確是如同字面上所說的""網路""這網路就在大腦裡運作著如果你看著這兩張圖片這幾個字眼就會在你腦海中浮現連續、序列且死板的；就像車子在高速公路上每件事都必須按照先後來處理但這邊代表的是平行的、流暢的資訊處理過程十分動態並具有適應性 我並不是第一個提出這個見解的人，布萊恩‧伊諾曾說過:""電腦討人厭的地方就是它裡面沒有非洲 (指十分死板且毫無生氣可言)""(笑)布萊恩在1995年時說了這句話但在當時根本沒人理他但現在人們開始注意到他講的話了因為我們遇到了難以解決的技術性問題我將會在後面幾張投影片跟你稍微介紹 這事實上，是一個非常令人印象深刻的巧合對於我們在電腦中用來計算的裝置和大腦中負責計算的部份電腦中負責計算的裝置叫電晶體這裡的電極，稱為閘極，負責控制電流的進出從源極到洩極然後電流呢就像家裡用的那些當你把閘極打開，這裡發生什麼事呢電流通過的量將會瞬間增加，然後得到一穩定的電流當關上閘極時，將沒有電流通過電腦就是利用電流通過代表一沒有電流通過時代表零 現在，如果當電晶體變得越來越小的時候會發生什麼事?他們就不再呈現這樣的行為事實上，將變得類似神經元傳送訊息一樣的方法我們稱之為離子通道這是一個小小的蛋白質分子意思是，神經元有幾千個這種分子它位於細胞膜上並且位於中央有條通道在細胞中的鉀離子就可以穿過這條通道而且這條通道可關可開但當通道開啟時，離子必須排成一列一次只能通過一個，所以變成零星發生的並非持續穩定的呈現的是斷斷續續的電流而且當你關閉通道的時候，神經元可以這樣做他們可以藉由開關離子通道來產生電流當它關閉的時候，因為離子體積很小所以他們其實可以偶爾偷偷從通道溜走所以變成當通道開啟時，你可以得到電流通過但裡面偶爾會有些""零電流""藏在裡面當通道關閉時，基本上是沒有電流通過的但你偶爾會收到一些電流訊號流過，懂嗎? 現在這就是我們希望讓電晶體產生同樣的效果原因是直到現在，2007年我們的科技才足以讓電晶體中的通道大到能同時讓電子並列地通過事實上，大概可以允許十二個電子同時流過通道那樣代表著一個電晶體相當於12個平行的離子通道在未來幾年，也許在2015之前，我們打算將電晶體縮到更小這就是英特爾打算進行的，目的是加更多核心到一個晶片上或是記憶體上，這樣你就可以擁有1GB容量記得以前只有256MB呢電晶體體積越來越小導致上述的這些可能成真而且也為科技帶來不少利益 現在我們打算做的是，在2015年，電晶體縮小到相當一次只能讓一個電子流過它的通道那就代表一個獨立的離子通道有時候在通道內就會產生了像交通阻塞那樣的情況電流將不規則地斷斷續續、若有若無就算它本來應該是開放要讓電流通過的這樣代表你們的電腦將會收到混合著一和零的訊號，這將會讓電腦當機 所以這就是我們現階段面臨的問題我們並不知道該如何用這種方法來進行計算我們現在唯一知道的是能用這種機制進行計算的，就只有人腦而已。 好，所以電腦從記憶體中挑取某些資料送到中央處理器或是算術邏輯單元然後再將結果送回記憶體這就是圖上用紅線標示的途徑大腦運作的方式是，用這些神經元他們呈現這些訊息的方法是將訊息打散成許多碎片分別以脈衝和不同的神經元負責所以這些訊息片斷透過神經的網路分散各地這些資料再被經過處理並產生結果的方法就是轉譯原本的行為模式進入另一種行為模式而且只靠通過這個網路就可達成目的所以我們現在畫面上可以看到這些連結讓輸入的行為模式只要通過就可以產生輸出的新模式 你在這裡可以看到多重的連結分支所以如果這個綠色的片斷或是另個綠色的片斷遺失了它沒有在另一端出現，那麼另兩個片斷就可以補完遺失的部份透過這些多重的分支就算設備品質本身不良或有瑕疵意思是你想要""一""卻收到""零""的時候如果這裡有多重的網路分支他們就可以恢復遺失的部份資料這就是大腦天生強健的秘密這邊的系統使用在局部儲存資料的方式這種方法很脆弱，因為每一步不能發生任何差錯要不然資料就會遺失，但在大腦中資料是以分散式的方式儲存著，十分強韌 我現在想談論的是有關我的夢想也就是建造一台能像大腦般運作的電腦這是我們過去幾年來一直在持續進行的計劃我現在要介紹給你看我們所設計的系統用以模擬視網膜就是在眼球內連接大腦的細胞我們進行這個計劃不用一般編寫程式碼的方式事實上，那是因為腦中的處理訊息的過程很像電腦當要透過網路傳送影片的程序他們必須將大量的資料進行壓縮只針對那些有改變的影像進行傳送而這就是如何你的眼睛具有壓縮所有訊息到你的視神經並送到大腦的其他部份 而非用軟體處理這個模擬，或用演算法進行我們和一位神經生物學家進行訪談過他曾經對視網膜進行反向工程分析出所有不同的細胞和解構出神經網路，我們就參考該網路做為藍圖並設計了一顆矽晶片我們在晶片上用節點和電路來代表神經元並且神經元之間用電晶體來作為連接當然這些電晶體必須要正常運作就像大腦中的離子通道一樣我等會將會介紹給你我剛描述的那個穩固的架構模式 做出的人工眼睛長的像這樣我們設計的視網膜晶片設置在這個透鏡後還有晶片，我將會播放一段影片顯示這個矽製的視網膜輸出的結果當它看著卡林姆‧沙酷也就是設計了這整個晶片的學生讓我解釋你等會將看到什麼，好嗎?因為它輸出很多不同的訊息所以這並不像照相機一樣簡單明瞭這個視網膜晶片可以解析出四種資訊它可以解析出較暗的區域並在影片中用紅色表示和解析出白色或較亮的區域用綠色標記在影片中 這是卡林姆的深褐色眼睛你這裡可以看到的是白色的部份它同時可以解析出動作當卡林姆將頭往右移你可以見到藍色字的這裡表現出影像中的對比度增加了從暗轉向亮度漸增你也可以看到黃色字的這裡代表對比度下降了影像從亮漸漸變暗這四種訊息在你的視神經內有大概一百萬個纖維而其中九十萬個這種纖維會送出上述的這四種訊號所以實際上我們正在仿傚視神經內的這幾種訊號 你現在看到的這幾張從視網膜晶片輸出的影像事實上都非常粗糙稀疏幾乎很少有綠點整片出現在影像中只有少數零星幾個出現在邊緣，諸如此類這跟人類要壓縮影片準備傳送是同樣的原理他們打算將影像弄的非常分散因為這樣可以讓檔案容量大幅縮小，而這正是視網膜在進行的事我們用電路裝置來模擬其行為，並用晶片補捉神經元網路的行為模式 但我想強調的是，最上面這個影像跟其他的並沒有什麼相異之處可是我們能重建這個影像所以，就你所知，你幾乎可以從上面這個圖來辨認卡林姆請看。這就是我們最主要的概念當人靜止不動的時候，你只能見到黑和白的對比但當人前後移動的時候，視網膜可以接收到這些改變這也就是為什麼，以你的經驗，當你坐在那有事情在你背後發生時你很少會把眼神轉移過去細胞們就可以偵測到那些改變然後才讓你去注意到它所以這對於發現是誰從背後偷偷走近你是十分重要的 讓我說句話作結：這就是當你把一個""非洲""裝進鋼琴裡的情況這是一個被改裝過的鋼鼓而這就是把""非洲""放到一架鋼琴裡的情況而我想要做的就是，把""非洲""裝入電腦裡並且研發出一種全新的電腦它可以產生想法、幻想、創意等那些東西謝謝你們。(掌聲) 基斯安德森：問你一個問題，卡貝納你覺得你現在作的工作和非洲的未來、這次的大會在這三點之間，有何關聯? 卡貝納‧博罕：是的，就如我剛在開頭講過的我在阿克拉的童年時期曾獲得一台電腦我的直覺告訴我這種方法是錯誤的因為這樣非常不理性且一點也不優雅我認為我不會對電腦產生興趣如果我從小就讀著科幻小說長大聽著有關星際大戰機器人RD2D2，不管怎麼稱呼以及認同有關電腦的誇大炒作消息我是從另一個不同的視角來接觸電腦的我正是帶著這種不同的觀點來解決這些問題並且我相信很多非洲人有這種不同的觀點我認為那將會衝擊現有的科技並且會衝擊技術演化的方向我想你們應該能了解，利用那種新思維來激發出創新的點子因為你立足於另一種全然不同的觀點我覺得我們也可以產生貢獻，或是像其他人般做夢 基斯安德森：謝謝卡貝納，這真是非常有趣謝謝你們 (掌聲)"
