Talk	en	zh-tw
golan_levin_ted2009	"Hello! My name is Golan Levin. I'm an artist and an engineer, which is, increasingly, a more common kind of hybrid. But I still fall into this weird crack where people don't seem to understand me. And I was looking around and I found this wonderful picture. It's a letter from ""Artforum"" in 1967 saying ""We can't imagine ever doing a special issue on electronics or computers in art."" And they still haven't. And lest you think that you all, as the digerati, are more enlightened, I went to the Apple iPhone app store the other day. Where's art? I got productivity. I got sports. And somehow the idea that one would want to make art for the iPhone, which my friends and I are doing now, is still not reflected in our understanding of what computers are for. So, from both directions, there is kind of, I think, a lack of understanding about what it could mean to be an artist who uses the materials of his own day, or her own day, which I think artists are obliged to do, is to really explore the expressive potential of the new tools that we have. In my own case, I'm an artist, and I'm really interested in expanding the vocabulary of human action, and basically empowering people through interactivity. I want people to discover themselves as actors, as creative actors, by having interactive experiences. A lot of my work is about trying to get away from this. This a photograph of the desktop of a student of mine. And when I say desktop, I don't just mean the actual desk where his mouse has worn away the surface of the desk. If you look carefully, you can even see a hint of the Apple menu, up here in the upper left, where the virtual world has literally punched through to the physical. So this is, as Joy Mountford once said, ""The mouse is probably the narrowest straw you could try to suck all of human expression through."" (Laughter) And the thing I'm really trying to do is enabling people to have more rich kinds of interactive experiences. How can we get away from the mouse and use our full bodies as a way of exploring aesthetic experiences, not necessarily utilitarian ones. So I write software. And that's how I do it. And a lot of my experiences resemble mirrors in some way. Because this is, in some sense, the first way, that people discover their own potential as actors, and discover their own agency. By saying ""Who is that person in the mirror? Oh it's actually me."" And so, to give an example, this is a project from last year, which is called the Interstitial Fragment Processor. And it allows people to explore the negative shapes that they create when they're just going about their everyday business. So as people make shapes with their hands or their heads and so forth, or with each other, these shapes literally produce sounds and drop out of thin air — basically taking what's often this, kind of, unseen space, or this undetected space, and making it something real, that people then can appreciate and become creative with. So again, people discover their creative agency in this way. And their own personalities come out in totally unique ways. So in addition to using full-body input, something that I've explored now, for a while, has been the use of the voice, which is an immensely expressive system for us, vocalizing. Song is one of our oldest ways of making ourselves heard and understood. And I came across this fantastic research by Wolfgang Köhler, the so-called father of gestalt psychology, from 1927, who submitted to an audience like yourselves the following two shapes. And he said one of them is called Maluma. And one of them is called Taketa. Which is which? Anyone want to hazard a guess? Maluma is on top. Yeah. So. As he says here, most people answer without any hesitation. So what we're really seeing here is a phenomenon called phonaesthesia, which is a kind of synesthesia that all of you have. And so, whereas Dr. Oliver Sacks has talked about how perhaps one person in a million actually has true synesthesia, where they hear colors or taste shapes, and things like this, phonaesthesia is something we can all experience to some extent. It's about mappings between different perceptual domains, like hardness, sharpness, brightness and darkness, and the phonemes that we're able to speak with. So 70 years on, there's been some research where cognitive psychologists have actually sussed out the extent to which, you know, L, M and B are more associated with shapes that look like this, and P, T and K are perhaps more associated with shapes like this. And here we suddenly begin to have a mapping between curvature that we can exploit numerically, a relative mapping between curvature and shape. So it occurred to me, what happens if we could run these backwards? And thus was born the project called Remark, which is a collaboration with Zachary Lieberman and the Ars Electronica Futurelab. And this is an interactive installation which presents the fiction that speech casts visible shadows. So the idea is you step into a kind of a magic light. And as you do, you see the shadows of your own speech. And they sort of fly away, out of your head. If a computer speech recognition system is able to recognize what you're saying, then it spells it out. And if it isn't then it produces a shape which is very phonaesthetically tightly coupled to the sounds you made. So let's bring up a video of that. (Applause) Thanks. So. And this project here, I was working with the great abstract vocalist, Jaap Blonk. And he is a world expert in performing ""The Ursonate,"" which is a half-an-hour nonsense poem by Kurt Schwitters, written in the 1920s, which is half an hour of very highly patterned nonsense. And it's almost impossible to perform. But Jaap is one of the world experts in performing it. And in this project we've developed a form of intelligent real-time subtitles. So these are our live subtitles, that are being produced by a computer that knows the text of ""The Ursonate"" — fortunately Jaap does too, very well — and it is delivering that text at the same time as Jaap is. So all the text you're going to see is real-time generated by the computer, visualizing what he's doing with his voice. Here you can see the set-up where there is a screen with the subtitles behind him. Okay. So ... (Applause) The full videos are online if you are interested. I got a split reaction to that during the live performance, because there is some people who understand live subtitles are a kind of an oxymoron, because usually there is someone making them afterwards. And then a bunch of people who were like, ""What's the big deal? I see subtitles all the time on television."" You know? They don't imagine the person in the booth, typing it all. So in addition to the full body, and in addition to the voice, another thing that I've been really interested in, most recently, is the use of the eyes, or the gaze, in terms of how people relate to each other. It's a really profound amount of nonverbal information that's communicated with the eyes. And it's one of the most interesting technical challenges that's very currently active in the computer sciences: being able to have a camera that can understand, from a fairly big distance away, how these little tiny balls are actually pointing in one way or another to reveal what you're interested in, and where your attention is directed. So there is a lot of emotional communication that happens there. And so I've been beginning, with a variety of different projects, to understand how people can relate to machines with their eyes. And basically to ask the questions: What if art was aware that we were looking at it? How could it respond, in a way, to acknowledge or subvert the fact that we're looking at it? And what could it do if it could look back at us? And so those are the questions that are happening in the next projects. In the first one which I'm going to show you, called Eyecode, it's a piece of interactive software in which, if we read this little circle, ""the trace left by the looking of the previous observer looks at the trace left by the looking of previous observer."" The idea is that it's an image wholly constructed from its own history of being viewed by different people in an installation. So let me just switch over so we can do the live demo. So let's run this and see if it works. Okay. Ah, there is lots of nice bright video. There is just a little test screen that shows that it's working. And what I'm just going to do is — I'm going to hide that. And you can see here that what it's doing is it's recording my eyes every time I blink. Hello? And I can ... hello ... okay. And no matter where I am, what's really going on here is that it's an eye-tracking system that tries to locate my eyes. And if I get really far away I'm blurry. You know, you're going to have these kind of blurry spots like this that maybe only resemble eyes in a very very abstract way. But if I come up really close and stare directly at the camera on this laptop then you'll see these nice crisp eyes. You can think of it as a way of, sort of, typing, with your eyes. And what you're typing are recordings of your eyes as you're looking at other peoples' eyes. So each person is looking at the looking of everyone else before them. And this exists in larger installations where there are thousands and thousands of eyes that people could be staring at, as you see who's looking at the people looking at the people looking before them. So I'll just add a couple more. Blink. Blink. And you can see, just once again, how it's sort of finding my eyes and doing its best to estimate when it's blinking. Alright. Let's leave that. So that's this kind of recursive observation system. (Applause) Thank you. The last couple pieces I'm going to show are basically in the new realm of robotics — for me, new for me. It's called Opto-Isolator. And I'm going to show a video of the older version of it, which is just a minute long. Okay. In this case, the Opto-Isolator is blinking in response to one's own blinks. So it blinks one second after you do. This is a device which is intended to reduce the phenomenon of gaze down to the simplest possible materials. Just one eye, looking at you, and eliminating everything else about a face, but just to consider gaze in an isolated way as a kind of, as an element. And at the same time, it attempts to engage in what you might call familiar psycho-social gaze behaviors. Like looking away if you look at it too long because it gets shy, or things like that. Okay. So the last project I'm going to show is this new one called Snout. (Laughter) It's an eight-foot snout, with a googly eye. (Laughter) And inside it's got an 800-pound robot arm that I borrowed, (Laughter) from a friend. (Laughter) It helps to have good friends. I'm at Carnegie Mellon; we've got a great Robotics Institute there. I'd like to show you thing called Snout, which is — The idea behind this project is to make a robot that appears as if it's continually surprised to see you. (Laughter) The idea is that basically — if it's constantly like ""Huh? ... Huh?"" That's why its other name is Doubletaker, Taker of Doubles. It's always kind of doing a double take: ""What?"" And the idea is basically, can it look at you and make you feel as if like, ""What? Is it my shoes?"" ""Got something on my hair?"" Here we go. Alright. Checking him out ... For you nerds, here's a little behind-the-scenes. It's got a computer vision system, and it tries to look at the people who are moving around the most. Those are its targets. Up there is the skeleton, which is actually what it's trying to do. It's really about trying to create a novel body language for a new creature. Hollywood does this all the time, of course. But also have the body language communicate something to the person who is looking at it. This language is communicating that it is surprised to see you, and it's interested in looking at you. (Laughter) (Applause) Thank you very much. That's all I've got for today. And I'm really happy to be here. Thank you so much. (Applause)"	大家好！我的名字是戈蘭 萊文。我是一個藝術家和工程師。越來越普遍的一種誇領域的結合。不過我仍然常陷入這奇怪的夾縫中，人們似乎不理解我在做甚麼。我就到處搜索，發現了這幅美妙的圖片。這是在1967年來自“藝術論壇”雜誌的一封信上面說“我們永遠不能想像做一份關於電子或電腦藝術的特別刊物。”這對他們來說至今仍然是無法想像。如果而在座的各位都自認為是數字精英，比他們懂的多，我有一天登陸了蘋果的iPhone軟體商店。哪裡有藝術方面的軟體？我找到生產力相關的軟體，也找到了運動類軟體。不知何故，為iPhone製作藝術類軟體的想法，我和我的朋友們現在正在進行的，並沒有在我們對電腦用途的理解中反映出來。 從兩方面來講，我都覺得缺少了一些理解如果藝術家能夠使用他或她所處的時代的材料進行創作意味著甚麼。我覺得藝術家有責任要充分地探索我們所擁有的新工具地表達潛力。以我自己為例，我是一名藝術家，我非常熱衷於擴展人類行為的語匯並通過交互來鼓動人們這樣做。我希望人們可以將自己看待成表演者，通過交互性的體驗，成為富有創意的表演者。 我的很多工作也是希望避免這種情況。這是一幅拍攝了我一個學生的桌面的照片。當我講到桌面，我並不僅僅是指實際上被他的滑鼠磨損的桌面。如果你仔細地觀察，你甚至會隱約發現在左上角蘋果系統菜單的跡象。在那裡虛擬世界真正的穿越到了現實世界中。所以就像喬伊芒福德曾經說的，”滑鼠可能是最細小的吸管你能使用它來吸取所有人類的表達。“ 笑聲 我真正像嘗試要做的就是能夠讓人們擁有更豐富多樣的交互體驗。如何使用我們全身而不是鼠標作為一種探索審美體驗的方式。並不一定是實用主義的。我編寫軟體，這是我創作的方式。我許多的試驗都從某方面再現了鏡子的概念。因為在某種意義上，鏡子是人們最初能觀察到他們自己的動作，發現自己作為一個表演者的潛能的方式。通過提問“誰是鏡子里的那個人？喔，這原來就是我。” 所以就拿去年的一個項目來做一個例子。它被稱作縫隙碎片處理器。它能讓人們探索自己在做他們的日常動作的時候所創造的負形狀。當人們用他們的雙手或者頭部，或者互相一起創造形狀的時候。這些形狀會產生聲音並憑空出現然後墜落。這基本上是將未被看見的空間，或是未被察覺的空間，轉變成一種真實的物體,人們可以欣賞並對其發揮創造力。所以再一次，人們這樣發現他們的創造性動作。並且他們自身的人格也因此以非常特別的方式顯現。 除了使用全身來輸入之外，我利用了一段時間探索了一些別的東西，就是對聲音的利用。有聲化是對我們來說非常具有表達力的系統。歌唱是我們所使用來互相聽見並理解的最古老的方法之一。我發現這個由格式塔心理學之父沃爾夫岡 科勒在1927年所進行的奇妙的研究。他發送如下2個圖形給像你們一樣的觀眾。他說其中一個叫做Maluma。还有一个叫做Teketa,哪個是哪個呢？有誰想嘗試着猜一下嗎？Maluma是上邊的這個，對是這樣。就像他說的，大部份人都能毫不猶豫地給出答案。 從中我們真正觀察到的是一個現象稱作音義聯覺，就好像你們大家都擁有的聯覺一樣。奧利佛薩科斯博士曾提到可能每一百萬人中只有一個人真正地擁有聯覺，他們可以聽見顏色或者品嘗到形狀之類。音義聯覺是我們所有人都能在某種程度上體驗的東西。這在不同的感知領域之間的映射，比如硬度，鋒利感，明度以及黑暗，以及我們能發出的音素。 70年來，通過一些研究認知心理學加已經弄清楚了這個現象。L，M，B更多地於這種形狀聯繫在一起，P，T和K可能更多地和這種形狀相關連。突然間我們就開始有了一在不同地曲率之間映射可以量化地來研究開發，一個在語音和曲率之間的相對的映射。 我突然想到，如果我们把这个过程回放会是怎么样的？由此这个称作Remark的項目就誕生了。我們與扎迦利還有電子藝術未來實驗室一起合作。這個交互裝置呈現了一個語音會產生可見的陰影的幻象。這個理念是你走進一種魔幻的光線中，當你這樣做的時候，你會看見自己說出的言語的影子。它們從你頭部飛出。如果電腦的語音識別系統可以聽懂你在說甚麼，它就會講你所說的拼寫出來。如果電腦聽不懂，它就會按照音義聯覺的原理產生一個與你發出的聲音相匹配的形狀。請播放一段關於這個項目的影片。 （掌聲） 謝謝。在這一個作品中，我和非常厲害的抽象聲樂家，雅普 布朗克。他是一個世界級的表演“Ursonate”的專家，一首由柯特 舒維特在1920年所創作的長達半小時的無意義詩歌。它是一段長達半小時的高度模式化的無意義的語音。一般人幾乎是不可能表演出來的。不過雅普是少數能表演它的世界級專家之一。在這個項目中我們開發了一種智能實時字幕。這裡是我們的現場字幕，它們是由一台瞭解“Ursonate”的文字的電腦所生成的。幸好雅普也很瞭解這些文字。電腦在雅普發出聲音的同時呈現字幕。所有你將看到的字幕都是由電腦實時生成的，將他的語音可視化。這裡你能看見在他身後佈置着一個有字幕的螢幕。好了。（掌聲）如果你有興趣的話可以在網路上找到整部的影片。 在現場表演的時候我碰到兩種截然不同的反映。因為有些人理解實時字幕這個概念是有些自相矛盾的。因為通常字幕都是有人在影片完成後來製作字幕。還有一群人感覺：“這有甚麼大不了的？”我每天都在電視上看到字幕。“他們想像不到有人在幕後編寫所有的字幕的情景。 除了全身和語音之外，還有一件最近我非常有興趣的事情就是對眼睛的使用，或者說注視，從人們互相之間如何聯繫來看。許多深刻的信息是用眼睛而不是語言來傳達的。這也是技術上最有意思的挑戰之一，它是當下計算機科學領域非常熱門的問題。能夠有一台攝像機，可以在比較遠的距離就理解這些小小的眼睛在往哪裡看，來揭露你對甚麼感興趣，你的注意力集中在哪裡？在眼睛這個部位進行着許多情感上的交流。 所以我啓動了一些不同的項目，來瞭解人們如何通過眼睛與機器產生聯繫。大概地問這樣一些問題，如果一件藝術作品知道我們在看着它？它會作何反映，在某種程度上，來知曉或者是顛覆我們在觀賞它的事實?如果它也看着我們會是怎樣？這些是下面這個項目里所發生的事情， 第一個我要展示給你們看的作品，被稱作眼睛編碼，它是一個交互軟體如果我們讀一下這個圈小字，先前一個觀眾觀察時留下的痕跡看着先前一個觀眾觀察時留下的痕跡這個概念就是一個完全由被不同的觀眾觀察的過程所組成的圖像。讓我切換過來，做一個現場演示。來試試看行不行。 哦，嗯，這里有很多影像。這只是一個用來顯示它正常工作的測試屏幕。我會把這個窗口隱藏掉，這樣你們就可以看見每當我眨眼的時候它就紀錄我的眼睛的圖像。哈嘍？ 我可以...哈嘍...好了。不管我在甚麼位置，實際發生的是這個眼睛追蹤系統都在嘗試定位我的眼睛。如果距離的比較遠我的眼睛就會變得模糊。屏幕上就會出現這些模糊的小點。它們只是以非常抽象的方式再現了我的眼睛。如果我靠的非常近並且盯這筆記型電腦上的攝像機看，你們就會看到清晰的眼睛。 你們可以把這看成是在用眼睛打字一樣。你所輸入的是當你在看別人的眼睛的時候對你眼睛的紀錄。所以每個人都在看着之前的人。這系統被應用在更大的裝置里，有數千雙眼睛供人觀看，你看着那個看著之前看著之前的人的人。我再加上幾雙眼睛，眨眼，眨眼。還有你能觀察到它又一次尋找到我的眼睛並且盡可能估計我甚麼時候眨眼。好了，就這樣吧。這就是一種循環觀察系統。（掌聲）謝謝 最後幾個我相展示的是是在新的機器人領域，對我來說比較新。叫做光電隔離體。我先放一個影片來展示早先的版本。大約一分鐘左右，好。在這個項目中，這個光電隔離器在對眨眼的觀看者眨眼。在你眨眼後的一秒種它也眨眼。這是設備的用意是用最簡單的材料來表現注視的現象。僅僅是一隻眼睛看着你，去掉了其他任何臉部的東西。只是單獨來思考注視就好像是，一種元素。於此同時，它嘗試參與到我們所熟悉的社會心理注視行為中。如果你盯着它看太久它就看別的地方因為它會害羞或諸如此類。 好了，最後一個要給你們看的項目是這個叫做長鼻子。（笑聲）它是一個八尺高的長鼻子，加上一個曲棍球做眼睛。（笑聲）在它裡面有一台我借來的800磅重的機械臂，（笑聲）從一個朋友那裡。（笑聲）有好的朋友確是會幫到很多。我在卡耐基梅隆大學。我們有非常棒的機器人研究所。我想展示給你們這個叫長鼻子的東西這個項目背後的概念是做一個好像是一直很驚訝地看着你地機器人。（笑聲）這個概念基本上來說如果他一直表現出“啊？...嗯？“這就是為甚麼它的綽號是反覆打量，打量兩次。它好像總是在反覆地打量着：”甚麼啊？“這概念大概就是它會看着你然後讓你覺得好像，”怎麼了？是我的鞋子有問題嗎？““還是頭髮上黏了甚麼東西？”開始了，好。 看看他...電腦迷們，這就是幕後的樣子。它有一套電腦視覺系統，它盡可能地盯着活動幅度最大的人。這些是它的目標。上邊是一個骨架系統，就是它實際所要做的。這件作品嘗試為一種新的生物創造一種新穎的肢體語言。當然好萊塢經常做這種事情。但同時也能使用這肢體語言與看着它的人來交流一些事情。這語言所表達的是它看到你很驚訝，同時也很有興趣來觀察你。 笑聲 掌聲 非常感謝，以上就是我今天所有要講的內容。我很高興能在這裡演講，謝謝大家！ 掌聲
