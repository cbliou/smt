Talk	en	zh-tw
torsten_reil_studies_biology_to_make_animation	"I'm going to talk about a technology that we're developing at Oxford now, that we think is going to change the way that computer games and Hollywood movies are being made. That technology is simulating humans. It's simulated humans with a simulated body and a simulated nervous system to control that body. Now, before I talk more about that technology, let's have a quick look at what human characters look like at the moment in computer games. This is a clip from a game called ""Grand Theft Auto 3."" We already saw that briefly yesterday. And what you can see is — it is actually a very good game. It's one of the most successful games of all time. But what you'll see is that all the animations in this game are very repetitive. They pretty much look the same. I've made him run into a wall here, over and over again. And you can see he looks always the same. The reason for that is that these characters are actually not real characters. They are a graphical visualization of a character. To produce these animations, an animator at a studio has to anticipate what's going to happen in the actual game, and then has to animate that particular sequence. So, he or she sits down, animates it, and tries to anticipate what's going to happen, and then these particular animations are just played back at appropriate times in the computer game. Now, the result of that is that you can't have real interactivity. All you have is animations that are played back at more or less the appropriate times. It also means that games aren't really going to be as surprising as they could be, because you only get out of it, at least in terms of the character, what you actually put into it. There's no real emergence there. And thirdly, as I said, most of the animations are very repetitive because of that. Now, the only way to get around that is to actually simulate the human body and to simulate that bit of the nervous system of the brain that controls that body. And maybe, if I could have you for a quick demonstration to show what the difference is — because, I mean, it's very, very trivial. If I push Chris a bit, like this, for example, he'll react to it. If I push him from a different angle, he'll react to it differently, and that's because he has a physical body, and because he has the motor skills to control that body. It's a very trivial thing. It's not something you get in computer games at the moment, at all. Thank you very much. Chris Anderson: That's it? Torsten Reil: That's it, yes. So, that's what we're trying to simulate — not Chris specifically, I should say, but humans in general. Now, we started working on this a while ago at Oxford University, and we tried to start very simply. What we tried to do was teach a stick figure how to walk. That stick figure is physically stimulated. You can see it here on the screen. So, it's subject to gravity, has joints, etc. If you just run the simulation, it will just collapse, like this. The tricky bit is now to put an AI controller in it that actually makes it work. And for that, we use the neural network, which we based on that part of the nervous system that we have in our spine that controls walking in humans. It's called the central pattern generator. So, we simulated that as well, and then the really tricky bit is to teach that network how to walk. For that we used artificial evolution — genetic algorithms. We heard about those already yesterday, and I suppose that most of you are familiar with that already. But, just briefly, the concept is that you create a large number of different individuals — neural networks, in this case — all of which are random at the beginning. You hook these up — in this case, to the virtual muscles of that two-legged creature here — and hope that it does something interesting. At the beginning, they're all going to be very boring. Most of them won't move at all, but some of them might make a tiny step. Those are then selected by the algorithm, reproduced with mutation and recombinations to introduce sex as well. And you repeat that process over and over again, until you have something that walks — in this case, in a straight line, like this. So that was the idea behind this. When we started this, I set up the simulation one evening. It took about three to four hours to run the simulation. I got up the next morning, went to the computer and looked at the results, and was hoping for something that walked in a straight line, like I've just demonstrated, and this is what I got instead. (Laughter) So, it was back to the drawing board for us. We did get it to work eventually, after tweaking a bit here and there. And this is an example of a successful evolutionary run. So, what you'll see in a moment is a very simple biped that's learning how to walk using artificial evolution. At the beginning, it can't walk at all, but it will get better and better over time. So, this is the one that can't walk at all. (Laughter) Now, after five generations of applying evolutionary process, the genetic algorithm is getting a tiny bit better. (Laughter) Generation 10 and it'll take a few steps more — still not quite there. But now, after generation 20, it actually walks in a straight line without falling over. That was the real breakthrough for us. It was, academically, quite a challenging project, and once we had reached that stage, we were quite confident that we could try and do other things as well with this approach — actually simulating the body and simulating that part of the nervous system that controls it. Now, at this stage, it also became clear that this could be very exciting for things like computer games or online worlds. What you see here is the character standing there, and there's an obstacle that we put in its way. And what you see is, it's going to fall over the obstacle. Now, the interesting bit is, if I move the obstacle a tiny bit to the right, which is what I'm doing now, here, it will fall over it in a completely different way. And again, if you move the obstacle a tiny bit, it'll again fall differently. (Laughter) Now, what you see, by the way, at the top there, are some of the neural activations being fed into the virtual muscles. Okay. That's the video. Thanks. Now, this might look kind of trivial, but it's actually very important because this is not something you get at the moment in any interactive or any virtual worlds. Now, at this stage, we decided to start a company and move this further, because obviously this was just a very simple, blocky biped. What we really wanted was a full human body. So we started the company. We hired a team of physicists, software engineers and biologists to work on this, and the first thing we had to work on was to create the human body, basically. It's got to be relatively fast, so you can run it on a normal machine, but it's got to be accurate enough, so it looks good enough, basically. So we put quite a bit of biomechanical knowledge into this thing, and tried to make it as realistic as possible. What you see here on the screen right now is a very simple visualization of that body. I should add that it's very simple to add things like hair, clothes, etc., but what we've done here is use a very simple visualization, so you can concentrate on the movement. Now, what I'm going to do right now, in a moment, is just push this character a tiny bit and we'll see what happens. Nothing really interesting, basically. It falls over, but it falls over like a rag doll, basically. The reason for that is that there's no intelligence in it. It becomes interesting when you put artificial intelligence into it. So, this character now has motor skills in the upper body — nothing in the legs yet, in this particular one. But what it will do — I'm going to push it again. It will realize autonomously that it's being pushed. It's going to stick out its hands. It's going to turn around into the fall, and try and catch the fall. So that's what you see here. Now, it gets really interesting if you then add the AI for the lower part of the body as well. So here, we've got the same character. I'm going to push it a bit harder now, harder than I just pushed Chris. But what you'll see is — it's going to receive a push now from the left. What you see is it takes steps backwards, it tries to counter-balance, it tries to look at the place where it thinks it's going to land. I'll show you this again. And then, finally hits the floor. Now, this becomes really exciting when you push that character in different directions, again, just as I've done. That's something that you cannot do right now. At the moment, you only have empty computer graphics in games. What this is now is a real simulation. That's what I want to show you now. So, here's the same character with the same behavior I've just shown you, but now I'm just going to push it from different directions. First, starting with a push from the right. This is all slow motion, by the way, so we can see what's going on. Now, the angle will have changed a tiny bit, so you can see that the reaction is different. Again, a push, now this time from the front. And you see it falls differently. And now from the left — and it falls differently. That was really exciting for us to see that. That was the first time we've seen that. This is the first time the public sees this as well, because we have been in stealth mode. I haven't shown this to anybody yet. Now, just a fun thing: what happens if you put that character — this is now a wooden version of it, but it's got the same AI in it — but if you put that character on a slippery surface, like ice. We just did that for a laugh, just to see what happens. (Laughter) And this is what happens. (Laughter) (Applause) It's nothing we had to do about this. We just took this character that I just talked about, put it on a slippery surface, and this is what you get out of it. And that's a really fascinating thing about this approach. Now, when we went to film studios and games developers and showed them that technology, we got a very good response. And what they said was, the first thing they need immediately is virtual stuntmen. Because stunts are obviously very dangerous, they're very expensive, and there are a lot of stunt scenes that you cannot do, obviously, because you can't really allow the stuntman to be seriously hurt. So, they wanted to have a digital version of a stuntman and that's what we've been working on for the past few months. And that's our first product that we're going to release in a couple of weeks. So, here are just a few very simple scenes of the guy just being kicked. That's what people want. That's what we're giving them. (Laughter) You can see, it's always reacting. This is not a dead body. This is a body who basically, in this particular case, feels the force and tries to protect its head. Only, I think it's quite a big blow again. You feel kind of sorry for that thing, and we've seen it so many times now that we don't really care any more. (Laughter) There are much worse videos than this, by the way, which I have taken out, but ... Now, here's another one. What people wanted as a behavior was to have an explosion, a strong force applied to the character, and have the character react to it in midair. So that you don't have a character that looks limp, but actually a character that you can use in an action film straight away, that looks kind of alive in midair as well. So this character is going to be hit by a force, it's going to realize it's in the air, and it's going to try and, well, stick out its arm in the direction where it's landing. That's one angle; here's another angle. We now think that the realism we're achieving with this is good enough to be used in films. And let's just have a look at a slightly different visualization. This is something I just got last night from an animation studio in London, who are using our software and experimenting with it right now. So this is exactly the same behavior that you saw, but in a slightly better rendered version. So if you look at the character carefully, you see there are lots of body movements going on, none of which you have to animate like in the old days. Animators had to actually animate them. This is all happening automatically in the simulation. This is a slightly different angle, and again a slow motion version of this. This is incredibly quick. This is happening in real time. You can run this simulation in real time, in front of your eyes, change it, if you want to, and you get the animation straight out of it. At the moment, doing something like this by hand would take you probably a couple of days. This is another behavior they requested. I'm not quite sure why, but we've done it anyway. It's a very simple behavior that shows you the power of this approach. In this case, the character's hands are fixed to a particular point in space, and all we've told the character to do is to struggle. And it looks organic. It looks realistic. You feel kind of sorry for the guy. It's even worse — and that is another video I just got last night — if you render that a bit more realistically. Now, I'm showing this to you just to show you how organic it actually can feel, how realistic it can look. And this is all a physical simulation of the body, using AI to drive virtual muscles in that body. Now, one thing which we did for a laugh was to create a slightly more complex stunt scene, and one of the most famous stunts is the one where James Bond jumps off a dam in Switzerland and then is caught by a bungee. Got a very short clip here. Yes, you can just about see it here. In this case, they were using a real stunt man. It was a very dangerous stunt. It was just voted, I think in the Sunday Times, as one of the most impressive stunts. Now, we've just tried and — looked at our character and asked ourselves, ""Can we do that ourselves as well?"" Can we use the physical simulation of the character, use artificial intelligence, put that artificial intelligence into the character, drive virtual muscles, simulate the way he jumps off the dam, and then skydive afterwards, and have him caught by a bungee afterwards? We did that. It took about altogether just two hours, pretty much, to create the simulation. And that's what it looks like, here. Now, this could do with a bit more work. It's still very early stages, and we pretty much just did this for a laugh, just to see what we'd get out of it. But what we found over the past few months is that this approach — that we're pretty much standard upon — is incredibly powerful. We are ourselves surprised what you actually get out of the simulations. There's very often very surprising behavior that you didn't predict before. There's so many things we can do with this right now. The first thing, as I said, is going to be virtual stuntmen. Several studios are using this software now to produce virtual stuntmen, and they're going to hit the screen quite soon, actually, for some major productions. The second thing is video games. With this technology, video games will look different and they will feel very different. For the first time, you'll have actors that really feel very interactive, that have real bodies that really react. I think that's going to be incredibly exciting. Probably starting with sports games, which are going to become much more interactive. But I particularly am really excited about using this technology in online worlds, like there, for example, that Tom Melcher has shown us. The degree of interactivity you're going to get is totally different, I think, from what you're getting right now. A third thing we are looking at and very interested in is simulation. We've been approached by several simulation companies, but one project we're particularly excited about, which we're starting next month, is to use our technology — and in particular, the walking technology — to help aid surgeons who work on children with cerebral palsy, to predict the outcome of operations on these children. As you probably know, it's very difficult to predict what the outcome of an operation is if you try and correct the gait. The classic quote is, I think, it's unpredictable at best, is what people think right now, is the outcome. Now, what we want to do with our software is allow our surgeons to have a tool. We're going to simulate the gait of a particular child and the surgeon can then work on that simulation and try out different ways to improve that gait, before he actually commits to an actual surgery. That's one project we're particularly excited about, and that's going to start next month. Just finally, this is only just the beginning. We can only do several behaviors right now. The AI isn't good enough to simulate a full human body. The body yes, but not all the motor skills that we have. And, I think, we're only there if we can have something like ballet dancing. Right now, we don't have that but I'm very sure that we will be able to do that at some stage. We do have one unintentional dancer actually, the last thing I'm going to show you. This was an AI contour that was produced and evolved — half-evolved, I should say — to produce balance, basically. So, you kick the guy and the guy's supposed to counter-balance. That's what we thought was going to come out of this. But this is what emerged out of it, in the end. (Music) Bizarrely, this thing doesn't have a head. I'm not quite sure why. So, this was not something we actually put in there. He just started to create that dance himself. He's actually a better dancer than I am, I have to say. And what you see after a while — I think he even goes into a climax right at the end. And I think — there you go. (Laughter) So, that all happened automatically. We didn't put that in there. That's just the simulation creating this itself, basically. So it's just — (Applause) Thanks. Not quite John Travolta yet, but we're working on that as well, so thanks very much for your time. Thanks. (Applause) CA: Incredible. That was really incredible. TR: Thanks."	"我將向各位介紹一項目前我們正在牛津研究發展的科技我們認為它將改變電玩和好萊塢電影的製作方式。這一項科技就是模擬人類，它以模擬的人類身體及用以控制那身體的模擬神經系統來模擬人類。在往下介紹這項科技之前，先讓我們看一下現在電玩裡頭人物運動的樣子。這是俠盜獵車手 3 的片段，昨天我們有稍微看了一些。就如你看到的, 它是一個非常棒的遊戲。所有最成功遊戲的其中一個。不過你可以看出遊戲中動畫人物的動作不斷地重覆，它們看起來都一樣。這個角色撞上牆之後，一直重覆一樣的動作。主要的原因是這些角色並不是真實的人物。他們是一個角色的圖像視覺呈現結果。 遊戲公司裡的動畫師必需先預測遊戲中的情節，並針對特定橋段繪製，才能製作出這些動畫。因此動畫師只是坐在那，埋頭繪製並企圖預測將會發生的事，而這些動畫成果會在遊戲的適當時候播放。但這麼做的結果是玩家無法擁有真實的互動。你只能不斷看到相同的動作這也表示遊戲中不會發生驚喜角色只能做預設的內容你當時放了什麼在裡面因為裡面沒有真實的結合 所以大部份的動作只能反覆避免的唯一方法就是實際地模擬人體模擬控制的神經系統可以請你上來做個簡單示範嗎看看哪裡不一樣這是一個微不足道的現象如果我推Chris一下，他會產生反應如果我從另一個角度推，他會有不同反應這是因為他有一個真實的身體和控制身體的能力這是一個微不足道的這是一個與電玩無關的小事謝謝你就這樣。就這樣嗎？ 是的卻正是我們要模擬的不是模擬Chris 而是模擬人體我們在牛津大學開始一段時間了一切從最簡單的事開始也就是教這個火柴人走路它已經完全地模擬人體會受到地心引力影響也有關節一旦啟動模擬它就會像這樣跌倒，就像這樣困難的部份是加入控制也就是人工智慧讓它動起來我們用在神經系統上的是脊椎上控制走路的神經網絡叫中樞模式產生器我們同樣模擬了這套網絡，但真正困難的是訓練網絡走路為此我們使用人工演化以取得自然的演算法 我們昨天已聽過這個主題演講我想大部份的人都蠻熟悉了但是，簡單地來說， 它的概念就是要產生大量不同的個體以這個例子來說起初所有個體都是亂數產生再將它們連結火柴人的虛擬肌肉火柴人的虛擬肌肉希望它可以做出有趣的事一開始挺無聊的大部份火柴人不會動但有些會走小步路這些演算法就會被選用加上突變和重組之後進行繁殖接下來不斷重覆這些步驟直到它能像這樣走直線這就是它背後的概念現在我們開始示範我在前一天晚上設定完畢花幾個小時執行模擬運算隔天早晨回來檢查成果我希望能看到它走直線就像我所證明的一樣但這卻是我得到的 (笑聲) 結果它變成關節所以我們做了些許調整最後還是成功了人工演化成功地運行待會將看到一個簡易的雙足生物藉由人工演化學習如何走路一開始它完全不會走路但是有愈來愈進步這個完全不能走路 (笑聲) 然後我們改變演算法在演化的第五次演算它有點好轉 (笑聲) 第十次演算能夠往前走幾步但還是沒有成功到了第二十次演算它完全成功，不但能走直線也不會跌倒對我們來說是一個突破這是一個很有挑戰性的計劃一旦發展到下個階段，我們相當有信心我們相信能以同樣的方法，讓它做其他事模擬人體及其控制神經系統到了這個階段對電玩和線上遊戲無疑地是一個興奮的消息你們可以看到一個角色站在那兒在那裡我們放了一個障礙物它會在絆到障礙物時跌倒現在有趣的是，如果現在我把障礙物挪右邊一點就跟我現在做的一樣它會以完全不同的方式跌倒再把障礙物挪右邊一點，它還是會以不同的方式跌倒 (笑聲) 對了，畫面上方顯示的是虛擬肌肉系統的神經運動以上就是我的示範影片。謝謝這或許微不足道但卻很重要現在的虛擬的世界並沒有達到互動發展到這個階段，我們決定要開一家公司繼續發展下去這不過是一個簡單的雙足生物我們的目標要模擬人體全身所以我們開了一家公司聘了物理學家軟體工程師生物學家第一個任務就是建造一個人體它得輕巧以便在一般機器上執行外觀也要準確好看 我們也加了不少生化科技進去盡力把它做得精巧你們現在看到的是簡易的外型可以輕易地加上頭髮衣物等我應該要加一些簡單的東西，如頭髮、衣服等等我們現在只是用簡單的方式所以你們可以專注在它的動作我們現在要做的是輕推它一下，看看會怎麼樣沒什麼有趣的事發生它像個布娃娃一樣跌倒因為還沒有為它加入人工智慧如果加上會產生有趣的結果現在它的上半身可以做很多動作下半身則未放入任何東西我現在再次推倒它它知道自己被推了它會伸出手轉身試圖不摔倒就像這樣 把人工智慧加到下半身會更有趣現在使用同一個角色當我更用力地推它就像我推Chris一樣它會試圖抵抗左邊來的力量你們可以看到它先退了一步試著保持平衡眼睛還會看向我再播一次最後它才跌倒這是非常有趣的如果你把它推向不同的方向，就像我剛剛做的一樣這是現在的遊戲做不到的這裡頭可沒有預設動作這才是真實的模擬，也就是我現在要展示的 再次現在使用同一個角色我把它推到不同方向第一次是往右推這些是慢動作影片讓你們能看清楚現在角度變了它倒下的方向也變了下一次我從前面推看它跌倒的方式不一樣從左邊推它跌倒的方式也不同我們對此結果感到十分高興這是本計畫結果第一次公開我們還沒有讓任何人看過因為我們是祕密進行的我還沒有把這個給任何人看過現在來點有趣的它會發生的事是這是木頭人的版本有相同的智慧如果把它放到平滑的冰面我們這麼試只想看點滑稽的東西 (笑聲) 而結果是這樣 (笑聲) (掌聲) 我們不需要在它身上做什麼只是把它放在平滑表面就能得到這個結果這就是我們的模擬方法神奇之處當這項技術介紹給電玩公司時介紹給電玩公司時，我們得到很好的回應他們的第一個反應是用在特技演員因為特技很危險很貴還有更多的特技鏡頭是做不到的因為不能讓特技演員受傷所以他們想要數位的特技演員這正是我們過去幾個月來的工作我們很榮幸能在幾個星期內發表這裡有幾個簡單的鏡頭，一個傢伙被踢那是他們要的，這是我們給的 (笑聲) 你們可以看到不停地表演一點都不僵硬，在這個鏡頭他試著平衡也會保護自己的頭另一個鏡頭又是一記重擊你們或許為木頭人感到難過但我們已經看了太多遍了現在可是一點感覺也沒有 (笑聲) 後面還有很多更痛的鏡頭我得拿掉另一個人們想要的是在爆炸之後角色在空中如何反應強大的力量它看起來不可以軟綿綿的它得是可以用在電影裡的角色能飛在空中看起來有活力現在這個角色會被用力一擊它會發現自己在空中而試著抓住些什麼它被推出去然後著地這是一個拍攝角度，還有另一個角度我們認為達到這個程度就已經可以用在電影上 現在我們來看點不一樣的的圖像這是我昨天拿到的影片一家倫敦動畫公司正在試用我們軟體現在正在試驗中它會做一樣的動作算圖看起來比較漂亮仔細看身體每一部份都有反應動作不再像以往得親自調動畫現在動作都能自動模擬出來從不同角度看用慢動作再看一次這非常快即時運算馬上就能看到你可以執行模擬馬上調整得到結果如果手動調動畫大概會花去好幾天 他們還要求另一個動作不知道為什麼但我們還要做了它是很簡單的動作這個角色的手被固定在一個地方可以看出角色在掙扎看起來不生硬還令人有點不舒服如果你們為他感到同情接下來的畫面更糟，這是我昨晚拿到的片段算圖之後看來更真實了 只是想讓你們看看它多麼地寫實這全是真實模擬的結果用人工智慧驅動虛擬的肌肉最後要介紹的是較複雜的特技一個最有名的特技鏡頭是007的腳被彈力繩綁住從瑞士一個水壩一躍而下這裡有一個簡短的片段 就如同你看到的一樣當時用真人演出十分危險它被週日時報票選為最令人印象深刻的特技我看到這一幕時自問能不能做到實際模擬身體利用人工智慧並且放入人工智慧模擬它綁著彈跳繩高空跳下水壩捕捉它跳下的畫面總共花了二個小時產生模擬接近真實創造了模擬結果看起來是這樣的它只是初步階段還需要一些調整我們會這樣握只是要讓大家笑一笑看看到底可以得到什麼東西過去幾個月來我們發現這個方法非常強大對於模擬的結果令人感到吃驚有些結果是我們沒有預期的 現在它可以做很多事第一虛擬特技演員有7家公司已經開始使用這套軟體做數位替身很快會在大銀幕上看到為知名的產品做模擬第二電玩用這項科技會使電玩看起來大不同將是第一次角色具有互動感它們有真實的身體對周遭做出反應我覺得這真的令人興奮或許從運動遊戲開始將變得更有互動感特別希望看到這科技用在線上世界像是剛才看到的""There""互動程度將會和現在的遊戲大不同 第三是模擬我們接到很多模擬公司的詢問其中一項計畫特別令我感到興趣，這個計畫下個月就要開始它將使用我們行走的技術幫助兒童腦性麻痺開刀的外科醫生預測孩子手術的結果你們知道矯正腦性麻痺患者的行走的手術結果是很難預測的 我覺得這是現下最難的是人們的想法我們會提供醫師另一套工具模擬小朋友的步行手術可以依照模擬的結果去進行所以他可以在手術之前進行模擬從不同的矯正結果中找出最好的，再進行手術這就是我們特別興奮的計劃它下個月就會開始最後，這只是一開始的結果我們現在只能做出幾個行為模式現在它的人工智慧還無法模擬全身可以做出軀體但是還沒能做出所有動作不過很快它就能做出芭蕾的動作雖然現在我們沒有芭蕾舞動作但是我知道總有一天我們可以做出來的 事實上我們不小心模擬出芭蕾舞蹈這裡有一個偶然的成果是我最後想展示給大家看的這個角色被創造時有加入人工智慧來創造平衡當你踢它的時候它會去取得平衡這是我們想像會是這樣的結果但是最後卻得到這樣的結果 (音樂) 我也不知道為什麼沒有把頭放進去這不是我們設定的這些動作都是自己發生的他比我還會跳舞, 我必須承認如同你所看到的最後的動作是最精彩的部份然後, 你看 (笑聲) 這不是我們有意設計的動作它完全是模擬的成果所以這是— (掌聲) 謝謝還不能像John Travolta那麼厲害, 但是我們會繼續努力謝謝大家的時間謝謝 (掌聲) Chris Anderson: 真的是很令人驚嘆 Torsten Reil：謝謝"
