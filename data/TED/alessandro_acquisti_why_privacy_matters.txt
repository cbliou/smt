Talk	en	zh-tw
alessandro_acquisti_why_privacy_matters	"I would like to tell you a story connecting the notorious privacy incident involving Adam and Eve, and the remarkable shift in the boundaries between public and private which has occurred in the past 10 years. You know the incident. Adam and Eve one day in the Garden of Eden realize they are naked. They freak out. And the rest is history. Nowadays, Adam and Eve would probably act differently. [@Adam Last nite was a blast! loved dat apple LOL] [@Eve yep.. babe, know what happened to my pants tho?] We do reveal so much more information about ourselves online than ever before, and so much information about us is being collected by organizations. Now there is much to gain and benefit from this massive analysis of personal information, or big data, but there are also complex tradeoffs that come from giving away our privacy. And my story is about these tradeoffs. We start with an observation which, in my mind, has become clearer and clearer in the past few years, that any personal information can become sensitive information. Back in the year 2000, about 100 billion photos were shot worldwide, but only a minuscule proportion of them were actually uploaded online. In 2010, only on Facebook, in a single month, 2.5 billion photos were uploaded, most of them identified. In the same span of time, computers' ability to recognize people in photos improved by three orders of magnitude. What happens when you combine these technologies together: increasing availability of facial data; improving facial recognizing ability by computers; but also cloud computing, which gives anyone in this theater the kind of computational power which a few years ago was only the domain of three-letter agencies; and ubiquitous computing, which allows my phone, which is not a supercomputer, to connect to the Internet and do there hundreds of thousands of face metrics in a few seconds? Well, we conjecture that the result of this combination of technologies will be a radical change in our very notions of privacy and anonymity. To test that, we did an experiment on Carnegie Mellon University campus. We asked students who were walking by to participate in a study, and we took a shot with a webcam, and we asked them to fill out a survey on a laptop. While they were filling out the survey, we uploaded their shot to a cloud-computing cluster, and we started using a facial recognizer to match that shot to a database of some hundreds of thousands of images which we had downloaded from Facebook profiles. By the time the subject reached the last page on the survey, the page had been dynamically updated with the 10 best matching photos which the recognizer had found, and we asked the subjects to indicate whether he or she found themselves in the photo. Do you see the subject? Well, the computer did, and in fact did so for one out of three subjects. So essentially, we can start from an anonymous face, offline or online, and we can use facial recognition to give a name to that anonymous face thanks to social media data. But a few years back, we did something else. We started from social media data, we combined it statistically with data from U.S. government social security, and we ended up predicting social security numbers, which in the United States are extremely sensitive information. Do you see where I'm going with this? So if you combine the two studies together, then the question becomes, can you start from a face and, using facial recognition, find a name and publicly available information about that name and that person, and from that publicly available information infer non-publicly available information, much more sensitive ones which you link back to the face? And the answer is, yes, we can, and we did. Of course, the accuracy keeps getting worse. [27% of subjects' first 5 SSN digits identified (with 4 attempts)] But in fact, we even decided to develop an iPhone app which uses the phone's internal camera to take a shot of a subject and then upload it to a cloud and then do what I just described to you in real time: looking for a match, finding public information, trying to infer sensitive information, and then sending back to the phone so that it is overlaid on the face of the subject, an example of augmented reality, probably a creepy example of augmented reality. In fact, we didn't develop the app to make it available, just as a proof of concept. In fact, take these technologies and push them to their logical extreme. Imagine a future in which strangers around you will look at you through their Google Glasses or, one day, their contact lenses, and use seven or eight data points about you to infer anything else which may be known about you. What will this future without secrets look like? And should we care? We may like to believe that the future with so much wealth of data would be a future with no more biases, but in fact, having so much information doesn't mean that we will make decisions which are more objective. In another experiment, we presented to our subjects information about a potential job candidate. We included in this information some references to some funny, absolutely legal, but perhaps slightly embarrassing information that the subject had posted online. Now interestingly, among our subjects, some had posted comparable information, and some had not. Which group do you think was more likely to judge harshly our subject? Paradoxically, it was the group who had posted similar information, an example of moral dissonance. Now you may be thinking, this does not apply to me, because I have nothing to hide. But in fact, privacy is not about having something negative to hide. Imagine that you are the H.R. director of a certain organization, and you receive résumés, and you decide to find more information about the candidates. Therefore, you Google their names and in a certain universe, you find this information. Or in a parallel universe, you find this information. Do you think that you would be equally likely to call either candidate for an interview? If you think so, then you are not like the U.S. employers who are, in fact, part of our experiment, meaning we did exactly that. We created Facebook profiles, manipulating traits, then we started sending out résumés to companies in the U.S., and we detected, we monitored, whether they were searching for our candidates, and whether they were acting on the information they found on social media. And they were. Discrimination was happening through social media for equally skilled candidates. Now marketers like us to believe that all information about us will always be used in a manner which is in our favor. But think again. Why should that be always the case? In a movie which came out a few years ago, ""Minority Report,"" a famous scene had Tom Cruise walk in a mall and holographic personalized advertising would appear around him. Now, that movie is set in 2054, about 40 years from now, and as exciting as that technology looks, it already vastly underestimates the amount of information that organizations can gather about you, and how they can use it to influence you in a way that you will not even detect. So as an example, this is another experiment actually we are running, not yet completed. Imagine that an organization has access to your list of Facebook friends, and through some kind of algorithm they can detect the two friends that you like the most. And then they create, in real time, a facial composite of these two friends. Now studies prior to ours have shown that people don't recognize any longer even themselves in facial composites, but they react to those composites in a positive manner. So next time you are looking for a certain product, and there is an ad suggesting you to buy it, it will not be just a standard spokesperson. It will be one of your friends, and you will not even know that this is happening. Now the problem is that the current policy mechanisms we have to protect ourselves from the abuses of personal information are like bringing a knife to a gunfight. One of these mechanisms is transparency, telling people what you are going to do with their data. And in principle, that's a very good thing. It's necessary, but it is not sufficient. Transparency can be misdirected. You can tell people what you are going to do, and then you still nudge them to disclose arbitrary amounts of personal information. So in yet another experiment, this one with students, we asked them to provide information about their campus behavior, including pretty sensitive questions, such as this one. [Have you ever cheated in an exam?] Now to one group of subjects, we told them, ""Only other students will see your answers."" To another group of subjects, we told them, ""Students and faculty will see your answers."" Transparency. Notification. And sure enough, this worked, in the sense that the first group of subjects were much more likely to disclose than the second. It makes sense, right? But then we added the misdirection. We repeated the experiment with the same two groups, this time adding a delay between the time we told subjects how we would use their data and the time we actually started answering the questions. How long a delay do you think we had to add in order to nullify the inhibitory effect of knowing that faculty would see your answers? Ten minutes? Five minutes? One minute? How about 15 seconds? Fifteen seconds were sufficient to have the two groups disclose the same amount of information, as if the second group now no longer cares for faculty reading their answers. Now I have to admit that this talk so far may sound exceedingly gloomy, but that is not my point. In fact, I want to share with you the fact that there are alternatives. The way we are doing things now is not the only way they can done, and certainly not the best way they can be done. When someone tells you, ""People don't care about privacy,"" consider whether the game has been designed and rigged so that they cannot care about privacy, and coming to the realization that these manipulations occur is already halfway through the process of being able to protect yourself. When someone tells you that privacy is incompatible with the benefits of big data, consider that in the last 20 years, researchers have created technologies to allow virtually any electronic transactions to take place in a more privacy-preserving manner. We can browse the Internet anonymously. We can send emails that can only be read by the intended recipient, not even the NSA. We can have even privacy-preserving data mining. In other words, we can have the benefits of big data while protecting privacy. Of course, these technologies imply a shifting of cost and revenues between data holders and data subjects, which is why, perhaps, you don't hear more about them. Which brings me back to the Garden of Eden. There is a second privacy interpretation of the story of the Garden of Eden which doesn't have to do with the issue of Adam and Eve feeling naked and feeling ashamed. You can find echoes of this interpretation in John Milton's ""Paradise Lost."" In the garden, Adam and Eve are materially content. They're happy. They are satisfied. However, they also lack knowledge and self-awareness. The moment they eat the aptly named fruit of knowledge, that's when they discover themselves. They become aware. They achieve autonomy. The price to pay, however, is leaving the garden. So privacy, in a way, is both the means and the price to pay for freedom. Again, marketers tell us that big data and social media are not just a paradise of profit for them, but a Garden of Eden for the rest of us. We get free content. We get to play Angry Birds. We get targeted apps. But in fact, in a few years, organizations will know so much about us, they will be able to infer our desires before we even form them, and perhaps buy products on our behalf before we even know we need them. Now there was one English author who anticipated this kind of future where we would trade away our autonomy and freedom for comfort. Even more so than George Orwell, the author is, of course, Aldous Huxley. In ""Brave New World,"" he imagines a society where technologies that we created originally for freedom end up coercing us. However, in the book, he also offers us a way out of that society, similar to the path that Adam and Eve had to follow to leave the garden. In the words of the Savage, regaining autonomy and freedom is possible, although the price to pay is steep. So I do believe that one of the defining fights of our times will be the fight for the control over personal information, the fight over whether big data will become a force for freedom, rather than a force which will hiddenly manipulate us. Right now, many of us do not even know that the fight is going on, but it is, whether you like it or not. And at the risk of playing the serpent, I will tell you that the tools for the fight are here, the awareness of what is going on, and in your hands, just a few clicks away. Thank you. (Applause)"	我想告訴各位一則故事是著名的亞當和夏娃隱私事件是著名的亞當和夏娃隱私事件以及這10年來, 在公眾與個人之間分界的重大改變以及這10年來, 在公眾與個人之間分界的重大改變以及這10年來, 在公眾與個人之間分界的重大改變 你知道這起事件有一天亞當和夏娃在伊甸園發現他們都沒穿衣服他們嚇壞了其餘的部分你們都知道了 換做是現在的話, 亞當和夏娃可能會有不同的反應 (twitter)@亞當 昨晚的表現真是精采! 愛死那顆蘋果了 (twitter)@夏娃 寶貝你知道我的褲子怎麼了嗎? 在網路上, 我們都比從前透露出更多關於自己的訊息在網路上, 我們都比從前透露出更多關於自己的訊息而這些有關我們的訊息正被許多政府機構收集起來現在可以從分析個人資訊或是巨量資料中得到許多利益現在可以從分析個人資訊或是巨量資料中得到許多利益現在可以從分析個人資訊或是巨量資料中得到許多利益但是在捨棄隱私權的同時, 也伴隨著複雜的得失交換但是在捨棄隱私權的同時, 也伴隨著複雜的得失交換我要講的故事是有關這些得失交換 我們先從觀察開始在我看來, 過去幾年中, 這個情況已經變得越來越明確任何個人資訊都能變成敏感的訊息回朔到西元2000年的時候,全世界上所有人約拍出1000億張照片但是只有極少數的照片被上傳到網路上在2010年 光是一個月, Facebook用戶就上傳25億張照片在2010年時 光是一個月, Facebook用戶就上傳25億張照片而多數照片上的人都可以被辨識出來在這段時間裡辨認照片內人物的電腦運算能力也加快了1000倍如果將這些技術結合起來會發生什麼事?取得了更多的臉部資料改善了電腦臉部辨識的能力還有雲端運算這會給與現場任何一個人一種運算能力而這種運算能力在幾年前只專屬於那些政府機構這種普及的運算能力能讓我的普通手機連上網際網路在幾秒之內進行數十萬次的人臉辨識在幾秒之內進行數十萬次的人臉辨識我們推測這些技術結合的結果會顛覆我們對於隱私權與匿名性最初的想法 為了進行測試 我們做了一項實驗在卡內基美隆大學的校園裡我們找路過的學生來參與這項研究我們拿視訊攝影機拍照請他們用筆電填寫問卷調查他們在填寫問卷的時候上傳他們的照片到一個雲端運算群組使用一個臉部辨識系統將這組照片拿去與一個約有數十萬張圖像的資料庫比對這些圖像是我們從Facebook的個人簡介下載下來的等到受測者填寫到問卷最後一頁的時候畫面會更新成辨識器找出的10張最相符的照片畫面會更新成辨識器找出的10張最相符的照片畫面會更新成辨識器找出的10張最相符的照片我們要求受測者指出是否有在這些照片中找到他們自己 你有看到這名受測者嗎?是的, 電腦有找到三人之中就有一人被找到 基本上 我們能夠從一張不知名的臉開始,不管是離線或在線 我們都能利用臉部辨識讓一張不知名的臉找到它的名字這都是拜社群媒體資料庫所賜但是幾年前 我們又做其他的事情我們從社群媒體開始著手-我們將它與美國社會安全局的資料做結合我們將它與美國社會安全局的資料做結合我們可以猜出個人的社會安全號碼這在美國是一項極度敏感的個人資訊這在美國是一項極度敏感的個人資訊 你知道我在講什麼嗎?所以如果你們將兩種研究結果加在一起,那這個問題就會變成你能從一張臉開始利用臉部辨識技術 找到他的名字找到這個人的公開資訊找到這個人的公開資訊再從公開資訊推測出那些更加敏感的非公開資訊推測出那些更加敏感的非公開資訊然後你再回想起這張臉嗎?答案是可以的, 而且我們也做到了當然, 準確度還不是很好在四次嘗試中, 可以辨識出27%受測者的社會安全號碼前五碼但事實上 我們甚至決定做一個 iPhone app利用手機內建像機幫受測者拍一張照片然後上傳至雲端網路接下來馬上就像我對大家描述的一樣即時找出相符的臉, 找出公開資訊試著推斷敏感的私人資訊然後傳回手機這些資訊會顯示在受測者的臉部照片旁這是一個擴增實境的例子也許是一個會令人毛骨悚然的擴增實境案例事實上我們並沒有讓這個app上市只是做為一種觀念的證明 事實上, 利用這些科技到極致的時候事實上, 利用這些科技到極致的時候想像一下未來, 你身旁的陌生人能透過Google眼鏡來看你或者有一天 隱形眼鏡也能做到同樣的事情使用七或八個有關於你的資訊去推測其他可能與你相關的事沒有秘密的未來會是什麼樣子?我們應該關心這個嗎? 我們可能比較願意去相信一個有這麼多數據資料的未來會是一個沒有偏差的未來但是, 事實上, 擁有這麼多資訊不表示我們能夠做出更客觀的決定在另一個實驗裡 我們把求職者的資訊給受測者看在另一個實驗裡 我們把求職者的資訊給受測者看我們的資料含括關於一些有趣, 絕對合法但也許稍微有點尷尬的訊息這些都是受測者張貼在網路上的資訊有趣的是 我們實驗的對象中有些人也發表了類似的訊息但有些人則沒有你認為哪一組人比較可能嚴厲批評我們的受測者?答案出乎意料的是那些發表類似訊息的人這也是種道德觀念不一致的例子 現在你可能正在想這對我來說沒用因為我沒有什麼要藏的東西但事實上 隱私不只是有什麼不好的東西要藏起來而已想像你是某個組織的人事主管你收到應徵者寄來的履歷你決定要找出更多該名應徵者的訊息因此 你就在google上搜尋他們的名字在特定時空你可以找到這筆資訊或是在平行時空 你找到這筆資訊你認為你會同樣的打電話通知應徵者前來面試嗎?如果你這麼認為,那你就不像美國雇主事實上, 他們也在我們的實驗當中我們創造了一些Facebook個人簡介,然後寄送履歷到美國各家公司然後我們偵查、監控看是否他們正在上網搜尋我們的應徵者並且依照這些社群媒體上找到的資訊做事.他們真的這麼作.透過社群媒體, 對技能相當的應徵者們來說也會發生不公平待遇的事情 現在行銷的人想讓我們相信所有關於我們的個人資訊都會用在對我們有利的面向但是再想想, 真的會這樣嗎?幾年前上映的一部電影「關鍵報告」裡一個著名的場景就是湯姆克‧魯斯走進一間賣場有一個個人化的雷射投影廣告出現在他旁邊那部電影的時空背景設定於2054年從現在算起 大約是40年之後那種技術看起來很精彩它已經大大低估各組織能夠匯集起有關你個人的資料量與他們是如何運用這些資料以某一個你無法查覺的方式, 對你造成影響 還有一個例子 這是另一項實驗是我們正在進行中的實驗, 還沒有完成想像一個組織能夠進入你的Facebook好友清單透過某種運算規則他們可以偵測到你最喜歡的兩個好友然後他們就能即時創造出由這兩個好友所組成的臉部合成照在我們之前有研究已經顯示人們在看臉部合成照, 連他們自己都認不出來人們在看臉部合成照, 連他們自己都認不出來但是他們對那些合成照有正面評價所以下次你在找某項產品此時有一個建議購買的廣告廣告將不會是一個固定的代言人他很可能是你其中一位朋友你甚至不知道這種事正發生在你的生活中 現在問題就是目前政策機制是我們必須保護我們自己免於個人資料遭到濫用這就像是以卵擊石其中一項機制就是資訊透明化你必須告訴人們你想拿他們資料做什麼原則上 這是一件非常好的事情這是應該的, 但是這麼做還不夠資訊透明化的方向可能會被誤導你可以告訴大家你想做什麼然後你促使他人揭露或多或少的個人資訊 在另一項實驗中, 實驗對象是學生我們要求他們提供個人資訊關於他們在學校裡做的事包括一些相當敏感的問題 就像這一個在考試的時候 你有作弊過嗎?對其中一組受測者, 我們告訴他們只有其他的同學會看到你的答案對另一組受測者 我們告訴他們所有學生和教職員都會看到你的答案透明化 告知. 這真的有用.第一組受測者比第二組受測者更有可能公佈事實合理吧?但是之後我們加入誤導手段我們對相同兩組學生重覆進行實驗這次在不同的時間告訴受測者我們是如何使用他們的資料這次在不同的時間告訴受測者我們是如何使用他們的資料這次在不同的時間告訴受測者我們是如何使用他們的資料現在我們知道了 你認為我們必須要延遲多久時間為使這種抑制效應無效而這種效應就是知道教職員會看見你的答案?10分鐘?5分鐘?1分鐘?15秒怎樣?15秒就足夠讓兩組人透露出相同資訊量就好像第二組人現在不再關心教職員會看他們的答案就好像第二組人現在不再關心教職員會看他們的答案 現在我必須承認目前為止我說的這些話可能聽起來非常沉悶但我要說的不是這個事實上 我想與大家分享的是還有替代方案我們現在實驗的方式並不是唯一可行的方式當然也不是最好的辦法有人告訴你 「沒人會在乎他的隱私」想想看是否這場遊戲已經遭到設計暗中操作 所以他們才不在意隱私權逐漸發現這些操作手段的已經入侵到那些能夠能夠保護你的方法中逐漸發現這些操作手段的已經入侵到那些能夠能夠保護你的方法中逐漸發現這些操作手段的已經入侵到那些能夠能夠保護你的方法中有人告訴你隱私與巨量資料所帶來的利益是無法共存的想想看近20年研究人員已經研發出數套技術讓幾乎所有電子交易能夠在有更高度的隱私環境下進行我們可以匿名瀏覽網頁傳送唯讀電子郵件這些電子郵件僅能由指定的收件者閱讀就連國家安全局都沒辦法查看我們甚至能在隱私受到保護的情況下進行資料開採另一方面, 在保護隱私權的同時, 我們仍擁有巨量資料所帶來的好處另一方面, 在保護隱私權的同時, 我們仍擁有巨量資料所帶來的好處當然 這些技術也可以看出,在資料持有人與資料提供者之間利益的變化這也許是為什麼你沒有聽過太多有關這些技術的事情 就讓我將話題轉回伊甸園有第二種對伊甸園故事的隱私解釋這與亞當和夏娃覺得全身赤裸和感到羞恥沒有關係你可以在約翰·密爾頓的《失樂園》裡發現對於這個解釋的迴響你可以在約翰·密爾頓的《失樂園》裡發現對於這個解釋的迴響在伊甸園裡 亞當和夏娃只是物品他們很快樂 很滿足然而 他們也缺乏知識和自覺此刻他們恰好吃下名叫「知識」的水果就在那時他們才發現自我他們開始擁有自覺和自主能力然而 所付出的代價就是必須離開伊甸園所以, 隱私權是自由的意義也是代價所以, 隱私權是自由的意義也是代價 市場商人告訴我們巨量資料與社群媒體不只對於他們是獲利的天堂對我們其餘的人也是座伊甸園我們可以得到免費的內容我們可以玩憤怒鳥 使用挑選好的app實際上,在幾年之內政府機構將知道許多關於我們的資訊他們能在我們想到之前推斷我們想做的事情他們能在我們想到之前推斷我們想做的事情也許在我們知道我們需要這些東西之前, 就替我們購買產品也許在我們知道我們需要這些東西之前, 就替我們購買產品 現在有一名英國作家考慮到未來可能會發生這種情況到時候我們可能會為了過舒適的生活而賤賣我們的自主能力與自由其著作比喬治·歐威爾還多這名作家當然就是奧爾德斯·赫胥黎在《美麗新世界》書中 他想像出一個社會那裡的科技是我們為了自由而創造的技術最後我們反被科技奴役然而 在書中 他也提供我們一個逃離那個社會的方式 與那條路很像就是亞當和夏娃離開伊甸園的那條路就「野蠻人」這個詞而言重新找回自主能力和自由是可行的雖然需要付出的代價實在太高所以我相信我們這個時代的其中一個決定性的戰鬥將會是為掌控個人資訊而戰不管巨量資料是否將成為一股迎向自由的力量這場戰鬥終將結束而不會成為一股暗中操縱我們的力量- 現在 我們當中許多人甚至都不知道 戰鬥正在進行不管你喜不喜歡 這就是現況冒著玩弄魔鬼的危險我告訴各位, 這場戰爭的工具就在這裡了解現在發生什麼事就掌握在你手裡只要用滑鼠點幾下就行了 謝謝大家 (掌聲)
