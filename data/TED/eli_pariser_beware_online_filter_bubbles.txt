Talk	en	zh-tw
eli_pariser_beware_online_filter_bubbles	"Mark Zuckerberg, a journalist was asking him a question about the news feed. And the journalist was asking him, ""Why is this so important?"" And Zuckerberg said, ""A squirrel dying in your front yard may be more relevant to your interests right now than people dying in Africa."" And I want to talk about what a Web based on that idea of relevance might look like. So when I was growing up in a really rural area in Maine, the Internet meant something very different to me. It meant a connection to the world. It meant something that would connect us all together. And I was sure that it was going to be great for democracy and for our society. But there's this shift in how information is flowing online, and it's invisible. And if we don't pay attention to it, it could be a real problem. So I first noticed this in a place I spend a lot of time — my Facebook page. I'm progressive, politically — big surprise — but I've always gone out of my way to meet conservatives. I like hearing what they're thinking about; I like seeing what they link to; I like learning a thing or two. And so I was surprised when I noticed one day that the conservatives had disappeared from my Facebook feed. And what it turned out was going on was that Facebook was looking at which links I clicked on, and it was noticing that, actually, I was clicking more on my liberal friends' links than on my conservative friends' links. And without consulting me about it, it had edited them out. They disappeared. So Facebook isn't the only place that's doing this kind of invisible, algorithmic editing of the Web. Google's doing it too. If I search for something, and you search for something, even right now at the very same time, we may get very different search results. Even if you're logged out, one engineer told me, there are 57 signals that Google looks at — everything from what kind of computer you're on to what kind of browser you're using to where you're located — that it uses to personally tailor your query results. Think about it for a second: there is no standard Google anymore. And you know, the funny thing about this is that it's hard to see. You can't see how different your search results are from anyone else's. But a couple of weeks ago, I asked a bunch of friends to Google ""Egypt"" and to send me screen shots of what they got. So here's my friend Scott's screen shot. And here's my friend Daniel's screen shot. When you put them side-by-side, you don't even have to read the links to see how different these two pages are. But when you do read the links, it's really quite remarkable. Daniel didn't get anything about the protests in Egypt at all in his first page of Google results. Scott's results were full of them. And this was the big story of the day at that time. That's how different these results are becoming. So it's not just Google and Facebook either. This is something that's sweeping the Web. There are a whole host of companies that are doing this kind of personalization. Yahoo News, the biggest news site on the Internet, is now personalized — different people get different things. Huffington Post, the Washington Post, the New York Times — all flirting with personalization in various ways. And this moves us very quickly toward a world in which the Internet is showing us what it thinks we want to see, but not necessarily what we need to see. As Eric Schmidt said, ""It will be very hard for people to watch or consume something that has not in some sense been tailored for them."" So I do think this is a problem. And I think, if you take all of these filters together, you take all these algorithms, you get what I call a filter bubble. And your filter bubble is your own personal, unique universe of information that you live in online. And what's in your filter bubble depends on who you are, and it depends on what you do. But the thing is that you don't decide what gets in. And more importantly, you don't actually see what gets edited out. So one of the problems with the filter bubble was discovered by some researchers at Netflix. And they were looking at the Netflix queues, and they noticed something kind of funny that a lot of us probably have noticed, which is there are some movies that just sort of zip right up and out to our houses. They enter the queue, they just zip right out. So ""Iron Man"" zips right out, and ""Waiting for Superman"" can wait for a really long time. What they discovered was that in our Netflix queues there's this epic struggle going on between our future aspirational selves and our more impulsive present selves. You know we all want to be someone who has watched ""Rashomon,"" but right now we want to watch ""Ace Ventura"" for the fourth time. (Laughter) So the best editing gives us a bit of both. It gives us a little bit of Justin Bieber and a little bit of Afghanistan. It gives us some information vegetables; it gives us some information dessert. And the challenge with these kinds of algorithmic filters, these personalized filters, is that, because they're mainly looking at what you click on first, it can throw off that balance. And instead of a balanced information diet, you can end up surrounded by information junk food. What this suggests is actually that we may have the story about the Internet wrong. In a broadcast society — this is how the founding mythology goes — in a broadcast society, there were these gatekeepers, the editors, and they controlled the flows of information. And along came the Internet and it swept them out of the way, and it allowed all of us to connect together, and it was awesome. But that's not actually what's happening right now. What we're seeing is more of a passing of the torch from human gatekeepers to algorithmic ones. And the thing is that the algorithms don't yet have the kind of embedded ethics that the editors did. So if algorithms are going to curate the world for us, if they're going to decide what we get to see and what we don't get to see, then we need to make sure that they're not just keyed to relevance. We need to make sure that they also show us things that are uncomfortable or challenging or important — this is what TED does — other points of view. And the thing is, we've actually been here before as a society. In 1915, it's not like newspapers were sweating a lot about their civic responsibilities. Then people noticed that they were doing something really important. That, in fact, you couldn't have a functioning democracy if citizens didn't get a good flow of information, that the newspapers were critical because they were acting as the filter, and then journalistic ethics developed. It wasn't perfect, but it got us through the last century. And so now, we're kind of back in 1915 on the Web. And we need the new gatekeepers to encode that kind of responsibility into the code that they're writing. I know that there are a lot of people here from Facebook and from Google — Larry and Sergey — people who have helped build the Web as it is, and I'm grateful for that. But we really need you to make sure that these algorithms have encoded in them a sense of the public life, a sense of civic responsibility. We need you to make sure that they're transparent enough that we can see what the rules are that determine what gets through our filters. And we need you to give us some control so that we can decide what gets through and what doesn't. Because I think we really need the Internet to be that thing that we all dreamed of it being. We need it to connect us all together. We need it to introduce us to new ideas and new people and different perspectives. And it's not going to do that if it leaves us all isolated in a Web of one. Thank you. (Applause)"	有位新聞工作者問馬克·祖克柏一個關於動態消息的問题。那新聞工作者問他：「動態消息究竟為什麼重要？」馬克·祖克柏說：「一隻松鼠正死在你的前院，對你來說可能會比非洲人正在死去更有關切性。」現在我想談論當網路基於「相關性」會是什麼樣子。 我在缅因州極之郊區的環境長大，網路的意義對我極之不同。它意味著與世界的連接。它意味著與所有人的連接。當時我非常肯定它會有助民主及會有助我們的社會。但現在網路上资料流動的形色漸漸地、無形地在轉移。假若我們不留心注意，它可能會變成一個問题。我在我經常流覽的地方首先注意到這個問題，這個地方當然是我的臉書頁面。可想而知，我對政治的態度是進步主義，但我亦會叛經離道地結識保守主義者。我喜歡知道他們在想什麼；我喜歡知道他們與什麼有聯繫；我喜歡能從中學到一些東西。因此有一天我很駕訝當我察覺到有關保守派主義的消息從我臉書的動態消息消失。理由是因為臉書能看見我按過哪些連結，它注意到我其實按自由黨朋友的連結多過保守派朋友的連結。在未與我商量過的情況下，它便編走那些連結。那些消息全消失了。 但不是淨只是臉書會做這種無形的、算法式的來編輯網路。Google（谷歌）也會這樣。若我在搜尋一樣東西，你也在搜索一樣東西，即使是在現在、同一個時間，我們搜索的結果都會不同。一個工程師曾告訴過我，即使你登出帳戶，仍然有 57 個訊號在被谷歌觀察著：從你所用的電腦類型到你所用的瀏覽器甚至是你的所在位置，它會以這些來量身訂造你的搜尋結果。試想一下：現在已經沒有標準的谷歌。而且，好笑的是，這很難察覺的。你根本無法看到你的搜尋結果會跟其他人的有所不同。 所以在兩個星期前，我請一些朋友用谷歌搜尋「埃及」，並且寄給我他們搜尋結果的螢幕截圖。這張是我朋友史考特的截圖，而這張是我朋友丹尼爾的截圖。當你將它們並排比較，你根本不用細看那些連結就可以看得出這兩頁是不一樣。但當你細看這些連結，這確實是蠻驚人的。在丹尼爾的谷歌搜尋結果第一頁裡，完全沒有關於埃及抗議的連結。在史考特的搜尋結果就有很多。但在那陣子卻是當日的大新聞。這就是搜尋結果越來越不同的例子。 不只是谷歌及臉書。這趨勢在網路正漸散播。現有很多機構都實施個人化。雅虎新聞，網路上最大型的新聞網站，現在已經個人化，即是不同人會看到不同的東西。哈芬登郵報、華盛頓郵報、紐約時報等等都正在用不同方式盤弄個人化。這種趨勢正在快速地推我們前往一個新世界，一個網路應為我們想看的世界，但未必是一個我們需要看到的世界。正如艾立克·史密特所說：「現在很難要人們觀看或消化一些一點兒也沒有替他們量身訂造的東西。」 我認為這是一個問題，而且我想，如果將全部的過濾器放在一起，用盡所有算法，得到的是一個我稱為「過濾氣泡」的東西。而你的過濾氣泡便是你個人在網上存在的獨特資料宇宙。你個人過濾氣泡的內容基於你是誰和你的行為。但問題是，氣泡的內容不是你可選擇。更重要的是，你完全看不到什麼被刪除。過濾氣泡的其中一個問題被 Netflix 的研究員發現。當在察看 Netflix 的電影列表時，他們發覺一樣有趣的現象，可能我們很多人亦有察覺到，也就是，有些電影馬上就會被訂去看。才剛入上架，就馬上被訂去看了。例如《鋼鐵人》很快就被看完，但《等待超人》便真要等很久。 他們發現在我們 Netflix 的列表裡，正在發生一個很巨型的鬥爭：我們未來的自我志向和現在較衝動的自我之間在拔河。我們全都想成為那個曾經看過《羅生門》的人，但現在我們想看第四遍的《王牌威龍》。（笑聲）所以其實最好的編輯是每樣都給我們一些。它會給我們一點小賈斯汀，亦會給我們一些阿富汗。它會給我們一些像蔬菜一樣的重要資訊，亦會給我們一些像甜點一樣的資訊。所以對這類算法式過濾和這些個人過濾的挑戰，是因為，它們主要是看你首先按的是什麼連結，這個方法會有阻平衡。你不但沒得到均衡的資訊菜單，你可能會得到很多垃圾資訊。 這個想法是在說可能我們對網路的印象是不正確。在這個廣播社會，根據流傳的說法，在這個廣播社會，有一些看門人，叫編輯者，他們控制資料的流通。隨後登場便是網際網路，它掃走這些看門人，讓我們全部人可無阻地聯繫在一起，這真是棒啊。但實在不是這樣。我們看到的是像傳火炬，由人類看門人到算法看門人。但現在這種算法程式還未有編輯人所擁有的嵌入概念。所以若我們讓算法用它的方式來看世界，若讓它來決定我們可看什麼、不可看什麼，那我們便要確定它的決定不只是基於關切性。我們要確定它亦會給我們看一些我們看了未必舒服，但有重要性及有挑戰性的東西，正如 TED 大會那樣會展示其他觀點。 其實像現在這種過濾在以前的社會也發生過。在 1915 年，那時的報章對它們的民事責任不太在意。之後人們發覺到報章實在很重要。因為事實上，一個正常運作的民主社會是不存在的，除非人民能獲得有效的資訊流通。所以報章對事有評論，因為它們扮演過濾網，也因此才有新聞道德的構成。雖然不是完美，但這種方式帶我們走過上一個世紀。現在，在網上我們又像回到 1915 年。我們需要新的看門人將道德責任輸入它們算法的程式裡。 我知道在座很多人替臉書及谷歌工作，像賴利和塞吉，很多人參與建立現今的網際網路，我對此很感謝。但我們真的需要你們確保這些算法的程式裡要有公眾生活和民事責任感。我們需要你們確保它有一定的透明度，讓我們能知道是用什麼準則來決定什麼可通過過濾網。而且我們需要你們給予一些控制力，讓我們可以選擇什麼能通過和不通過。因為我認為我們真的需要網路成為一個我們夢寐以求的平臺。我們需要它連結所有人。我們需要它給我們介紹新的想法、新的人和不同觀點。而它是不可能辦到這些，如果它將我們孤立在唯一自我旳網路裡就絕不可能。 謝謝。 （鼓掌）
