Talk	en	zh-tw
jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn	"It used to be that if you wanted to get a computer to do something new, you would have to program it. Now, programming, for those of you here that haven't done it yourself, requires laying out in excruciating detail every single step that you want the computer to do in order to achieve your goal. Now, if you want to do something that you don't know how to do yourself, then this is going to be a great challenge. So this was the challenge faced by this man, Arthur Samuel. In 1956, he wanted to get this computer to be able to beat him at checkers. How can you write a program, lay out in excruciating detail, how to be better than you at checkers? So he came up with an idea: he had the computer play against itself thousands of times and learn how to play checkers. And indeed it worked, and in fact, by 1962, this computer had beaten the Connecticut state champion. So Arthur Samuel was the father of machine learning, and I have a great debt to him, because I am a machine learning practitioner. I was the president of Kaggle, a community of over 200,000 machine learning practictioners. Kaggle puts up competitions to try and get them to solve previously unsolved problems, and it's been successful  hundreds of times. So from this vantage point, I was able to find out a lot about what machine learning can do in the past, can do today, and what it could do in the future. Perhaps the first big success of  machine learning commercially was Google. Google showed that it is possible to find information by using a computer algorithm, and this algorithm is based on machine learning. Since that time, there have been many commercial successes of machine learning. Companies like Amazon and Netflix use machine learning to suggest products that you might like to buy, movies that you might like to watch. Sometimes, it's almost creepy. Companies like LinkedIn and Facebook sometimes will tell you about who your friends might be and you have no idea how it did it, and this is because it's using the power of machine learning. These are algorithms that have learned how to do this from data rather than being programmed by hand. This is also how IBM was successful in getting Watson to beat the two world champions at ""Jeopardy,"" answering incredibly subtle and complex questions like this one. [""The ancient 'Lion of Nimrud' went missing from this city's national museum in 2003  (along with a lot of other stuff)""] This is also why we are now able to see the first self-driving cars. If you want to be able to tell the difference between, say, a tree and a pedestrian, well, that's pretty important. We don't know how to write those programs by hand, but with machine learning, this is now possible. And in fact, this car has driven  over a million miles without any accidents on regular roads. So we now know that computers can learn, and computers can learn to do things that we actually sometimes don't know how to do ourselves, or maybe can do them better than us. One of the most amazing examples I've seen of machine learning happened on a project that I ran at Kaggle where a team run by a guy called Geoffrey Hinton from the University of Toronto won a competition for automatic drug discovery. Now, what was extraordinary here is not just that they beat all of the algorithms developed by Merck or the international academic community, but nobody on the team had any background in chemistry or biology or life sciences, and they did it in two weeks. How did they do this? They used an extraordinary algorithm called deep learning. So important was this that in fact the success was covered in The New York Times in a front page article a few weeks later. This is Geoffrey Hinton here on the left-hand side. Deep learning is an algorithm inspired by how the human brain works, and as a result it's an algorithm which has no theoretical limitations on what it can do. The more data you give it and the more computation time you give it, the better it gets. The New York Times also showed in this article another extraordinary result of deep learning which I'm going to show you now. It shows that computers  can listen and understand. (Video) Richard Rashid: Now, the last step that I want to be able to take in this process is to actually speak to you in Chinese. Now the key thing there is, we've been able to take a large amount  of information from many Chinese speakers and produce a text-to-speech system that takes Chinese text and converts it into Chinese language, and then we've taken an hour or so of my own voice and we've used that to modulate the standard text-to-speech system so that it would sound like me. Again, the result's not perfect. There are in fact quite a few errors. (In Chinese) (Applause) There's much work to be done in this area. (In Chinese) (Applause) Jeremy Howard: Well, that was at a machine learning conference in China. It's not often, actually, at academic conferences that you do hear spontaneous applause, although of course sometimes at TEDx conferences, feel free. Everything you saw there was happening with deep learning. (Applause) Thank you. The transcription in English was deep learning. The translation to Chinese and the text in the top right, deep learning, and the construction of the voice was deep learning as well. So deep learning is this extraordinary thing. It's a single algorithm that can seem to do almost anything, and I discovered that a year earlier, it had also learned to see. In this obscure competition from Germany called the German Traffic Sign  Recognition Benchmark, deep learning had learned to recognize traffic signs like this one. Not only could it recognize the traffic signs better than any other algorithm, the leaderboard actually showed it was better than people, about twice as good as people. So by 2011, we had the first example of computers that can see better than people. Since that time, a lot has happened. In 2012, Google announced that they had a deep learning algorithm watch YouTube videos and crunched the data on 16,000 computers for a month, and the computer independently learned about concepts such as people and cats just by watching the videos. This is much like the way that humans learn. Humans don't learn by being told what they see, but by learning for themselves what these things are. Also in 2012, Geoffrey Hinton, who we saw earlier, won the very popular ImageNet competition, looking to try to figure out  from one and a half million images what they're pictures of. As of 2014, we're now down to a six percent error rate in image recognition. This is better than people, again. So machines really are doing an extraordinarily good job of this, and it is now being used in industry. For example, Google announced last year that they had mapped every single location in France in two hours, and the way they did it was that they fed street view images into a deep learning algorithm to recognize and read street numbers. Imagine how long it would have taken before: dozens of people, many years. This is also happening in China. Baidu is kind of  the Chinese Google, I guess, and what you see here in the top left is an example of a picture that I uploaded to Baidu's deep learning system, and underneath you can see that the system has understood what that picture is and found similar images. The similar images actually have similar backgrounds, similar directions of the faces, even some with their tongue out. This is not clearly looking at the text of a web page. All I uploaded was an image. So we now have computers which really understand what they see and can therefore search databases of hundreds of millions of images in real time. So what does it mean now that computers can see? Well, it's not just  that computers can see. In fact, deep learning has done more than that. Complex, nuanced sentences like this one are now understandable with deep learning algorithms. As you can see here, this Stanford-based system showing the red dot at the top has figured out that this sentence is expressing negative sentiment. Deep learning now in fact is near human performance at understanding what sentences are about and what it is saying about those things. Also, deep learning has been used to read Chinese, again at about native Chinese speaker level. This algorithm developed out of Switzerland by people, none of whom speak or understand any Chinese. As I say, using deep learning is about the best system in the world for this, even compared to native human understanding. This is a system that we put together at my company which shows putting all this stuff together. These are pictures which have no text attached, and as I'm typing in here sentences, in real time it's understanding these pictures and figuring out what they're about and finding pictures that are similar to the text that I'm writing. So you can see, it's actually understanding my sentences and actually understanding these pictures. I know that you've seen something like this on Google, where you can type in things and it will show you pictures, but actually what it's doing is it's searching the webpage for the text. This is very different from actually understanding the images. This is something that computers have only been able to do for the first time in the last few months. So we can see now that computers can not only see but they can also read, and, of course, we've shown that they can understand what they hear. Perhaps not surprising now that I'm going to tell you they can write. Here is some text that I generated using a deep learning algorithm yesterday. And here is some text that an algorithm out of Stanford generated. Each of these sentences was generated by a deep learning algorithm to describe each of those pictures. This algorithm before has never seen a man in a black shirt playing a guitar. It's seen a man before, it's seen black before, it's seen a guitar before, but it has independently generated this novel description of this picture. We're still not quite at human performance here, but we're close. In tests, humans prefer the computer-generated caption one out of four times. Now this system is now only two weeks old, so probably within the next year, the computer algorithm will be well past human performance at the rate things are going. So computers can also write. So we put all this together and it leads to very exciting opportunities. For example, in medicine, a team in Boston announced that they had discovered dozens of new clinically relevant features of tumors which help doctors make a prognosis of a cancer. Very similarly, in Stanford, a group there announced that, looking at tissues under magnification, they've developed  a machine learning-based system which in fact is better than human pathologists at predicting survival rates for cancer sufferers. In both of these cases, not only were the predictions more accurate, but they generated new insightful science. In the radiology case, they were new clinical indicators that humans can understand. In this pathology case, the computer system actually discovered that the cells around the cancer are as important as the cancer cells themselves in making a diagnosis. This is the opposite of what pathologists had been taught for decades. In each of those two cases, they were systems developed by a combination of medical experts and machine learning experts, but as of last year, we're now beyond that too. This is an example of identifying cancerous areas of human tissue under a microscope. The system being shown here can identify those areas more accurately, or about as accurately, as human pathologists, but was built entirely with deep learning using no medical expertise by people who have no background in the field. Similarly, here, this neuron segmentation. We can now segment neurons about as accurately as humans can, but this system was developed with deep learning using people with no previous  background in medicine. So myself, as somebody with no previous background in medicine, I seem to be entirely well qualified to start a new medical company, which I did. I was kind of terrified of doing it, but the theory seemed to suggest that it ought to be possible to do very useful medicine using just these data analytic techniques. And thankfully, the feedback has been fantastic, not just from the media but from the medical community, who have been very supportive. The theory is that we can take the middle part of the medical process and turn that into data analysis as much as possible, leaving doctors to do what they're best at. I want to give you an example. It now takes us about 15 minutes to generate a new medical diagnostic test and I'll show you that in real time now, but I've compressed it down to  three minutes by cutting some pieces out. Rather than showing you creating a medical diagnostic test, I'm going to show you  a diagnostic test of car images, because that's something we can all understand. So here we're starting with  about 1.5 million car images, and I want to create something that can split them into the angle of the photo that's being taken. So these images are entirely unlabeled, so I have to start from scratch. With our deep learning algorithm, it can automatically identify areas of structure in these images. So the nice thing is that the human and the computer can now work together. So the human, as you can see here, is telling the computer about areas of interest which it wants the computer then to try and use to improve its algorithm. Now, these deep learning systems actually are in 16,000-dimensional space, so you can see here the computer rotating this through that space, trying to find new areas of structure. And when it does so successfully, the human who is driving it can then point out the areas that are interesting. So here, the computer has successfully found areas, for example, angles. So as we go through this process, we're gradually telling the computer more and more about the kinds of structures we're looking for. You can imagine in a diagnostic test this would be a pathologist identifying areas of pathosis, for example, or a radiologist indicating potentially troublesome nodules. And sometimes it can be difficult for the algorithm. In this case, it got kind of confused. The fronts and the backs of the cars are all mixed up. So here we have to be a bit more careful, manually selecting these fronts as opposed to the backs, then telling the computer that this is a type of group that we're interested in. So we do that for a while, we skip over a little bit, and then we train the machine learning algorithm based on these couple of hundred things, and we hope that it's gotten a lot better. You can see, it's now started to fade some of these pictures out, showing us that it already is recognizing how to understand some of these itself. We can then use this concept of similar images, and using similar images, you can now see, the computer at this point is able to entirely find just the fronts of cars. So at this point, the human can tell the computer, okay, yes, you've done a good job of that. Sometimes, of course, even at this point it's still difficult to separate out groups. In this case, even after we let the computer try to rotate this for a while, we still find that the left sides and the right sides pictures are all mixed up together. So we can again give the computer some hints, and we say, okay, try and find a projection that separates out the left sides and the right sides as much as possible using this deep learning algorithm. And giving it that hint — ah, okay, it's been successful. It's managed to find a way of thinking about these objects that's separated out these together. So you get the idea here. This is a case not where the human is being replaced by a computer, but where they're working together. What we're doing here is we're replacing something that used to take a team of five or six people about seven years and replacing it with something that takes 15 minutes for one person acting alone. So this process takes about four or five iterations. You can see we now have 62 percent of our 1.5 million images  classified correctly. And at this point, we can start to quite quickly grab whole big sections, check through them to make sure that there's no mistakes. Where there are mistakes, we can let the computer know about them. And using this kind of process for each of the different groups, we are now up to an 80 percent success rate in classifying the 1.5 million images. And at this point, it's just a case of finding the small number that aren't classified correctly, and trying to understand why. And using that approach, by 15 minutes we get to 97 percent classification rates. So this kind of technique could allow us to fix a major problem, which is that there's a lack of medical expertise in the world. The World Economic Forum says that there's between a 10x and a 20x shortage of physicians in the developing world, and it would take about 300 years to train enough people to fix that problem. So imagine if we can help enhance their efficiency using these deep learning approaches? So I'm very excited about the opportunities. I'm also concerned about the problems. The problem here is that every area in blue on this map is somewhere where services are over 80 percent of employment. What are services? These are services. These are also the exact things that computers have just learned how to do. So 80 percent of the world's employment in the developed world is stuff that computers  have just learned how to do. What does that mean? Well, it'll be fine. They'll be replaced by other jobs. For example, there will be more jobs for data scientists. Well, not really. It doesn't take data scientists  very long to build these things. For example, these four algorithms were all built by the same guy. So if you think, oh,  it's all happened before, we've seen the results in the past of when new things come along and they get replaced by new jobs, what are these new jobs going to be? It's very hard for us to estimate this, because human performance grows at this gradual rate, but we now have a system, deep learning, that we know actually grows in capability exponentially. And we're here. So currently, we see the things around us and we say, ""Oh, computers are still pretty dumb."" Right? But in five years' time, computers will be off this chart. So we need to be starting to think about this capability right now. We have seen this once before, of course. In the Industrial Revolution, we saw a step change in capability thanks to engines. The thing is, though, that after a while, things flattened out. There was social disruption, but once engines were used  to generate power in all the situations, things really settled down. The Machine Learning Revolution is going to be very different from the Industrial Revolution, because the Machine Learning Revolution, it never settles down. The better computers get at intellectual activities, the more they can build better computers to be better at intellectual capabilities, so this is going to be a kind of change that the world has actually never experienced before, so your previous understanding of what's possible is different. This is already impacting us. In the last 25 years, as capital productivity has increased, labor productivity has been flat, in fact even a little bit down. So I want us to start having this discussion now. I know that when I often tell people about this situation, people can be quite dismissive. Well, computers can't really think, they don't emote, they don't understand poetry, we don't really understand how they work. So what? Computers right now can do the things that humans spend most of their time being paid to do, so now's the time to start thinking about how we're going to adjust our social structures and economic structures to be aware of this new reality. Thank you. (Applause)"	過去如果想用電腦來作點新東西，你需要設計程式。而現在，你們可能沒做過程式設計這件事，它需要規劃相當詳細的細節那些你想讓電腦執行的每一個步驟以達到你的目的。如果你沒有概念要怎麼做的話那會是個很大的挑戰。 亞瑟·撒姆爾也曾面對這種挑戰。他在 1956 年便想到用這台電腦能夠在西洋跳棋棋賽打敗他。要如何設計這樣的程式？把細節通通寫出來，如何讓電腦比你還會下棋？於是他想出了一個點子：他讓電腦與電腦本身對弈數千次以學習如何玩西洋棋。然而，在 1962 年做到了，電腦打敗了康乃狄克州的冠軍。 於是亞瑟·撒姆爾成為了機器學習之父，我尊敬他，因為我也是個機器學習實踐者，我曾是 Kaggle 的會長，Kaggle 是個超過 20 萬人的機器學習實踐者的社群。Kaggle 設立了一些比賽讓他們參與解決過去無法解決的問題，而有上百的成功個案。從這有利的環境中，我發現很多機器學習在過去和現在可以做到的事情，還有未來可以做到的事。第一個機器學習的商業成功案例是谷歌。谷歌展示找尋資料的方法是使用計算機演算法，而這演算法是以機器學習為基礎。自此，機器學習有很多的商業成功例子，譬如亞馬遜和奈飛公司用機器學習會向你推薦你可能想買的商品，你可能想看的影片。有時，你可能會很訝異。像領英和臉書等公司有些時候會告訴你誰會是你的朋友而你根本不知道他們是如何做到的，因為他們用了機器學習這強大的功能。演算法從資料去學習這類事情不需要動手去編寫程式。 這也是 IBM 過去能成功的原因讓超級電腦「華生」在「危機遊戲」中打敗兩屆世界冠軍。回答一些細碎和複雜的問題，像是「2003年，古獅像在這城市的國家博物館消失了（連同其他物品）」這也是我們現在能看到第一部自行駕駛汽車的原因。如果你能說出不同點，像是一棵樹和一條行人道，那顯得非常重要。我們不知道如何設計這樣的程式，不過通過機器，這就成為可能。事實上，這部汽車已經行駛一百萬英哩在正常路面沒有發生事故。 我們現在都知道電腦能夠學習，學習做一些有時我們自己也不知道怎麼做的事，還可能比我們做得更好。其中一個機器學習的經典例子是我在 Kaggle 所做的一個專案由傑佛里·辛頓帶領的團隊他是多倫多大學的教授他們贏了新藥研發的比賽。他們出色地方不只打敗了默克藥廠或國際學術社群所研發的演算法，他們的團隊沒有化學生物或生命科學的背景，而且只花了兩個星期就完成。他們怎麼做到的？他們用了一個很出色的演算法叫做「深度學習」。這是重要且成功的事情在數星期後被刊登在紐約時報頭版。左手邊那位是傑佛里·辛頓。深度學習是一種受到人類大腦啟發的演算法，它是一種演算法做法不受理論限制的演算法。你給它越多的資料和運算時間，會得到更好的結果。 紐約時報的文章裡也介紹到深度學習的非凡成就我現在要展示給你們看。它顯示電腦能聽懂和理解資料的能力。 （影片）理察·拉希德: 現在，最後一步是我能夠理解這個程序我能夠跟你說中文。現在關鍵的是，我們從很多講中文的人士中收集大量的資訊然後產生文字轉化語言的系統將中文文字轉化成中文語言，然後錄一個小時我自己的聲音我們使用它去調變使標準文字轉化語音系統的聲音聽起來像我的聲音。再一次，雖然結果沒有很完美，裡面還有一些錯誤。（中文）（掌聲）在這個領域還有很多工作要做。（中文）（掌聲） 傑里米·霍華德：那是在中國舉行的機器學習研討會。那不常有，事實上，在學術會議上聽到熱烈的掌聲，雖然有些時候 TEDx 講座不拘泥形式。你所看到的都是出於深度學習（掌聲）謝謝。英文文字翻譯由深度學習完成的。翻譯成中文和右上角的文稿也是出於深度學習，連創建聲音也都是深度學習。 深度學習是如此的神奇。它是個單一的演算法似乎可以完成任何事情，我一年前還發現它可以學會看這個德國遊戲的比賽叫德國交通標誌確認基準，深度學習能認出這個交通標誌。它不只確認交通標誌的能力比其他的演算法好，在排行榜上更顯示它做得比人類好，正確性是人類的兩倍。2011 以前，我們有了第一個例子視力高於人類的電腦。從那時開始，許多電腦也可以做到。2012 年谷歌宣佈使用深度學習演算法來監看 Youtube 影片收集一個月 1,600 台電電腦的資料，電腦獨立識別人或貓的概念僅透過觀看影片。這樣更像人類的學習方式。人類並非通過別人的指示來學習，而是從自己搞懂事情來學習。在 2012 年傑佛里·辛頓我們之前看到的人，贏了很有名的映像網路比賽，嘗試從 150 萬的圖像中找出想要的圖像。2014 年, 我們現在圖像辨識的錯誤率降到 6％ 以下。這再次證明它比人類優秀。 可見機器真可以做到如此非凡的成就，它現在已經用在產業上了。比如說，谷歌去年宣佈他們可以在兩小時内把法國每一個位置繪成地圖，他們用的方式是把街景圖像輸入深度學習演算法來辨認和讀取街道號碼。想想我們以前需要花多少時間？至少好幾十人加上好幾年呢。同樣的情況也發生在中國。我想「百度」類似中國的谷歌，在左上角你會看見一張我上傳到百度深度學習系統的圖片，下方你可以看到系統可以理解這張圖片而且能找到相似的圖像。類似的圖像也就是有相似的背景，相似面孔的角度，有的圖像甚至有伸出舌頭。這個網頁的文字看不大清楚，因為我上傳的都是圖像。這顯示了電腦能明白他們所看到的電腦能夠搜尋資料庫以即時的方式從億萬張圖片中搜尋。 現在的電腦能夠去看是表示什麼意思呢？其實電腦不只能看見。事實上深度學習可以做得更多。像這個樣複雜，僅有小小差別的句子現在的深度學習演算法能夠理解。你可以看到，這以史丹福為基礎的系統顯示上面的紅點指這句子是在表達負面的情緒。深度學習現在已經接近人類的行為能理解句子是要表達什麼。同時，深度學習也能用以閱讀中文，程度相當於以中文為母語的水平。這演算法發展於瑞士沒有一個會說中文的團隊。像我說的，深度學習是一個最好的系統對完成這任務來說，甚至比人類還要好。 這個系統是我公司建立的要把這些東西都集中在一起。這是一些沒有文字描述的圖片，我在這裡輸入句子，它在同步理解這些照片找出它們是有關什麼的照片也找出跟我句子相關類似的圖片。所以你看，它真的能理解我的句子。也完全的理解這些圖片。你在谷歌上也看過類似的，你可以輸入文字而它會顯示圖片，但事實上，它在尋索網頁上的文字。這跟理解圖片有很大的不同。理解圖片只有電腦可以做電腦在過去幾個月才會做的事。 電腦不單能看見也能閱讀，而且我們顯示了電腦能理解所聽到的。或許不意外地，我要告訴你們電腦也能書寫。這是我昨天用深度學習演算法所產生的文字。這裡有一些非史丹佛演算法所產生的文字。這些句子的產生是透過深度學習演算法對圖片進行描述。這演算法是電腦從來沒有看見過一個穿黑襯衫的男子彈吉他。電腦見過男人，看過黑色，見過吉他，它自己便對圖片做出描述。雖然還沒有超越人類，不過很接近了。依據統計，人們較喜歡電腦的圖片說明有四分之一的人會做這樣的選擇。這系統在兩個星期前開發完成，估計在明年，電腦演算法將會超越人類如果依照這樣的速度發展下的話。到時候電腦也會書寫了。 我們把這些都放在一起，讓它來引導到一個令人振奮的時機。像在藥物方面，一個波士頓的團隊宣佈他們發現了數十種腫瘤的臨床特徵幫助醫生預測癌症。同樣的，在史丹佛，一個組織宣佈在放大鏡下觀察組織，他們開發一個以機器學習為基礎的系統比人類病理學家更有效地預測癌症病患的生存率。這些例子，不但能更準確地預測，而且也能帶來更多科技上的洞見。在放射學的個案中，他們是人類所能理解的新臨床指標。在這病理學個案，電腦系統發現癌症周圍的細胞在診斷的時候是跟癌細胞一樣重要。這跟病理學家10 年來的說法相反。在這兩個個案，系統的開發人員是由醫學專家和機器學習專家所組成，但自去年開始，我們也超越了這些。這是確認癌症範圍的例子是在顯微鏡下的人類組織。系統顯示可以更準確地確認範圍，如病理學家般準確，不過沒有藥物專家來建構整套深度學習系統系統是由一些沒有專業背景的人完成。同樣地，從是細胞分裂。我們的系統可以像人類般精確地分裂神經細胞，不過開發這套深度學習系統沒有一個人來自醫學背景。 就是我和一些沒有醫學背景的人，看來我頗有資格開一家醫藥公司。我確實這麼做了。我是以戒慎恐懼的心情開始做，不過理論顯示這是可行的用這些資料分析技術來製作有效的藥物。感恩的是回應也挺不錯，這回應不只是來自媒體，而且還有醫藥社群，他們都很支持。理論上我們能在醫務過程中盡量轉換成資料分析，讓醫生去做他們擅長的。我舉一個例子。我們現在花 15 分鐘來創造一項新的醫學診斷測試我會讓你同步看到過程，不過我已刪除部分資料壓縮成三分鐘。我不會向你們展示創造出來的醫學診斷測試，我要向你們展示一項汽車圖片的診斷測試，因為這個我們都能理解。 我們從 150 萬張的汽車圖片開始，我希望創造一些東西把圖片分類而且依圖片拍攝的角度來分類。這些圖片完全沒有標題，我必需從零開始。深度學習演算法，它能自動確認這些圖片的結構。美好的是人和電腦可以合作看看這裡，這個人，正在告訴電腦關於感興趣的範圍而電腦會嘗試用它來改善電腦的演算法。這些深度學習系統有 16,000 個立體空間，你可以看見電腦讓他們在這空間旋轉，嘗試找出新的區域結構。當它成功時，在開車的人能夠指出有興趣的地方。這裡，電腦成功的找到了那地區，再舉例，角度，通過這個過程，我們漸漸地告訴電腦更多關於我們在找的結構類型。你可以想像一個診斷測試像是一個病理學家辨認病症的範圍，或是放射治療師界定潛在的腫瘤。有些時候對演算法來說是有些困難。在我們這個例子，它會出現混亂。汽車的正面和背面都混淆不清了。我們需要更小心，手動選出正面跟背面有相反效果的文字，然後告知電腦這是一種我們有興趣的一類。 這要花了一些時間來做，所以我們跳過，然後我們訓練機器學習演算法以好幾百張圖片去訓練它，我們希望它會做得更好。你可以看見，它開始刪除一些圖片，顯示它已經知道可以自己理解這些圖片。我們運用相似圖片的概念，用類似的圖片，你可以看到，電腦現在可以完全找到正面的汽車。這時，人類可以告訴電腦，對，你做的很好。 當然，有些時候，即使在這個階段分組仍然是困難的。在這情況，儘管我們讓電腦嘗試旋轉圖片一陣子，我們還是發現左邊和右邊的圖片是混淆在一起的。於是我們再次給電腦一些提示，像是嘗試去發現一個計畫可以儘量區分出左邊和右邊的圖片是透過使用深度學習演算法。給予提示後，好，它已經完成了。它找到一個方法想像這些目標來分別這些分類。 你現在知道了。這並不是電腦取代人類，而是兩者一起合作。我們在做的事情是在過去需要5 或 6 個人花 7 年時間完成的事情現在只需一個人15 分鐘來完成。 這個過程需要重覆 4 或 5 次。你現在可以看到我們在 150 萬的圖片中有 62％ 是正確分類。現在，可見我們可以迅速地掌握整個大部分資料，再檢查以確定沒有錯誤。有錯誤，我們可以讓電腦知道錯誤的地方。每一個不同的分類我們都使用這種程序來做，我們現在在分辨 150 萬張的圖片時有超過 80％ 的成功率，現在，在這個案例找到少數幾個不正確的分類，讓電腦了解原因。用這種方法，15 分鐘就有 97％ 的分辨率。 這種技術可以幫助解決一個重要的問題，醫療專家不足的問題。世界經濟論壇表示在發展中國家，內科醫生有 10 倍到 20 倍的短缺。這要三百年的時間才能訓練足夠的人來處理這個問題。想像一下，我們是否可以幫助提高效率是使用深度學習這個方法來提升？ 我對這個機會感到很興奮。我也關注這些問題。問題是在這地圖上每個藍色的地方那裡都有 80％ 的服務人員。什麼是服務？這些就是服務。電腦剛學會如何去做是確實的事。發展中國家 80％ 的僱員工作電腦已開始學習如何做。這意味什麼？那可好。他們將會被其他的職業取代。舉例：需要更多科學家來工作。不過，這不完全正確。數據科學家不需要花很久的時間去做這些事情。例如，這四個演算法是同一個人設計的。若你認為這些以前都發生過，過去我們看過新事物出現的結果他們被新的職務所取替，那些新的職業會是什麼呢？我們很難去判斷，因為人類的能力以這個速度逐漸成長，我們現在有了深度學習系統，我們知道以指數的方式增長。我們在這裡。最近，我們看周圍的事物會說：電腦還是很笨，不是嗎？但是在五年內，電腦將會超越這張圖表。我們需要開始思考這個能力。 當然，我們曾經看過這個。在工業革命時期，發動機讓生產力往前跨一大步。雖然，一段時間之後，事情轉為平靜。那時社會混亂，發動機被普遍使用產生動力，事情就能真正得到解決。機器學習革命與工業革命大不相同，因為機器學習革命，永遠不會停下來。電腦更具智力活動，他們能製造更好的電腦去運作更好的智能活動，這是一種改變從未經歷過的改變，你之前的理解的可能性是不同的。 這已經影響我們。過去 25 年，資本生產力一直在增長，勞動生產力已經放緩，事實上已有一點點下降。 我想我們開始討論這個議題。我知道當我告訴別人這種情況時，人們可以不以為然。電腦不會思考，它們沒有感情，也不了解詩，我們不真正理解它們怎麼運作。可是，哪又如何？電腦現在可以作人們花大部分時間得到報酬所做的事情，所以我們該是思考的時候我們如何調整我們的社會和經濟結構請關注這些新的改變。謝謝（掌聲）
