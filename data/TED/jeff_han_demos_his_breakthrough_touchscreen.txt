Talk	en	zh-tw
jeff_han_demos_his_breakthrough_touchscreen	I'm really excited to be here today. I'll show you some stuff that's just ready to come out of the lab, literally, and I'm really glad that you guys are going to be among the first to see it in person, because I really think this is going to really change the way we interact with machines from this point on. Now, this is a rear-projected drafting table. It's about 36 inches wide and it's equipped with a multi-touch sensor. Normal touch sensors that you see, like on a kiosk or interactive whiteboards, can only register one point of contact at a time. This thing allows you to have multiple points at the same time. They can use both my hands; I can use chording actions; I can just go right up and use all 10 fingers if I wanted to. You know, like that. Now, multi-touch sensing isn't completely new. People like Bill Buxton have been playing around with it in the '80s. However, the approach I built here is actually high-resolution, low-cost, and probably most importantly, very scalable. So, the technology, you know, isn't the most exciting thing here right now, other than probably its newfound accessibility. What's really interesting here is what you can do with it and the kind of interfaces you can build on top of it. So let's see. So, for instance, we have a lava lamp application here. Now, you can see, I can use both of my hands to kind of squeeze and put the blobs together. I can inject heat into the system here, or I can pull it apart with two of my fingers. It's completely intuitive; there's no instruction manual. The interface just kind of disappears. This started out as a screensaver app that one of the Ph.D. students in our lab, Ilya Rosenberg, made. But I think its true identity comes out here. Now what's great about a multi-touch sensor is that, you know, I could be doing this with as many fingers here, but of course multi-touch also inherently means multi-user. Chris could be interacting with another part of Lava, while I play around with it here. You can imagine a new kind of sculpting tool, where I'm kind of warming something up, making it malleable, and then letting it cool down and solidifying in a certain state. Google should have something like this in their lobby. (Laughter) I'll show you a little more of a concrete example here, as this thing loads. This is a photographer's light-box application. Again, I can use both of my hands to interact and move photos around. But what's even cooler is that if I have two fingers, I can actually grab a photo and then stretch it out like that really easily. I can pan, zoom and rotate it effortlessly. I can do that grossly with both of my hands, or I can do it just with two fingers on each of my hands together. If I grab the canvas, I can do the same thing — stretch it out. I can do it simultaneously, holding this down, and gripping on another one, stretching this out. Again, the interface just disappears here. There's no manual. This is exactly what you expect, especially if you haven't interacted with a computer before. Now, when you have initiatives like the $100 laptop, I kind of cringe at the idea of introducing a whole new generation to computing with this standard mouse-and-windows-pointer interface. This is something that I think is really the way we should be interacting with machines from now on. (Applause) Now, of course, I can bring up a keyboard. (Laughter) And I can bring that around, put that up there. Obviously, this is a standard keyboard, but of course I can rescale it to make it work well for my hands. That's really important, because there's no reason in this day and age that we should be conforming to a physical device. That leads to bad things, like RSI. We have so much technology nowadays that these interfaces should start conforming to us. There's so little applied now to actually improving the way we interact with interfaces from this point on. This keyboard is probably actually the really wrong direction to go. You can imagine, in the future, as we develop this kind of technology, a keyboard that kind of automatically drifts as your hand moves away, and really intelligently anticipates which key you're trying to stroke. So — again, isn't this great? (Laughter) Audience: Where's your lab? Jeff Han: I'm a research scientist at NYU in New York. Here's an example of another kind of app. I can make these little fuzz balls. It'll remember the strokes I'm making. Of course I can do it with all my hands. It's pressure-sensitive. What's neat about that is, I showed that two-finger gesture that zooms in really quickly. Because you don't have to switch to a hand tool or the magnifying glass tool, you can just continuously make things in real multiple scales, all at the same time. I can create big things out here, but I can go back and really quickly go back to where I started, and make even smaller things here. This is going to be really important as we start getting to things like data visualization. For instance, I think we all enjoyed Hans Rosling's talk, and he really emphasized the fact I've been thinking about for a long time: We have all this great data, but for some reason, it's just sitting there. We're not accessing it. And one of the reasons why I think that is will be helped by things like graphics and visualization and inference tools, but I also think a big part of it is going to be having better interfaces, to be able to drill down into this kind of data, while still thinking about the big picture here. Let me show you another app here. This is called WorldWind. It's done by NASA. We've all seen Google Earth; this is an open-source version of that. There are plug-ins to be able to load in different data sets that NASA's collected over the years. As you can see, I can use the same two-fingered gestures to go down and go in really seamlessly. There's no interface, again. It really allows anybody to kind of go in — and it just does what you'd expect, you know? Again, there's just no interface here. The interface just disappears. I can switch to different data views. That's what's neat about this app here. NASA's really cool. These hyper-spectral images are false-colored so you can — it's really good for determining vegetative use. Well, let's go back to this. The great thing about mapping applications — it's not really 2D, it's 3D. So, again, with a multi-point interface, you can do a gesture like this — so you can be able to tilt around like that — (Surprised laughter) It's not just simply relegated to a kind of 2D panning and motion. This gesture is just putting two fingers down — it's defining an axis of tilt — and I can tilt up and down that way. We just came up with that on the spot, it's probably not the right thing to do, but there's such interesting things you can do with this interface. It's just so much fun playing around with it, too. (Laughter) And so the last thing I want to show you is — I'm sure we can all think of a lot of entertainment apps that you can do with this thing. I'm more interested in the creative applications we can do with this. Now, here's a simple application here — I can draw out a curve. And when I close it, it becomes a character. But the neat thing about it is I can add control points. And then what I can do is manipulate them with both of my fingers at the same time. And you notice what it does. It's kind of a puppeteering thing, where I can use as many fingers as I have to draw and make — Now, there's a lot of actual math going on under here for this to control this mesh and do the right thing. This technique of being able to manipulate a mesh here, with multiple control points, is actually state of the art. It was released at SIGGRAPH last year. It's a great example of the kind of research I really love: all this compute power to make things do the right things, intuitive things, to do exactly what you expect. So, multi-touch interaction research is a very active field right now in HCI. I'm not the only one doing it, a lot of other people are getting into it. This kind of technology is going to let even more people get into it, I'm looking forward to interacting with all of you over the next few days and seeing how it can apply to your respective fields. Thank you. (Applause)	我真的，真的非常興奮，今天能夠在這邊，因為，我將要向你們展示一些，剛剛完成研發的技術，真的，而且我非常高興，你們能夠成為世界上，親眼目睹這個技術，最早的一群人之一，因為我真的，真的認為這將會改變，真真正正的改變，從今以後我們與機器互動的方式。 現在，這邊有一個背投影式繪圖桌。它大概有 36 吋寬，而且它配備有多點觸控感應器。平日所見的一般觸控感應器，像是自動櫃台，或是互動式白板，在同一時間，只能辨識一個接觸點。而這東西，能允許你同時間，具有多點控制。這些觸控點，可以是從我的雙手而來，我可以單手使用多指，或者，如果我想要，我可以一次使用全部十根手指。你知道的，就像這樣。 雖然多點觸控不是全然嶄新的概念。我是說，像是著名的 Bill Buxton 早在 80 年代就已經開始嘗試這個概念。然而，我在這裡所建造的這個方法，是具有高解析度、低價，並且也許最重要的是，其硬體可以非常簡單的調整尺寸。因此，這個技術，如我所言，並不是現在你所見到，最令人興奮的東西，可能除了它與眾不同的低價位以外。這邊真正令人感興趣的是，你能夠用它來做什麼事？以及你將能夠利用它，所創造出來的介面。現在讓我們來看看。 舉例來說，這邊我們有一個岩漿燈軟體。你可以看到，我可以使用我的雙手，將這些斑點經由擠壓，而使它們變成一團。我可以像這樣將系統加熱，或者我可以用我的兩跟手指，將它們分開。這是完全直覺性的操作，你將不會需要使用手冊。操控介面似乎就這樣憑空消失了。這一開始，是我們實驗室的一個博士班學生，所創造的一個類似螢幕保護程式的軟體他的名字叫做 Ilya Rosenberg. 但是我認為，這個軟體真正的價值，在這邊顯現了。 多點觸控感應器厲害的地方在這邊，像這樣，我可以使用許多手指來操控這個，但是當然，多點觸控本質上也意味著多使用者。所以克里斯 (Chris) 可以到台上來，與岩漿的另一部份互動，在此同時，我在這邊玩弄這一部份。你可以將之想像為一種新的雕塑工具，在這邊我將部份加熱，增加它的可塑性，然後讓它冷卻、硬化到某個程度。谷歌 (Google) 在他們大廳應該要有個像這樣的東西。（笑聲） 我將要向你們展示某樣東西，一個比較實際的應用範例，等它載入完成。這是一個攝影師的燈箱工具應用程式。再一次地，我可以使用我的兩隻手，來與這些照片互動並移動它們。但是更酷的是，如果我用兩跟手指，事實上我可以抓取一張照片，並且非常簡單的將之放大，就像這樣。我可以任意地拖移、縮放並旋轉這張照片。我可以使用我的雙手掌來這樣做，或者，我可以只用我任何一隻手的兩隻手指一起來完成。如果我抓住整個畫板，我可以做同樣的事，將之放大。我可以同時操作，現在我一邊抓住這個不放，然後握住另外一張，把它像這樣放大。 同樣的，在這裡我們也看不到操控介面。不需要說明書。它的操作結果完全如你所預期，特別是如果你以前沒有接觸過電腦的話。現在，如果你想創造一個事業，像是一百美元的筆電，其實，我對於我們將要向一個全新世代的族群介紹傳統電腦的視窗滑鼠點擊介面這個想法，有所保留。多點觸控才是我認為從今爾後，我們所應該與機器互動的真正方法。（掌聲）多點觸控才是我認為從今爾後，我們所應該與機器互動的真正方法。（掌聲）當然，我可以在這邊叫出鍵盤。我可以將我打的字帶出來，將之放在這邊。很明顯的，這與一般標準鍵盤並無不同，但是當然，我可以重新縮放這個鍵盤的大小，讓它適合我的雙手使用。這是非常重要的，因為在今日的科技下，沒有理由我們應該去適應物理性的裝置。那將會導致不好的後果，例如：重複性勞損。今日我們有許多這麼好的科技，這些操控介面應該開始來適應我們。直到今日，真正改善我們與介面互動的應用還太少。直到今日，真正改善我們與介面互動的應用還太少。這個虛擬鍵盤，事實上可能是一個錯誤的發展方向。你可以想像，在未來，當我們開始發展這種技術之後，一個鍵盤，當你移開手的時候，也會自動漂移開來，而且會非常聰明地預測，你將要用你的手打擊那一顆按鍵。因此～ 再問一次，很棒吧？ 聽眾：你的實驗室在哪裡？ 韓傑夫：我是紐約市紐約大學的研究科學家。 這是另一種應用程式的範例。我可以創造出這些小毛球。它會記憶我所做過的點擊。當然，我可以使用兩手操作。你會注意到，它具有壓感功能。但是，真正棒的在這邊，我已經向你們示範過兩指操控手勢，它能夠讓你很快的放大。因為你不需要事先切換到手掌工具或者放大鏡工具;你可以連續地在多個不同的比例上，即時的創造東西，一次完成。我可以在這邊建造大東西，我也可以回去，非常迅速的回去回到我一開始的地方，然後甚至在這邊建造更小的東西。 這將會變得非常重要，當我們一旦開始從事像是資料視覺化的工作。舉例來說，我想我們都非常喜歡 Hans Rosling 的演說，而且他真正的，著重在強調一個我也已經思考許久的事實，我們都擁有這些了不起的資料，但是，基於某些原因，它們只是被擺在那邊。我們並沒有真正的去使用它。我認為其中一個原因就是，藉由圖像、視覺化和參考工具，可以幫助我們處理這些資料。但是，很大的一部分，我也著重在開始能夠創建更好的使用者介面，能夠鑽研深入像這樣的資料，但同時仍能保持對整體的宏觀性。 現在，讓我向你們展示另一個應用程式。這個程式叫做「世界風」。是太空總署 (NASA) 所研發的。它就像我們都見過的谷歌地球 (Google Earth);這就好像是它的開源碼版本。有附加檔能夠載入太空總署經年蒐集的各種資料集。但是，如你所見，我可以使用同樣的兩指手勢非常迅速的下潛、進入地球。又一次的，我們看不到操控介面的存在。這真的能讓所有人，感覺非常地融入那個環境，而且操作就如同你所預期一般，你能體會嗎？再一次，在這邊你看不到操作介面。介面就這樣消失了。我可以切換到不同的資料瀏覽。這就是這個應用程式厲害的地方。就像這樣。太空總署非常的酷。它們有這些超光譜影像這些影像是人工成色的，對於決定植物的繁茂程度非常有幫助。讓我們回到剛剛那裡。 地圖軟體的偉大之處在於，它不只是 2D 平面，它也可以是 3D 立體影像。所以再一次，應用多點觸控介面，你可以使用像這樣的手勢，所以你可以使畫面像這樣傾斜，你知道。不只是受限於簡單的 2D 平面攀移與移動。我們已經研發出了這些手勢，像這樣，只要放入你的兩跟手指，它界定了傾斜的軸線，如此這般我就可以向上或向下任意傾斜。這是我們在這邊，剛剛想出來的點子，你知道嗎？也許這不是這樣做最好的方法，但是使用這種介面，你可以做許多很有趣的事情。就算你什麼都不做，只是玩玩，也會感覺愉快。（笑聲） 所以，最後一件我想要向你們展示的東西是，你知道，我確信我們都可以想到很多，你可以利用這個東西所做的娛樂方面的應用。我對於我們可以利用這東西，所做的創意性應用更感興趣。現在，這邊有一個簡單的程式，我可以畫曲線。當我將曲線封閉起來的時候，它就變成一個人偶。但是有趣的事情是，我可以增加控制點。然後我可以用我雙手的手指同時操控它們。你會注意到它是怎麼做的。這就像是操控木偶一般，這邊我可以使用我的十跟手指去畫出並做出玩偶。 事實上，在這表象之下需要很多數學運算，然後它才能控制這些圖案，並正確的反應。我的意思是，在這邊這個能夠操控圖案的技術，並使用多個操控點，事實上是屬於一種尖端科技。這個技術去年才在計算機圖形學會議上公開，但這是我真正喜好研究領域的良好範例。所有這些需要使事情做「對」，背後的電腦運算。直覺性的事情。做如你所預期一模一樣的事。 多點觸控互動研究，現在在人機介面領域非常地活躍。我不是唯一一個在做這方面研究的，還有很多其他的人也在這領域。而這種技術，將會讓更多人加入這個領域的研究，我真的非常期待，跟在場各位接下來幾天的互動看看這技術，將能如何應用在你們所處的領域。謝謝大家。（掌聲）
