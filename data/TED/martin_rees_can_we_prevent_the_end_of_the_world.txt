Talk	en	zh-tw
martin_rees_can_we_prevent_the_end_of_the_world	"Ten years ago, I wrote a book which I entitled ""Our Final Century?"" Question mark. My publishers cut out the question mark. (Laughter) The American publishers changed our title to ""Our Final Hour."" Americans like instant gratification and the reverse. (Laughter) And my theme was this: Our Earth has existed for 45 million centuries, but this one is special — it's the first where one species, ours, has the planet's future in its hands. Over nearly all of Earth's history, threats have come from nature — disease, earthquakes, asteroids and so forth — but from now on, the worst dangers come from us. And it's now not just the nuclear threat; in our interconnected world, network breakdowns can cascade globally; air travel can spread pandemics worldwide within days; and social media can spread panic and rumor literally at the speed of light. We fret too much about minor hazards — improbable air crashes, carcinogens in food, low radiation doses, and so forth — but we and our political masters are in denial about catastrophic scenarios. The worst have thankfully not yet happened. Indeed, they probably won't. But if an event is potentially devastating, it's worth paying a substantial premium to safeguard against it, even if it's unlikely, just as we take out fire insurance on our house. And as science offers greater power and promise, the downside gets scarier too. We get ever more vulnerable. Within a few decades, millions will have the capability to misuse rapidly advancing biotech, just as they misuse cybertech today. Freeman Dyson, in a TED Talk, foresaw that children will design and create new organisms just as routinely as his generation played with chemistry sets. Well, this may be on the science fiction fringe, but were even part of his scenario to come about, our ecology and even our species would surely not survive long unscathed. For instance, there are some eco-extremists who think that it would be better for the planet, for Gaia, if there were far fewer humans. What happens when such people have mastered synthetic biology techniques that will be widespread by 2050? And by then, other science fiction nightmares may transition to reality: dumb robots going rogue, or a network that develops a mind of its own threatens us all. Well, can we guard against such risks by regulation? We must surely try, but these enterprises are so competitive, so globalized, and so driven by commercial pressure, that anything that can be done will be done somewhere, whatever the regulations say. It's like the drug laws — we try to regulate, but can't. And the global village will have its village idiots, and they'll have a global range. So as I said in my book, we'll have a bumpy ride through this century. There may be setbacks to our society — indeed, a 50 percent chance of a severe setback. But are there conceivable events that could be even worse, events that could snuff out all life? When a new particle accelerator came online, some people anxiously asked, could it destroy the Earth or, even worse, rip apart the fabric of space? Well luckily, reassurance could be offered. I and others pointed out that nature has done the same experiments zillions of times already, via cosmic ray collisions. But scientists should surely be precautionary about experiments that generate conditions without precedent in the natural world. Biologists should avoid release of potentially devastating genetically modified pathogens. And by the way, our special aversion to the risk of truly existential disasters depends on a philosophical and ethical question, and it's this: Consider two scenarios. Scenario A wipes out 90 percent of humanity. Scenario B wipes out 100 percent. How much worse is B than A? Some would say 10 percent worse. The body count is 10 percent higher. But I claim that B is incomparably worse. As an astronomer, I can't believe that humans are the end of the story. It is five billion years before the sun flares up, and the universe may go on forever, so post-human evolution, here on Earth and far beyond, could be as prolonged as the Darwinian process that's led to us, and even more wonderful. And indeed, future evolution will happen much faster, on a technological timescale, not a natural selection timescale. So we surely, in view of those immense stakes, shouldn't accept even a one in a billion risk that human extinction would foreclose this immense potential. Some scenarios that have been envisaged may indeed be science fiction, but others may be disquietingly real. It's an important maxim that the unfamiliar is not the same as the improbable, and in fact, that's why we at Cambridge University are setting up a center to study how to mitigate these existential risks. It seems it's worthwhile just for a few people to think about these potential disasters. And we need all the help we can get from others, because we are stewards of a precious pale blue dot in a vast cosmos, a planet with 50 million centuries ahead of it. And so let's not jeopardize that future. And I'd like to finish with a quote from a great scientist called Peter Medawar. I quote, ""The bells that toll for mankind are like the bells of Alpine cattle. They are attached to our own necks, and it must be our fault if they do not make a tuneful and melodious sound."" Thank you very much. (Applause)"	十年前，我寫了一本書。書名為《 我們的末世紀？》以問號結尾。我的出版商去掉了問號。（笑聲）美國出版商把我們的書名改成了《我們的末日》。美國人喜歡即刻的滿足與逆反。（笑聲） 我的主題是這樣的，我們的地球已經存在了四千五百多萬個世紀。但這個世紀是特殊的，第一次有一個物種，也就是我們，掌握了這個星球的命運。地球過去的歷史中，威脅主要來源於自然——疾病、地震、小行星等等——但是從今往後，最大的威脅來源於我們自己。現今不止是核威脅，在這個相互連接的世界裡，網路故障可以波及全球，航空旅行可以在幾天內將流行病傳遍世界。社會媒體簡直能以光速散播恐慌和謠言。我們太過苦惱於那些次要的危害，像是發生概率極小的空難、食品中的致癌物、低輻射等等。但我們和政治領袖們卻否認那些災難性的情節。幸運的是最可怕的事情還沒有發生，的確，他們可能不會發生。但是如果有一件事具潛在的毀滅性，那就值得我們付出大量的精力與金錢。把它掐死在搖籃裡，即使它不太可能發生。這就像給我們的房子買火災險。 科學提供了更強大的力量和保證，隨之而來的負面影響也變得更加可怕，我們變得更加脆弱。數十年之內，數百萬人將會有能力濫用飛速發展的生物技術，就像他們今天濫用網路技術一樣。費裡曼•戴森在TED演講中預言孩子們會設計並創造新的有機體，就像他們那一代人擺弄化學裝置一樣平常。好吧，這大概已經到科幻小說的邊緣了，但是即使他情節中一小部份發生了，我們的生態系統乃至整個人類種族必定不會安然無恙地存活太久。比如說，有一些生態極端主義者認為如果能大大減少人口，那會對這整個星球和大地母親更好。當這樣的人掌握了那些將在2050年普及的合成生物技術，會發生什麽？到那時，其他科幻小說中的噩夢也可能變為現實。成了流氓的愚蠢機器人或者一套發展出自我意識的網路系統威脅我們所有人。 那麼，我們能不能通過條例來防範這樣的風險？無疑我們必將嘗試，但那些企業是如此求勝心切，如此全球化，如此被商業壓力所驅使，以至於他們會不擇手段，不管法規條例說了些什麽。這就像製毒法律——我們試圖管制，但做不到。地球村裡將會有些愚蠢的村民，影響到整個地球。 所以就像我在書中所說，我們會在顛簸中走完這個世紀。我們的社會可能會遭遇挫折——事實上，有 50% 的機率是極其嚴重的挫折。但是，能否想像更糟糕的事件，那些可以毀滅所有生命的事件？當一台新的粒子加速器開始運行時，有人焦急地問它會毀滅地球嗎？或者更糟，撕破時空的結構？幸運的是對此我們可以放心，我和其他一些人指出大自然已經將同樣的實驗通過宇宙射線的撞擊做了無數次。但是對於那些在自然界中沒有先例的實驗，科學家們應該警鐘長鳴，生物學家應該預防具有潛在毀滅性的轉基因病原體。 順便說一句，我們對於毀滅性災難的風險尤其反感，這是基於一個哲學倫理問題。這個問題是這樣的。想像如下兩個場景：情景 A：90%的人類會消亡；情景 B：100%的人類會消亡。情景 B 比情景 A 糟糕多少呢？有人會說糟糕 10%，因為死亡人數多 10%。但我堅持情景 B 是無比糟糕的。做為天文學家，我無法相信人類是整個故事的結尾。在太陽開始燃燒的五十億年前，宇宙就誕生了，而且可能會永遠持續下去。因此，在地球和及其遙遠的地方，後人類的進化會被延長，就像產生了我們人類的式的達爾文式進化過程，甚至更加絕妙。事實上，未來的進化會發生得更快，會在一個技術時間尺度上，而不是一個自然選擇的時間尺度上。 所以，考慮到這些重大的利害關係，我們不應該接受哪怕十億分之一的風險，因人類滅絕而中止了這巨大潛力的風險。有些設想中的情景的確可能只會在科幻小說裡出現，但其他的一些可能會是令人不安的現實。一句重要的格言這麼說：不熟悉不等於不可能。事實上，這就是為什麽我們正在劍橋大學創建一個中心來研究如何緩解這些生存風險。看來讓一小部分人思考這些潛在災難是值得的。我們需要可以從其他人那裡得到的所有幫助。因為我們是來自茫茫宇宙中那顆珍貴暗藍色圓石上的守護者，一顆已經走過五千多萬個世紀的星球，所以請我們不要危及它的未來。 我想用一段偉大科學家彼得•梅達沃的話結束今天的演講，這段話是這樣的：「為人類敲響的鐘就像阿爾卑斯山上牛的鈴鐺，繫在我們自己的脖子上。如果它們沒有發出和諧悠揚的樂聲，那一定是我們自己的錯。」 非常感謝。 （掌聲）
